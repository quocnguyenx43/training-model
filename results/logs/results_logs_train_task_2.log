vinai/phobert-base
[22:50:28] task: task-2                                                                                 my_import.py:91
           model_type: simple                                                                           my_import.py:91
           model_name: vinai/phobert-base                                                               my_import.py:91
           padding_len: 200                                                                             my_import.py:85
           batch_size: 32                                                                               my_import.py:91
           learning_rate: 0.001                                                                         my_import.py:91
           epochs: 20                                                                                   my_import.py:91
           device: cuda                                                                                 my_import.py:91
           saving_path: ./models/task_2/simple_phobert-base                                             my_import.py:91
           train_shape: (8444, 28)                                                                      my_import.py:91
           dev_shape: (1207, 28)                                                                        my_import.py:91
           test_shape: (2413, 28)                                                                       my_import.py:91

Training ...
Epoch 1/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 1/20, Batch 263/264: 100%|█████████▉| 263/264 [00:33<00:00,  8.30it/s]
Epoch 1/20, Batch 264/264: 100%|█████████▉| 263/264 [00:33<00:00,  8.30it/s]
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [00:33<00:00,  8.59it/s]
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [00:33<00:00,  7.95it/s]
Epoch 1/20, Loss: 360.7519, 0/1 Loss: 0.9955, Hamming Loss: 0.6724, EMR: 0.0045, Acc: 0.6145, F1: 0.4966, Precision: 0.6034, Recall: 0.4802
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:04<00:00,  8.61it/s]
Evaluation, Loss: 51.4578, 0/1 Loss: 0.9793, Hamming Loss: 0.4277, EMR: 0.0207, Acc: 0.9861, F1: 0.6742, Precision: 0.6171, Recall: 0.7683
Saved the best model to path: ./models/task_2/simple_phobert-base_0.pth

Epoch 2/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 2/20, Batch 263/264: 100%|█████████▉| 263/264 [00:30<00:00,  8.64it/s]
Epoch 2/20, Batch 264/264: 100%|█████████▉| 263/264 [00:30<00:00,  8.64it/s]
Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [00:30<00:00,  8.92it/s]
Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [00:30<00:00,  8.64it/s]
Epoch 2/20, Loss: 359.6704, 0/1 Loss: 0.9955, Hamming Loss: 0.6512, EMR: 0.0045, Acc: 0.6350, F1: 0.5141, Precision: 0.6205, Recall: 0.4971
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:04<00:00,  9.20it/s]
Evaluation, Loss: 51.3200, 0/1 Loss: 0.9801, Hamming Loss: 0.4565, EMR: 0.0199, Acc: 0.9718, F1: 0.6546, Precision: 0.5929, Recall: 0.7562
Saved the best model to path: ./models/task_2/simple_phobert-base_1.pth

Epoch 3/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 3/20, Batch 263/264: 100%|█████████▉| 263/264 [00:30<00:00,  8.57it/s]
Epoch 3/20, Batch 264/264: 100%|█████████▉| 263/264 [00:30<00:00,  8.57it/s]
Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [00:30<00:00,  8.88it/s]
Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [00:30<00:00,  8.53it/s]
Epoch 3/20, Loss: 359.2679, 0/1 Loss: 0.9948, Hamming Loss: 0.6516, EMR: 0.0052, Acc: 0.6448, F1: 0.5159, Precision: 0.6172, Recall: 0.5052
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:04<00:00,  8.75it/s]
Evaluation, Loss: 51.2114, 0/1 Loss: 0.9785, Hamming Loss: 0.4772, EMR: 0.0215, Acc: 0.9861, F1: 0.6437, Precision: 0.5723, Recall: 0.7683
Saved the best model to path: ./models/task_2/simple_phobert-base_2.pth

Epoch 4/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 4/20, Batch 263/264: 100%|█████████▉| 263/264 [00:30<00:00,  8.57it/s]
Epoch 4/20, Batch 264/264: 100%|█████████▉| 263/264 [00:30<00:00,  8.57it/s]
Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  8.82it/s]
Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  8.51it/s]
Epoch 4/20, Loss: 359.1104, 0/1 Loss: 0.9946, Hamming Loss: 0.6690, EMR: 0.0054, Acc: 0.6628, F1: 0.5110, Precision: 0.5873, Recall: 0.5201
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:04<00:00,  9.08it/s]
Evaluation, Loss: 51.0881, 0/1 Loss: 0.9801, Hamming Loss: 0.5752, EMR: 0.0199, Acc: 0.9861, F1: 0.5973, Precision: 0.5085, Recall: 0.7683
Saved the best model to path: ./models/task_2/simple_phobert-base_3.pth

Epoch 5/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 5/20, Batch 263/264: 100%|█████████▉| 263/264 [00:30<00:00,  8.50it/s]
Epoch 5/20, Batch 264/264: 100%|█████████▉| 263/264 [00:30<00:00,  8.50it/s]
Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  8.80it/s]
Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  8.50it/s]
Epoch 5/20, Loss: 358.9202, 0/1 Loss: 0.9947, Hamming Loss: 0.6829, EMR: 0.0053, Acc: 0.6617, F1: 0.5032, Precision: 0.5705, Recall: 0.5198
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:04<00:00,  9.06it/s]
Evaluation, Loss: 51.1092, 0/1 Loss: 0.9818, Hamming Loss: 0.5429, EMR: 0.0182, Acc: 0.9861, F1: 0.6149, Precision: 0.5358, Recall: 0.7683

Epoch 6/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 6/20, Batch 263/264: 100%|█████████▉| 263/264 [00:31<00:00,  8.52it/s]
Epoch 6/20, Batch 264/264: 100%|█████████▉| 263/264 [00:31<00:00,  8.52it/s]
Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  8.83it/s]
Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  8.40it/s]
Epoch 6/20, Loss: 358.7228, 0/1 Loss: 0.9959, Hamming Loss: 0.6765, EMR: 0.0041, Acc: 0.6501, F1: 0.5041, Precision: 0.5882, Recall: 0.5111
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:04<00:00,  9.01it/s]
Evaluation, Loss: 51.2203, 0/1 Loss: 0.9801, Hamming Loss: 0.6309, EMR: 0.0199, Acc: 0.9861, F1: 0.5739, Precision: 0.4778, Recall: 0.7683

Epoch 7/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 7/20, Batch 263/264: 100%|█████████▉| 263/264 [00:31<00:00,  8.45it/s]
Epoch 7/20, Batch 264/264: 100%|█████████▉| 263/264 [00:31<00:00,  8.45it/s]
Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  8.71it/s]
Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  8.40it/s]
Epoch 7/20, Loss: 358.5122, 0/1 Loss: 0.9944, Hamming Loss: 0.6785, EMR: 0.0056, Acc: 0.6794, F1: 0.5103, Precision: 0.5703, Recall: 0.5340
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:04<00:00,  9.06it/s]
Evaluation, Loss: 50.9303, 0/1 Loss: 0.9776, Hamming Loss: 0.5594, EMR: 0.0224, Acc: 0.9861, F1: 0.6074, Precision: 0.5255, Recall: 0.7683
Saved the best model to path: ./models/task_2/simple_phobert-base_6.pth

Epoch 8/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 8/20, Batch 263/264: 100%|█████████▉| 263/264 [00:31<00:00,  8.52it/s]
Epoch 8/20, Batch 264/264: 100%|█████████▉| 263/264 [00:31<00:00,  8.52it/s]
Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  8.82it/s]
Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  8.47it/s]
Epoch 8/20, Loss: 358.5659, 0/1 Loss: 0.9956, Hamming Loss: 0.6855, EMR: 0.0044, Acc: 0.6771, F1: 0.5067, Precision: 0.5645, Recall: 0.5330
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:04<00:00,  9.01it/s]
Evaluation, Loss: 51.1532, 0/1 Loss: 0.9826, Hamming Loss: 0.5760, EMR: 0.0174, Acc: 0.9861, F1: 0.5996, Precision: 0.5125, Recall: 0.7683

Epoch 9/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 9/20, Batch 263/264: 100%|█████████▉| 263/264 [00:31<00:00,  8.45it/s]
Epoch 9/20, Batch 264/264: 100%|█████████▉| 263/264 [00:31<00:00,  8.45it/s]
Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  8.74it/s]
Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  8.43it/s]
Epoch 9/20, Loss: 358.5477, 0/1 Loss: 0.9936, Hamming Loss: 0.6876, EMR: 0.0064, Acc: 0.6760, F1: 0.5061, Precision: 0.5647, Recall: 0.5313
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:04<00:00,  9.02it/s]
Evaluation, Loss: 51.1155, 0/1 Loss: 0.9801, Hamming Loss: 0.5928, EMR: 0.0199, Acc: 0.9861, F1: 0.5943, Precision: 0.5076, Recall: 0.7683

Epoch 10/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 10/20, Batch 263/264: 100%|█████████▉| 263/264 [00:31<00:00,  8.28it/s]
Epoch 10/20, Batch 264/264: 100%|█████████▉| 263/264 [00:31<00:00,  8.28it/s]
Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  8.67it/s]
Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  8.42it/s]
Epoch 10/20, Loss: 358.4930, 0/1 Loss: 0.9937, Hamming Loss: 0.6850, EMR: 0.0063, Acc: 0.6732, F1: 0.5056, Precision: 0.5659, Recall: 0.5292
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:04<00:00,  9.00it/s]
Evaluation, Loss: 50.9580, 0/1 Loss: 0.9768, Hamming Loss: 0.5441, EMR: 0.0232, Acc: 0.9861, F1: 0.6139, Precision: 0.5346, Recall: 0.7683

Epoch 11/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 11/20, Batch 263/264: 100%|█████████▉| 263/264 [00:31<00:00,  8.50it/s]
Epoch 11/20, Batch 264/264: 100%|█████████▉| 263/264 [00:31<00:00,  8.50it/s]
Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  8.78it/s]
Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  8.42it/s]
Epoch 11/20, Loss: 358.1714, 0/1 Loss: 0.9927, Hamming Loss: 0.6800, EMR: 0.0073, Acc: 0.6765, F1: 0.5076, Precision: 0.5681, Recall: 0.5313
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:04<00:00,  8.99it/s]
Evaluation, Loss: 51.1019, 0/1 Loss: 0.9793, Hamming Loss: 0.5253, EMR: 0.0207, Acc: 0.9861, F1: 0.6246, Precision: 0.5495, Recall: 0.7683

Epoch 12/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 12/20, Batch 263/264: 100%|█████████▉| 263/264 [00:30<00:00,  8.71it/s]
Epoch 12/20, Batch 264/264: 100%|█████████▉| 263/264 [00:30<00:00,  8.71it/s]
Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  9.02it/s]
Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [00:31<00:00,  8.50it/s]
Epoch 12/20, Loss: 358.3968, 0/1 Loss: 0.9940, Hamming Loss: 0.6884, EMR: 0.0060, Acc: 0.6643, F1: 0.5015, Precision: 0.5661, Recall: 0.5222
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:04<00:00,  9.33it/s]
Evaluation, Loss: 50.9396, 0/1 Loss: 0.9801, Hamming Loss: 0.5864, EMR: 0.0199, Acc: 0.9861, F1: 0.5917, Precision: 0.5025, Recall: 0.7683

Early stopping triggered!


uitnlp/visobert
[22:57:53] task: task-2                                                                                 my_import.py:91
           model_type: simple                                                                           my_import.py:91
           model_name: uitnlp/visobert                                                                  my_import.py:91
           padding_len: 400                                                                             my_import.py:85
           batch_size: 32                                                                               my_import.py:91
           learning_rate: 0.001                                                                         my_import.py:91
           epochs: 20                                                                                   my_import.py:91
           device: cuda                                                                                 my_import.py:91
           saving_path: ./models/task_2/simple_visobert                                                 my_import.py:91
           train_shape: (8444, 28)                                                                      my_import.py:91
           dev_shape: (1207, 28)                                                                        my_import.py:91
           test_shape: (2413, 28)                                                                       my_import.py:91

Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/visobert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.


Training ...
Epoch 1/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 1/20, Batch 263/264: 100%|█████████▉| 263/264 [01:02<00:00,  4.33it/s]
Epoch 1/20, Batch 264/264: 100%|█████████▉| 263/264 [01:02<00:00,  4.33it/s]
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [01:02<00:00,  4.49it/s]
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [01:02<00:00,  4.23it/s]
Epoch 1/20, Loss: 362.0973, 0/1 Loss: 0.9999, Hamming Loss: 0.8384, EMR: 0.0001, Acc: 0.3908, F1: 0.3474, Precision: 0.4956, Recall: 0.3048
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  5.04it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.67it/s]
Evaluation, Loss: 51.6554, 0/1 Loss: 1.0000, Hamming Loss: 0.8279, EMR: 0.0000, Acc: 0.4898, F1: 0.3885, Precision: 0.4219, Recall: 0.3790
Saved the best model to path: ./models/task_2/simple_visobert_0.pth

Epoch 2/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 2/20, Batch 263/264: 100%|█████████▉| 263/264 [01:01<00:00,  4.32it/s]
Epoch 2/20, Batch 264/264: 100%|█████████▉| 263/264 [01:01<00:00,  4.32it/s]
Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [01:01<00:00,  4.49it/s]
Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [01:01<00:00,  4.30it/s]
Epoch 2/20, Loss: 361.3845, 0/1 Loss: 1.0000, Hamming Loss: 0.8635, EMR: 0.0000, Acc: 0.3442, F1: 0.3114, Precision: 0.4846, Recall: 0.2686
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  5.03it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.67it/s]
Evaluation, Loss: 51.5420, 0/1 Loss: 1.0000, Hamming Loss: 0.8563, EMR: 0.0000, Acc: 0.4898, F1: 0.3768, Precision: 0.3987, Recall: 0.3790
Saved the best model to path: ./models/task_2/simple_visobert_1.pth

Epoch 3/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 3/20, Batch 263/264: 100%|█████████▉| 263/264 [01:02<00:00,  4.26it/s]
Epoch 3/20, Batch 264/264: 100%|█████████▉| 263/264 [01:02<00:00,  4.26it/s]
Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [01:02<00:00,  4.42it/s]
Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [01:02<00:00,  4.21it/s]
Epoch 3/20, Loss: 361.5414, 0/1 Loss: 1.0000, Hamming Loss: 0.8676, EMR: 0.0000, Acc: 0.3438, F1: 0.3091, Precision: 0.4755, Recall: 0.2690
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.93it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.52it/s]
Evaluation, Loss: 51.4952, 0/1 Loss: 1.0000, Hamming Loss: 0.7533, EMR: 0.0000, Acc: 0.4898, F1: 0.4198, Precision: 0.4859, Recall: 0.3790
Saved the best model to path: ./models/task_2/simple_visobert_2.pth

Epoch 4/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 4/20, Batch 263/264: 100%|█████████▉| 263/264 [01:02<00:00,  4.21it/s]
Epoch 4/20, Batch 264/264: 100%|█████████▉| 263/264 [01:02<00:00,  4.21it/s]
Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [01:02<00:00,  4.40it/s]
Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [01:02<00:00,  4.22it/s]
Epoch 4/20, Loss: 361.1263, 0/1 Loss: 1.0000, Hamming Loss: 0.8679, EMR: 0.0000, Acc: 0.3376, F1: 0.3045, Precision: 0.4685, Recall: 0.2637
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.85it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.41it/s]
Evaluation, Loss: 51.4617, 0/1 Loss: 1.0000, Hamming Loss: 0.8685, EMR: 0.0000, Acc: 0.4898, F1: 0.3723, Precision: 0.3888, Recall: 0.3790
Saved the best model to path: ./models/task_2/simple_visobert_3.pth

Epoch 5/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 5/20, Batch 263/264: 100%|█████████▉| 263/264 [01:02<00:00,  4.26it/s]
Epoch 5/20, Batch 264/264: 100%|█████████▉| 263/264 [01:02<00:00,  4.26it/s]
Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [01:02<00:00,  4.38it/s]
Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [01:02<00:00,  4.21it/s]
Epoch 5/20, Loss: 361.0195, 0/1 Loss: 1.0000, Hamming Loss: 0.8695, EMR: 0.0000, Acc: 0.3422, F1: 0.3064, Precision: 0.4658, Recall: 0.2671
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.94it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.58it/s]
Evaluation, Loss: 51.3866, 0/1 Loss: 1.0000, Hamming Loss: 0.8252, EMR: 0.0000, Acc: 0.4898, F1: 0.3880, Precision: 0.4221, Recall: 0.3790
Saved the best model to path: ./models/task_2/simple_visobert_4.pth

Epoch 6/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 6/20, Batch 263/264: 100%|█████████▉| 263/264 [01:02<00:00,  4.16it/s]
Epoch 6/20, Batch 264/264: 100%|█████████▉| 263/264 [01:02<00:00,  4.16it/s]
Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [01:02<00:00,  4.28it/s]
Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [01:02<00:00,  4.23it/s]
Epoch 6/20, Loss: 360.9812, 0/1 Loss: 1.0000, Hamming Loss: 0.8673, EMR: 0.0000, Acc: 0.3384, F1: 0.3049, Precision: 0.4699, Recall: 0.2645
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.87it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.51it/s]
Evaluation, Loss: 51.3034, 0/1 Loss: 1.0000, Hamming Loss: 0.8200, EMR: 0.0000, Acc: 0.4898, F1: 0.3902, Precision: 0.4267, Recall: 0.3790
Saved the best model to path: ./models/task_2/simple_visobert_5.pth

Epoch 7/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 7/20, Batch 263/264: 100%|█████████▉| 263/264 [01:02<00:00,  4.25it/s]
Epoch 7/20, Batch 264/264: 100%|█████████▉| 263/264 [01:02<00:00,  4.25it/s]
Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [01:02<00:00,  4.42it/s]
Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [01:02<00:00,  4.24it/s]
Epoch 7/20, Loss: 361.2225, 0/1 Loss: 1.0000, Hamming Loss: 0.8766, EMR: 0.0000, Acc: 0.3264, F1: 0.2945, Precision: 0.4498, Recall: 0.2556
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.95it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.60it/s]
Evaluation, Loss: 51.4047, 0/1 Loss: 1.0000, Hamming Loss: 0.8304, EMR: 0.0000, Acc: 0.4898, F1: 0.3861, Precision: 0.4185, Recall: 0.3790

Epoch 8/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 8/20, Batch 263/264: 100%|█████████▉| 263/264 [01:02<00:00,  4.26it/s]
Epoch 8/20, Batch 264/264: 100%|█████████▉| 263/264 [01:02<00:00,  4.26it/s]
Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [01:02<00:00,  4.43it/s]
Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [01:02<00:00,  4.22it/s]
Epoch 8/20, Loss: 360.8226, 0/1 Loss: 1.0000, Hamming Loss: 0.8684, EMR: 0.0000, Acc: 0.3319, F1: 0.3003, Precision: 0.4639, Recall: 0.2597
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  5.03it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.57it/s]
Evaluation, Loss: 51.3052, 0/1 Loss: 1.0000, Hamming Loss: 0.7902, EMR: 0.0000, Acc: 0.4898, F1: 0.4013, Precision: 0.4486, Recall: 0.3790

Epoch 9/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 9/20, Batch 263/264: 100%|█████████▉| 263/264 [01:01<00:00,  4.33it/s]
Epoch 9/20, Batch 264/264: 100%|█████████▉| 263/264 [01:01<00:00,  4.33it/s]
Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [01:01<00:00,  4.50it/s]
Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [01:01<00:00,  4.30it/s]
Epoch 9/20, Loss: 360.9464, 0/1 Loss: 1.0000, Hamming Loss: 0.8744, EMR: 0.0000, Acc: 0.3318, F1: 0.2979, Precision: 0.4535, Recall: 0.2596
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.83it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.63it/s]
Evaluation, Loss: 51.3558, 0/1 Loss: 1.0000, Hamming Loss: 0.8641, EMR: 0.0000, Acc: 0.4898, F1: 0.3738, Precision: 0.3928, Recall: 0.3790

Epoch 10/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 10/20, Batch 263/264: 100%|█████████▉| 263/264 [01:00<00:00,  4.30it/s]
Epoch 10/20, Batch 264/264: 100%|█████████▉| 263/264 [01:00<00:00,  4.30it/s]
Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [01:01<00:00,  4.49it/s]
Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [01:01<00:00,  4.32it/s]
Epoch 10/20, Loss: 360.8450, 0/1 Loss: 1.0000, Hamming Loss: 0.8692, EMR: 0.0000, Acc: 0.3346, F1: 0.3019, Precision: 0.4651, Recall: 0.2615
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  5.04it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.69it/s]
Evaluation, Loss: 51.3522, 0/1 Loss: 1.0000, Hamming Loss: 0.8449, EMR: 0.0000, Acc: 0.4898, F1: 0.3806, Precision: 0.4068, Recall: 0.3790

Epoch 11/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 11/20, Batch 263/264: 100%|█████████▉| 263/264 [01:00<00:00,  4.32it/s]
Epoch 11/20, Batch 264/264: 100%|█████████▉| 263/264 [01:00<00:00,  4.32it/s]
Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [01:01<00:00,  4.49it/s]
Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [01:01<00:00,  4.32it/s]
Epoch 11/20, Loss: 360.7099, 0/1 Loss: 1.0000, Hamming Loss: 0.8714, EMR: 0.0000, Acc: 0.3322, F1: 0.2990, Precision: 0.4574, Recall: 0.2598
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  5.03it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.68it/s]
Evaluation, Loss: 51.3641, 0/1 Loss: 1.0000, Hamming Loss: 0.8250, EMR: 0.0000, Acc: 0.4898, F1: 0.3883, Precision: 0.4224, Recall: 0.3790

Early stopping triggered!


uitnlp/CafeBERT
[23:11:06] task: task-2                                                                                 my_import.py:91
           model_type: simple                                                                           my_import.py:91
           model_name: uitnlp/CafeBERT                                                                  my_import.py:91
           padding_len: 400                                                                             my_import.py:85
           batch_size: 32                                                                               my_import.py:91
           learning_rate: 0.001                                                                         my_import.py:91
           epochs: 20                                                                                   my_import.py:91
           device: cuda                                                                                 my_import.py:91
           saving_path: ./models/task_2/simple_CafeBERT                                                 my_import.py:91
           train_shape: (8444, 28)                                                                      my_import.py:91
           dev_shape: (1207, 28)                                                                        my_import.py:91
           test_shape: (2413, 28)                                                                       my_import.py:91

Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/CafeBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.


Training ...
Epoch 1/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 1/20, Batch 263/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.48it/s]
Epoch 1/20, Batch 264/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.48it/s]
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [02:59<00:00,  1.54it/s]
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [02:59<00:00,  1.47it/s]
Epoch 1/20, Loss: 363.3084, 0/1 Loss: 0.9994, Hamming Loss: 0.8746, EMR: 0.0006, Acc: 0.3483, F1: 0.3036, Precision: 0.3935, Recall: 0.2718
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:23<00:00,  1.72it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:23<00:00,  1.59it/s]
Evaluation, Loss: 51.9395, 0/1 Loss: 1.0000, Hamming Loss: 0.8687, EMR: 0.0000, Acc: 0.4891, F1: 0.3792, Precision: 0.3904, Recall: 0.3786
Saved the best model to path: ./models/task_2/simple_CafeBERT_0.pth

Epoch 2/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 2/20, Batch 263/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.48it/s]
Epoch 2/20, Batch 264/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.48it/s]
Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.54it/s]
Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.48it/s]
Epoch 2/20, Loss: 362.3071, 0/1 Loss: 1.0000, Hamming Loss: 0.9158, EMR: 0.0000, Acc: 0.2947, F1: 0.2611, Precision: 0.3416, Recall: 0.2295
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.71it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.57it/s]
Evaluation, Loss: 51.7741, 0/1 Loss: 1.0000, Hamming Loss: 0.8687, EMR: 0.0000, Acc: 0.4891, F1: 0.3792, Precision: 0.3904, Recall: 0.3786
Saved the best model to path: ./models/task_2/simple_CafeBERT_1.pth

Epoch 3/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 3/20, Batch 263/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.47it/s]
Epoch 3/20, Batch 264/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.47it/s]
Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.53it/s]
Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.48it/s]
Epoch 3/20, Loss: 362.0394, 0/1 Loss: 1.0000, Hamming Loss: 0.9077, EMR: 0.0000, Acc: 0.3000, F1: 0.2672, Precision: 0.3589, Recall: 0.2332
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.71it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.58it/s]
Evaluation, Loss: 51.7605, 0/1 Loss: 1.0000, Hamming Loss: 0.8687, EMR: 0.0000, Acc: 0.4891, F1: 0.3792, Precision: 0.3904, Recall: 0.3786
Saved the best model to path: ./models/task_2/simple_CafeBERT_2.pth

Epoch 4/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 4/20, Batch 263/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.48it/s]
Epoch 4/20, Batch 264/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.48it/s]
Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [02:59<00:00,  1.54it/s]
Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [02:59<00:00,  1.47it/s]
Epoch 4/20, Loss: 362.2707, 0/1 Loss: 1.0000, Hamming Loss: 0.9124, EMR: 0.0000, Acc: 0.2969, F1: 0.2645, Precision: 0.3532, Recall: 0.2317
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.70it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.58it/s]
Evaluation, Loss: 51.7003, 0/1 Loss: 1.0000, Hamming Loss: 0.8687, EMR: 0.0000, Acc: 0.4891, F1: 0.3792, Precision: 0.3904, Recall: 0.3786
Saved the best model to path: ./models/task_2/simple_CafeBERT_3.pth

Epoch 5/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 5/20, Batch 263/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.47it/s]
Epoch 5/20, Batch 264/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.47it/s]
Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.53it/s]
Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.48it/s]
Epoch 5/20, Loss: 361.9771, 0/1 Loss: 1.0000, Hamming Loss: 0.9071, EMR: 0.0000, Acc: 0.3016, F1: 0.2683, Precision: 0.3611, Recall: 0.2347
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.71it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.58it/s]
Evaluation, Loss: 51.6970, 0/1 Loss: 1.0000, Hamming Loss: 0.8486, EMR: 0.0000, Acc: 0.4891, F1: 0.3848, Precision: 0.4042, Recall: 0.3786
Saved the best model to path: ./models/task_2/simple_CafeBERT_4.pth

Epoch 6/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 6/20, Batch 263/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.48it/s]
Epoch 6/20, Batch 264/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.48it/s]
Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [02:59<00:00,  1.54it/s]
Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [02:59<00:00,  1.47it/s]
Epoch 6/20, Loss: 361.9559, 0/1 Loss: 1.0000, Hamming Loss: 0.9060, EMR: 0.0000, Acc: 0.3008, F1: 0.2691, Precision: 0.3686, Recall: 0.2345
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.71it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.57it/s]
Evaluation, Loss: 51.7208, 0/1 Loss: 1.0000, Hamming Loss: 0.8403, EMR: 0.0000, Acc: 0.4891, F1: 0.3875, Precision: 0.4105, Recall: 0.3786

Epoch 7/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 7/20, Batch 263/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.48it/s]
Epoch 7/20, Batch 264/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.48it/s]
Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.54it/s]
Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.48it/s]
Epoch 7/20, Loss: 361.8365, 0/1 Loss: 1.0000, Hamming Loss: 0.9015, EMR: 0.0000, Acc: 0.3057, F1: 0.2727, Precision: 0.3726, Recall: 0.2379
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:23<00:00,  1.71it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:23<00:00,  1.58it/s]
Evaluation, Loss: 51.6798, 0/1 Loss: 1.0000, Hamming Loss: 0.8362, EMR: 0.0000, Acc: 0.4891, F1: 0.3905, Precision: 0.4175, Recall: 0.3786
Saved the best model to path: ./models/task_2/simple_CafeBERT_6.pth

Epoch 8/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 8/20, Batch 263/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.47it/s]
Epoch 8/20, Batch 264/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.47it/s]
Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [02:59<00:00,  1.53it/s]
Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [02:59<00:00,  1.47it/s]
Epoch 8/20, Loss: 361.7391, 0/1 Loss: 1.0000, Hamming Loss: 0.8992, EMR: 0.0000, Acc: 0.3056, F1: 0.2740, Precision: 0.3795, Recall: 0.2388
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.71it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.58it/s]
Evaluation, Loss: 51.6971, 0/1 Loss: 1.0000, Hamming Loss: 0.8380, EMR: 0.0000, Acc: 0.4891, F1: 0.3886, Precision: 0.4132, Recall: 0.3786

Epoch 9/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 9/20, Batch 263/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.48it/s]
Epoch 9/20, Batch 264/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.48it/s]
Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.54it/s]
Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.48it/s]
Epoch 9/20, Loss: 361.6338, 0/1 Loss: 1.0000, Hamming Loss: 0.8965, EMR: 0.0000, Acc: 0.3041, F1: 0.2744, Precision: 0.3861, Recall: 0.2365
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:23<00:00,  1.71it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:23<00:00,  1.58it/s]
Evaluation, Loss: 51.6385, 0/1 Loss: 1.0000, Hamming Loss: 0.8536, EMR: 0.0000, Acc: 0.4891, F1: 0.3832, Precision: 0.4007, Recall: 0.3786
Saved the best model to path: ./models/task_2/simple_CafeBERT_8.pth

Epoch 10/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 10/20, Batch 263/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.47it/s]
Epoch 10/20, Batch 264/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.47it/s]
Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.53it/s]
Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.48it/s]
Epoch 10/20, Loss: 361.6137, 0/1 Loss: 1.0000, Hamming Loss: 0.8974, EMR: 0.0000, Acc: 0.3080, F1: 0.2772, Precision: 0.3872, Recall: 0.2397
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:23<00:00,  1.71it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:23<00:00,  1.58it/s]
Evaluation, Loss: 51.6339, 0/1 Loss: 1.0000, Hamming Loss: 0.8494, EMR: 0.0000, Acc: 0.4891, F1: 0.3844, Precision: 0.4035, Recall: 0.3786
Saved the best model to path: ./models/task_2/simple_CafeBERT_9.pth

Epoch 11/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 11/20, Batch 263/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.47it/s]
Epoch 11/20, Batch 264/264: 100%|█████████▉| 263/264 [02:58<00:00,  1.47it/s]
Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.53it/s]
Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.48it/s]
Epoch 11/20, Loss: 361.7654, 0/1 Loss: 1.0000, Hamming Loss: 0.9032, EMR: 0.0000, Acc: 0.2996, F1: 0.2691, Precision: 0.3720, Recall: 0.2337
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.71it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.57it/s]
Evaluation, Loss: 51.7670, 0/1 Loss: 1.0000, Hamming Loss: 0.8239, EMR: 0.0000, Acc: 0.4891, F1: 0.3960, Precision: 0.4301, Recall: 0.3786

Epoch 12/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 12/20, Batch 263/264: 100%|█████████▉| 263/264 [02:56<00:00,  1.49it/s]
Epoch 12/20, Batch 264/264: 100%|█████████▉| 263/264 [02:56<00:00,  1.49it/s]
Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [02:57<00:00,  1.55it/s]
Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [02:57<00:00,  1.49it/s]
Epoch 12/20, Loss: 361.8298, 0/1 Loss: 1.0000, Hamming Loss: 0.8982, EMR: 0.0000, Acc: 0.3047, F1: 0.2745, Precision: 0.3843, Recall: 0.2374
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:23<00:00,  1.73it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:23<00:00,  1.59it/s]
Evaluation, Loss: 51.7126, 0/1 Loss: 1.0000, Hamming Loss: 0.8244, EMR: 0.0000, Acc: 0.4891, F1: 0.3954, Precision: 0.4285, Recall: 0.3786

Epoch 13/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 13/20, Batch 263/264: 100%|█████████▉| 263/264 [02:57<00:00,  1.49it/s]
Epoch 13/20, Batch 264/264: 100%|█████████▉| 263/264 [02:57<00:00,  1.49it/s]
Epoch 13/20, Batch 264/264: 100%|██████████| 264/264 [02:57<00:00,  1.55it/s]
Epoch 13/20, Batch 264/264: 100%|██████████| 264/264 [02:57<00:00,  1.49it/s]
Epoch 13/20, Loss: 361.5770, 0/1 Loss: 1.0000, Hamming Loss: 0.8983, EMR: 0.0000, Acc: 0.3063, F1: 0.2754, Precision: 0.3814, Recall: 0.2386
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:23<00:00,  1.72it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:23<00:00,  1.59it/s]
Evaluation, Loss: 51.7567, 0/1 Loss: 1.0000, Hamming Loss: 0.8219, EMR: 0.0000, Acc: 0.4891, F1: 0.3973, Precision: 0.4331, Recall: 0.3786

Epoch 14/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 14/20, Batch 263/264: 100%|█████████▉| 263/264 [02:57<00:00,  1.48it/s]
Epoch 14/20, Batch 264/264: 100%|█████████▉| 263/264 [02:57<00:00,  1.48it/s]
Epoch 14/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.54it/s]
Epoch 14/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.48it/s]
Epoch 14/20, Loss: 361.6623, 0/1 Loss: 1.0000, Hamming Loss: 0.9002, EMR: 0.0000, Acc: 0.2984, F1: 0.2690, Precision: 0.3764, Recall: 0.2323
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.71it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.58it/s]
Evaluation, Loss: 51.7941, 0/1 Loss: 1.0000, Hamming Loss: 0.8219, EMR: 0.0000, Acc: 0.4891, F1: 0.3965, Precision: 0.4312, Recall: 0.3786

Epoch 15/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 15/20, Batch 263/264: 100%|█████████▉| 263/264 [02:57<00:00,  1.48it/s]
Epoch 15/20, Batch 264/264: 100%|█████████▉| 263/264 [02:57<00:00,  1.48it/s]
Epoch 15/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.54it/s]
Epoch 15/20, Batch 264/264: 100%|██████████| 264/264 [02:58<00:00,  1.48it/s]
Epoch 15/20, Loss: 361.8683, 0/1 Loss: 1.0000, Hamming Loss: 0.9001, EMR: 0.0000, Acc: 0.3019, F1: 0.2733, Precision: 0.3856, Recall: 0.2354
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.71it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:24<00:00,  1.58it/s]
Evaluation, Loss: 51.6727, 0/1 Loss: 1.0000, Hamming Loss: 0.8662, EMR: 0.0000, Acc: 0.4891, F1: 0.3799, Precision: 0.3922, Recall: 0.3786

Early stopping triggered!


xlm-roberta-base
[00:03:16] task: task-2                                                                                 my_import.py:91
           model_type: simple                                                                           my_import.py:91
           model_name: xlm-roberta-base                                                                 my_import.py:91
           padding_len: 500                                                                             my_import.py:85
           batch_size: 32                                                                               my_import.py:91
           learning_rate: 0.001                                                                         my_import.py:91
           epochs: 20                                                                                   my_import.py:91
           device: cuda                                                                                 my_import.py:91
           saving_path: ./models/task_2/simple_xlm-roberta-base                                         my_import.py:91
           train_shape: (8444, 28)                                                                      my_import.py:91
           dev_shape: (1207, 28)                                                                        my_import.py:91
           test_shape: (2413, 28)                                                                       my_import.py:91

Training ...
Epoch 1/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 1/20, Batch 263/264: 100%|█████████▉| 263/264 [01:20<00:00,  3.39it/s]
Epoch 1/20, Batch 264/264: 100%|█████████▉| 263/264 [01:20<00:00,  3.39it/s]
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [01:20<00:00,  3.52it/s]
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [01:20<00:00,  3.28it/s]
Epoch 1/20, Loss: 361.6653, 0/1 Loss: 0.9989, Hamming Loss: 0.7677, EMR: 0.0011, Acc: 0.5530, F1: 0.4364, Precision: 0.4985, Recall: 0.4324
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.95it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.61it/s]
Evaluation, Loss: 51.7088, 0/1 Loss: 1.0000, Hamming Loss: 0.6564, EMR: 0.0000, Acc: 0.7397, F1: 0.5227, Precision: 0.4920, Recall: 0.5746
Saved the best model to path: ./models/task_2/simple_xlm-roberta-base_0.pth

Epoch 2/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 2/20, Batch 263/264: 100%|█████████▉| 263/264 [01:18<00:00,  3.36it/s]
Epoch 2/20, Batch 264/264: 100%|█████████▉| 263/264 [01:18<00:00,  3.36it/s]
Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.49it/s]
Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.35it/s]
Epoch 2/20, Loss: 361.4428, 0/1 Loss: 0.9999, Hamming Loss: 0.7903, EMR: 0.0001, Acc: 0.5183, F1: 0.4119, Precision: 0.4949, Recall: 0.4046
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.95it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.68it/s]
Evaluation, Loss: 51.6656, 0/1 Loss: 1.0000, Hamming Loss: 0.6564, EMR: 0.0000, Acc: 0.7397, F1: 0.5227, Precision: 0.4920, Recall: 0.5746
Saved the best model to path: ./models/task_2/simple_xlm-roberta-base_1.pth

Epoch 3/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 3/20, Batch 263/264: 100%|█████████▉| 263/264 [01:18<00:00,  3.21it/s]
Epoch 3/20, Batch 264/264: 100%|█████████▉| 263/264 [01:18<00:00,  3.21it/s]
Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.33it/s]
Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.35it/s]
Epoch 3/20, Loss: 361.8175, 0/1 Loss: 0.9999, Hamming Loss: 0.7999, EMR: 0.0001, Acc: 0.5126, F1: 0.4072, Precision: 0.4919, Recall: 0.4003
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.67it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.59it/s]
Evaluation, Loss: 51.6678, 0/1 Loss: 1.0000, Hamming Loss: 0.6564, EMR: 0.0000, Acc: 0.7397, F1: 0.5227, Precision: 0.4920, Recall: 0.5746

Epoch 4/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 4/20, Batch 263/264: 100%|█████████▉| 263/264 [01:19<00:00,  3.39it/s]
Epoch 4/20, Batch 264/264: 100%|█████████▉| 263/264 [01:19<00:00,  3.39it/s]
Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [01:19<00:00,  3.52it/s]
Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [01:19<00:00,  3.31it/s]
Epoch 4/20, Loss: 361.4379, 0/1 Loss: 0.9999, Hamming Loss: 0.7929, EMR: 0.0001, Acc: 0.5069, F1: 0.4067, Precision: 0.5005, Recall: 0.3957
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.97it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.70it/s]
Evaluation, Loss: 51.6675, 0/1 Loss: 1.0000, Hamming Loss: 0.6564, EMR: 0.0000, Acc: 0.7397, F1: 0.5227, Precision: 0.4920, Recall: 0.5746

Epoch 5/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 5/20, Batch 263/264: 100%|█████████▉| 263/264 [01:18<00:00,  3.37it/s]
Epoch 5/20, Batch 264/264: 100%|█████████▉| 263/264 [01:18<00:00,  3.37it/s]
Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.51it/s]
Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.34it/s]
Epoch 5/20, Loss: 361.5267, 0/1 Loss: 0.9999, Hamming Loss: 0.7955, EMR: 0.0001, Acc: 0.5045, F1: 0.4055, Precision: 0.5032, Recall: 0.3937
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.97it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.70it/s]
Evaluation, Loss: 51.6590, 0/1 Loss: 1.0000, Hamming Loss: 0.6564, EMR: 0.0000, Acc: 0.7397, F1: 0.5227, Precision: 0.4920, Recall: 0.5746
Saved the best model to path: ./models/task_2/simple_xlm-roberta-base_4.pth

Epoch 6/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 6/20, Batch 263/264: 100%|█████████▉| 263/264 [01:18<00:00,  3.38it/s]
Epoch 6/20, Batch 264/264: 100%|█████████▉| 263/264 [01:18<00:00,  3.38it/s]
Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.50it/s]
Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.35it/s]
Epoch 6/20, Loss: 361.1669, 0/1 Loss: 1.0000, Hamming Loss: 0.7884, EMR: 0.0000, Acc: 0.5069, F1: 0.4085, Precision: 0.5099, Recall: 0.3963
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.82it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.61it/s]
Evaluation, Loss: 51.5537, 0/1 Loss: 1.0000, Hamming Loss: 0.6564, EMR: 0.0000, Acc: 0.7397, F1: 0.5227, Precision: 0.4920, Recall: 0.5746
Saved the best model to path: ./models/task_2/simple_xlm-roberta-base_5.pth

Epoch 7/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 7/20, Batch 263/264: 100%|█████████▉| 263/264 [01:17<00:00,  3.43it/s]
Epoch 7/20, Batch 264/264: 100%|█████████▉| 263/264 [01:17<00:00,  3.43it/s]
Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.55it/s]
Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.39it/s]
Epoch 7/20, Loss: 361.3637, 0/1 Loss: 0.9996, Hamming Loss: 0.7906, EMR: 0.0004, Acc: 0.5096, F1: 0.4080, Precision: 0.5031, Recall: 0.3977
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  4.03it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.74it/s]
Evaluation, Loss: 51.6110, 0/1 Loss: 1.0000, Hamming Loss: 0.6645, EMR: 0.0000, Acc: 0.7397, F1: 0.5191, Precision: 0.4860, Recall: 0.5746

Epoch 8/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 8/20, Batch 263/264: 100%|█████████▉| 263/264 [01:17<00:00,  3.43it/s]
Epoch 8/20, Batch 264/264: 100%|█████████▉| 263/264 [01:17<00:00,  3.43it/s]
Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.56it/s]
Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.39it/s]
Epoch 8/20, Loss: 361.6885, 0/1 Loss: 0.9996, Hamming Loss: 0.7947, EMR: 0.0004, Acc: 0.5032, F1: 0.4054, Precision: 0.5048, Recall: 0.3934
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  4.03it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.74it/s]
Evaluation, Loss: 51.6129, 0/1 Loss: 1.0000, Hamming Loss: 0.6564, EMR: 0.0000, Acc: 0.7397, F1: 0.5227, Precision: 0.4920, Recall: 0.5746

Epoch 9/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 9/20, Batch 263/264: 100%|█████████▉| 263/264 [01:17<00:00,  3.33it/s]
Epoch 9/20, Batch 264/264: 100%|█████████▉| 263/264 [01:17<00:00,  3.33it/s]
Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.46it/s]
Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.38it/s]
Epoch 9/20, Loss: 361.3521, 0/1 Loss: 0.9998, Hamming Loss: 0.7908, EMR: 0.0002, Acc: 0.5047, F1: 0.4072, Precision: 0.5073, Recall: 0.3937
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.98it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.73it/s]
Evaluation, Loss: 51.6368, 0/1 Loss: 1.0000, Hamming Loss: 0.6564, EMR: 0.0000, Acc: 0.7397, F1: 0.5227, Precision: 0.4920, Recall: 0.5746

Epoch 10/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 10/20, Batch 263/264: 100%|█████████▉| 263/264 [01:18<00:00,  3.32it/s]
Epoch 10/20, Batch 264/264: 100%|█████████▉| 263/264 [01:18<00:00,  3.32it/s]
Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.44it/s]
Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.37it/s]
Epoch 10/20, Loss: 361.2257, 0/1 Loss: 1.0000, Hamming Loss: 0.7920, EMR: 0.0000, Acc: 0.4992, F1: 0.4035, Precision: 0.5055, Recall: 0.3889
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.98it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.67it/s]
Evaluation, Loss: 51.6052, 0/1 Loss: 1.0000, Hamming Loss: 0.6564, EMR: 0.0000, Acc: 0.7397, F1: 0.5227, Precision: 0.4920, Recall: 0.5746

Epoch 11/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 11/20, Batch 263/264: 100%|█████████▉| 263/264 [01:18<00:00,  3.34it/s]
Epoch 11/20, Batch 264/264: 100%|█████████▉| 263/264 [01:18<00:00,  3.34it/s]
Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.48it/s]
Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.36it/s]
Epoch 11/20, Loss: 361.4070, 0/1 Loss: 0.9999, Hamming Loss: 0.7885, EMR: 0.0001, Acc: 0.5085, F1: 0.4090, Precision: 0.5055, Recall: 0.3972
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.95it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.71it/s]
Evaluation, Loss: 51.6019, 0/1 Loss: 1.0000, Hamming Loss: 0.6564, EMR: 0.0000, Acc: 0.7397, F1: 0.5227, Precision: 0.4920, Recall: 0.5746

Early stopping triggered!


bert-base-multilingual-cased
[00:20:06] task: task-2                                                                                 my_import.py:91
           model_type: simple                                                                           my_import.py:91
           model_name: bert-base-multilingual-cased                                                     my_import.py:91
           padding_len: 500                                                                             my_import.py:85
           batch_size: 32                                                                               my_import.py:91
           learning_rate: 0.001                                                                         my_import.py:91
           epochs: 20                                                                                   my_import.py:91
           device: cuda                                                                                 my_import.py:91
           saving_path: ./models/task_2/simple_bert-base-multilingual-cased                             my_import.py:91
           train_shape: (8444, 28)                                                                      my_import.py:91
           dev_shape: (1207, 28)                                                                        my_import.py:91
           test_shape: (2413, 28)                                                                       my_import.py:91

Training ...
Epoch 1/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 1/20, Batch 263/264: 100%|█████████▉| 263/264 [01:19<00:00,  3.13it/s]
Epoch 1/20, Batch 264/264: 100%|█████████▉| 263/264 [01:19<00:00,  3.13it/s]
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [01:19<00:00,  3.26it/s]
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [01:19<00:00,  3.31it/s]
Epoch 1/20, Loss: 362.2454, 0/1 Loss: 0.9998, Hamming Loss: 0.7403, EMR: 0.0002, Acc: 0.4555, F1: 0.4150, Precision: 0.5976, Recall: 0.3566
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.90it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.56it/s]
Evaluation, Loss: 51.8187, 0/1 Loss: 1.0000, Hamming Loss: 0.5868, EMR: 0.0000, Acc: 0.7227, F1: 0.5643, Precision: 0.5864, Recall: 0.5619
Saved the best model to path: ./models/task_2/simple_bert-base-multilingual-cased_0.pth

Epoch 2/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 2/20, Batch 263/264: 100%|█████████▉| 263/264 [01:21<00:00,  3.35it/s]
Epoch 2/20, Batch 264/264: 100%|█████████▉| 263/264 [01:21<00:00,  3.35it/s]
Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [01:21<00:00,  3.47it/s]
Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [01:21<00:00,  3.25it/s]
Epoch 2/20, Loss: 362.0474, 0/1 Loss: 1.0000, Hamming Loss: 0.7533, EMR: 0.0000, Acc: 0.4308, F1: 0.3972, Precision: 0.5915, Recall: 0.3364
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:11<00:00,  3.63it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:11<00:00,  3.45it/s]
Evaluation, Loss: 51.7326, 0/1 Loss: 1.0000, Hamming Loss: 0.5860, EMR: 0.0000, Acc: 0.7249, F1: 0.5646, Precision: 0.5825, Recall: 0.5635
Saved the best model to path: ./models/task_2/simple_bert-base-multilingual-cased_1.pth

Epoch 3/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 3/20, Batch 263/264: 100%|█████████▉| 263/264 [01:19<00:00,  3.33it/s]
Epoch 3/20, Batch 264/264: 100%|█████████▉| 263/264 [01:19<00:00,  3.33it/s]
Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [01:20<00:00,  3.46it/s]
Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [01:20<00:00,  3.30it/s]
Epoch 3/20, Loss: 362.2295, 0/1 Loss: 1.0000, Hamming Loss: 0.7571, EMR: 0.0000, Acc: 0.4257, F1: 0.3928, Precision: 0.5894, Recall: 0.3326
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.94it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.60it/s]
Evaluation, Loss: 51.6851, 0/1 Loss: 1.0000, Hamming Loss: 0.5860, EMR: 0.0000, Acc: 0.7249, F1: 0.5647, Precision: 0.5825, Recall: 0.5635
Saved the best model to path: ./models/task_2/simple_bert-base-multilingual-cased_2.pth

Epoch 4/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 4/20, Batch 263/264: 100%|█████████▉| 263/264 [01:17<00:00,  3.39it/s]
Epoch 4/20, Batch 264/264: 100%|█████████▉| 263/264 [01:17<00:00,  3.39it/s]
Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.54it/s]
Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.38it/s]
Epoch 4/20, Loss: 362.1204, 0/1 Loss: 1.0000, Hamming Loss: 0.7594, EMR: 0.0000, Acc: 0.4173, F1: 0.3869, Precision: 0.5939, Recall: 0.3263
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  4.00it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.69it/s]
Evaluation, Loss: 51.7504, 0/1 Loss: 1.0000, Hamming Loss: 0.5824, EMR: 0.0000, Acc: 0.7287, F1: 0.5679, Precision: 0.5858, Recall: 0.5665

Epoch 5/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 5/20, Batch 263/264: 100%|█████████▉| 263/264 [01:17<00:00,  3.39it/s]
Epoch 5/20, Batch 264/264: 100%|█████████▉| 263/264 [01:17<00:00,  3.39it/s]
Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.52it/s]
Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.40it/s]
Epoch 5/20, Loss: 362.1070, 0/1 Loss: 1.0000, Hamming Loss: 0.7525, EMR: 0.0000, Acc: 0.4243, F1: 0.3953, Precision: 0.6164, Recall: 0.3315
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.97it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.68it/s]
Evaluation, Loss: 51.8228, 0/1 Loss: 1.0000, Hamming Loss: 0.5816, EMR: 0.0000, Acc: 0.7307, F1: 0.5691, Precision: 0.5866, Recall: 0.5680

Epoch 6/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 6/20, Batch 263/264: 100%|█████████▉| 263/264 [01:17<00:00,  3.41it/s]
Epoch 6/20, Batch 264/264: 100%|█████████▉| 263/264 [01:17<00:00,  3.41it/s]
Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.54it/s]
Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.39it/s]
Epoch 6/20, Loss: 362.0197, 0/1 Loss: 1.0000, Hamming Loss: 0.7581, EMR: 0.0000, Acc: 0.4193, F1: 0.3894, Precision: 0.6022, Recall: 0.3286
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.97it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.68it/s]
Evaluation, Loss: 51.7517, 0/1 Loss: 1.0000, Hamming Loss: 0.5833, EMR: 0.0000, Acc: 0.7274, F1: 0.5672, Precision: 0.5853, Recall: 0.5657

Epoch 7/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 7/20, Batch 263/264: 100%|█████████▉| 263/264 [01:17<00:00,  3.40it/s]
Epoch 7/20, Batch 264/264: 100%|█████████▉| 263/264 [01:17<00:00,  3.40it/s]
Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.53it/s]
Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.39it/s]
Epoch 7/20, Loss: 361.8421, 0/1 Loss: 1.0000, Hamming Loss: 0.7503, EMR: 0.0000, Acc: 0.4256, F1: 0.3964, Precision: 0.6159, Recall: 0.3319
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.97it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.68it/s]
Evaluation, Loss: 51.7074, 0/1 Loss: 1.0000, Hamming Loss: 0.5822, EMR: 0.0000, Acc: 0.7287, F1: 0.5679, Precision: 0.5858, Recall: 0.5666

Epoch 8/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 8/20, Batch 263/264: 100%|█████████▉| 263/264 [01:17<00:00,  3.36it/s]
Epoch 8/20, Batch 264/264: 100%|█████████▉| 263/264 [01:17<00:00,  3.36it/s]
Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.50it/s]
Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.40it/s]
Epoch 8/20, Loss: 361.8551, 0/1 Loss: 1.0000, Hamming Loss: 0.7489, EMR: 0.0000, Acc: 0.4264, F1: 0.3986, Precision: 0.6267, Recall: 0.3333
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.97it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.68it/s]
Evaluation, Loss: 51.7144, 0/1 Loss: 1.0000, Hamming Loss: 0.5814, EMR: 0.0000, Acc: 0.7309, F1: 0.5692, Precision: 0.5867, Recall: 0.5681

Early stopping triggered!


distilbert-base-multilingual-cased
[00:32:19] task: task-2                                                                                 my_import.py:91
           model_type: simple                                                                           my_import.py:91
           model_name: distilbert-base-multilingual-cased                                               my_import.py:91
           padding_len: 500                                                                             my_import.py:85
           batch_size: 32                                                                               my_import.py:91
           learning_rate: 0.001                                                                         my_import.py:91
           epochs: 20                                                                                   my_import.py:91
           device: cuda                                                                                 my_import.py:91
           saving_path: ./models/task_2/simple_distilbert-base-multilingual-cased                       my_import.py:91
           train_shape: (8444, 28)                                                                      my_import.py:91
           dev_shape: (1207, 28)                                                                        my_import.py:91
           test_shape: (2413, 28)                                                                       my_import.py:91

Training ...
Epoch 1/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 1/20, Batch 263/264: 100%|█████████▉| 263/264 [00:45<00:00,  6.20it/s]
Epoch 1/20, Batch 264/264: 100%|█████████▉| 263/264 [00:45<00:00,  6.20it/s]
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [00:45<00:00,  6.45it/s]
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [00:45<00:00,  5.84it/s]
Epoch 1/20, Loss: 360.8931, 0/1 Loss: 1.0000, Hamming Loss: 0.7881, EMR: 0.0000, Acc: 0.4686, F1: 0.3917, Precision: 0.5144, Recall: 0.3656
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.16it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.64it/s]
Evaluation, Loss: 51.4708, 0/1 Loss: 1.0000, Hamming Loss: 0.6348, EMR: 0.0000, Acc: 0.7020, F1: 0.5220, Precision: 0.5195, Recall: 0.5437
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_0.pth

Epoch 2/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 2/20, Batch 263/264: 100%|█████████▉| 263/264 [00:42<00:00,  6.18it/s]
Epoch 2/20, Batch 264/264: 100%|█████████▉| 263/264 [00:42<00:00,  6.18it/s]
Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.23it/s]
Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.13it/s]
Epoch 2/20, Loss: 360.4199, 0/1 Loss: 1.0000, Hamming Loss: 0.7842, EMR: 0.0000, Acc: 0.4898, F1: 0.4032, Precision: 0.5217, Recall: 0.3827
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.13it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.56it/s]
Evaluation, Loss: 51.2456, 0/1 Loss: 1.0000, Hamming Loss: 0.6493, EMR: 0.0000, Acc: 0.7393, F1: 0.5252, Precision: 0.4973, Recall: 0.5742
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_1.pth

Epoch 3/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 3/20, Batch 263/264: 100%|█████████▉| 263/264 [00:43<00:00,  6.21it/s]
Epoch 3/20, Batch 264/264: 100%|█████████▉| 263/264 [00:43<00:00,  6.21it/s]
Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.39it/s]
Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.11it/s]
Epoch 3/20, Loss: 359.9684, 0/1 Loss: 1.0000, Hamming Loss: 0.7739, EMR: 0.0000, Acc: 0.5028, F1: 0.4134, Precision: 0.5325, Recall: 0.3932
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.91it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.56it/s]
Evaluation, Loss: 51.2692, 0/1 Loss: 1.0000, Hamming Loss: 0.6201, EMR: 0.0000, Acc: 0.7300, F1: 0.5373, Precision: 0.5272, Recall: 0.5665

Epoch 4/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 4/20, Batch 263/264: 100%|█████████▉| 263/264 [00:43<00:00,  5.97it/s]
Epoch 4/20, Batch 264/264: 100%|█████████▉| 263/264 [00:43<00:00,  5.97it/s]
Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.24it/s]
Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.00it/s]
Epoch 4/20, Loss: 359.6830, 0/1 Loss: 0.9995, Hamming Loss: 0.7734, EMR: 0.0005, Acc: 0.5197, F1: 0.4199, Precision: 0.5223, Recall: 0.4054
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.94it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.44it/s]
Evaluation, Loss: 51.1757, 0/1 Loss: 1.0000, Hamming Loss: 0.6282, EMR: 0.0000, Acc: 0.7397, F1: 0.5336, Precision: 0.5148, Recall: 0.5746
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_3.pth

Epoch 5/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 5/20, Batch 263/264: 100%|█████████▉| 263/264 [00:44<00:00,  5.99it/s]
Epoch 5/20, Batch 264/264: 100%|█████████▉| 263/264 [00:44<00:00,  5.99it/s]
Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  6.21it/s]
Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  5.98it/s]
Epoch 5/20, Loss: 359.4233, 0/1 Loss: 0.9999, Hamming Loss: 0.7771, EMR: 0.0001, Acc: 0.5143, F1: 0.4146, Precision: 0.5165, Recall: 0.4019
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.93it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.45it/s]
Evaluation, Loss: 51.1637, 0/1 Loss: 0.9992, Hamming Loss: 0.6371, EMR: 0.0008, Acc: 0.7397, F1: 0.5309, Precision: 0.5142, Recall: 0.5746
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_4.pth

Epoch 6/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 6/20, Batch 263/264: 100%|█████████▉| 263/264 [00:44<00:00,  6.03it/s]
Epoch 6/20, Batch 264/264: 100%|█████████▉| 263/264 [00:44<00:00,  6.03it/s]
Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  6.23it/s]
Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  5.93it/s]
Epoch 6/20, Loss: 359.1561, 0/1 Loss: 0.9996, Hamming Loss: 0.7796, EMR: 0.0004, Acc: 0.5157, F1: 0.4139, Precision: 0.5128, Recall: 0.4030
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.95it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.46it/s]
Evaluation, Loss: 51.0450, 0/1 Loss: 1.0000, Hamming Loss: 0.6843, EMR: 0.0000, Acc: 0.7397, F1: 0.5045, Precision: 0.4681, Recall: 0.5746
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_5.pth

Epoch 7/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 7/20, Batch 263/264: 100%|█████████▉| 263/264 [00:45<00:00,  6.03it/s]
Epoch 7/20, Batch 264/264: 100%|█████████▉| 263/264 [00:45<00:00,  6.03it/s]
Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [00:45<00:00,  6.25it/s]
Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [00:45<00:00,  5.83it/s]
Epoch 7/20, Loss: 359.1271, 0/1 Loss: 0.9995, Hamming Loss: 0.7825, EMR: 0.0005, Acc: 0.5118, F1: 0.4113, Precision: 0.5095, Recall: 0.4000
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:06<00:00,  6.85it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:06<00:00,  6.27it/s]
Evaluation, Loss: 51.0127, 0/1 Loss: 1.0000, Hamming Loss: 0.6297, EMR: 0.0000, Acc: 0.7397, F1: 0.5310, Precision: 0.5095, Recall: 0.5746
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_6.pth

Epoch 8/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 8/20, Batch 263/264: 100%|█████████▉| 263/264 [00:44<00:00,  6.02it/s]
Epoch 8/20, Batch 264/264: 100%|█████████▉| 263/264 [00:44<00:00,  6.02it/s]
Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  6.25it/s]
Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  5.93it/s]
Epoch 8/20, Loss: 358.9471, 0/1 Loss: 0.9996, Hamming Loss: 0.7799, EMR: 0.0004, Acc: 0.5115, F1: 0.4121, Precision: 0.5126, Recall: 0.3999
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:06<00:00,  6.89it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:06<00:00,  6.16it/s]
Evaluation, Loss: 51.0346, 0/1 Loss: 1.0000, Hamming Loss: 0.6493, EMR: 0.0000, Acc: 0.7397, F1: 0.5231, Precision: 0.4938, Recall: 0.5746

Epoch 9/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 9/20, Batch 263/264: 100%|█████████▉| 263/264 [00:44<00:00,  5.98it/s]
Epoch 9/20, Batch 264/264: 100%|█████████▉| 263/264 [00:44<00:00,  5.98it/s]
Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  6.20it/s]
Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  5.97it/s]
Epoch 9/20, Loss: 359.1625, 0/1 Loss: 0.9996, Hamming Loss: 0.7853, EMR: 0.0004, Acc: 0.5117, F1: 0.4111, Precision: 0.5084, Recall: 0.4008
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:06<00:00,  6.89it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:06<00:00,  6.09it/s]
Evaluation, Loss: 51.0517, 0/1 Loss: 0.9992, Hamming Loss: 0.7077, EMR: 0.0008, Acc: 0.7397, F1: 0.4946, Precision: 0.4541, Recall: 0.5746

Epoch 10/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 10/20, Batch 263/264: 100%|█████████▉| 263/264 [00:44<00:00,  5.88it/s]
Epoch 10/20, Batch 264/264: 100%|█████████▉| 263/264 [00:44<00:00,  5.88it/s]
Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  6.11it/s]
Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  5.92it/s]
Epoch 10/20, Loss: 358.8881, 0/1 Loss: 0.9998, Hamming Loss: 0.7903, EMR: 0.0002, Acc: 0.5057, F1: 0.4052, Precision: 0.5045, Recall: 0.3960
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:06<00:00,  6.90it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:06<00:00,  6.14it/s]
Evaluation, Loss: 50.8945, 0/1 Loss: 0.9992, Hamming Loss: 0.6717, EMR: 0.0008, Acc: 0.7397, F1: 0.5091, Precision: 0.4759, Recall: 0.5746
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_9.pth

Epoch 11/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 11/20, Batch 263/264: 100%|█████████▉| 263/264 [00:44<00:00,  5.87it/s]
Epoch 11/20, Batch 264/264: 100%|█████████▉| 263/264 [00:44<00:00,  5.87it/s]
Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  6.17it/s]
Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  5.94it/s]
Epoch 11/20, Loss: 358.8687, 0/1 Loss: 0.9996, Hamming Loss: 0.7888, EMR: 0.0004, Acc: 0.4992, F1: 0.4036, Precision: 0.5070, Recall: 0.3903
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.97it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.47it/s]
Evaluation, Loss: 50.9823, 0/1 Loss: 0.9992, Hamming Loss: 0.7214, EMR: 0.0008, Acc: 0.7397, F1: 0.4894, Precision: 0.4466, Recall: 0.5746

Epoch 12/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 12/20, Batch 263/264: 100%|█████████▉| 263/264 [00:44<00:00,  6.02it/s]
Epoch 12/20, Batch 264/264: 100%|█████████▉| 263/264 [00:44<00:00,  6.02it/s]
Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  6.22it/s]
Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  5.97it/s]
Epoch 12/20, Loss: 358.8742, 0/1 Loss: 0.9999, Hamming Loss: 0.7840, EMR: 0.0001, Acc: 0.5079, F1: 0.4098, Precision: 0.5099, Recall: 0.3974
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.90it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.44it/s]
Evaluation, Loss: 50.9080, 0/1 Loss: 1.0000, Hamming Loss: 0.6843, EMR: 0.0000, Acc: 0.7397, F1: 0.5052, Precision: 0.4682, Recall: 0.5746

Epoch 13/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 13/20, Batch 263/264: 100%|█████████▉| 263/264 [00:43<00:00,  5.99it/s]
Epoch 13/20, Batch 264/264: 100%|█████████▉| 263/264 [00:43<00:00,  5.99it/s]
Epoch 13/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.23it/s]
Epoch 13/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.01it/s]
Epoch 13/20, Loss: 358.3696, 0/1 Loss: 0.9995, Hamming Loss: 0.7852, EMR: 0.0005, Acc: 0.5072, F1: 0.4068, Precision: 0.5066, Recall: 0.3964
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.96it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.47it/s]
Evaluation, Loss: 50.8029, 0/1 Loss: 0.9992, Hamming Loss: 0.6622, EMR: 0.0008, Acc: 0.7397, F1: 0.5143, Precision: 0.4857, Recall: 0.5746
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_12.pth

Epoch 14/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 14/20, Batch 263/264: 100%|█████████▉| 263/264 [00:44<00:00,  5.99it/s]
Epoch 14/20, Batch 264/264: 100%|█████████▉| 263/264 [00:44<00:00,  5.99it/s]
Epoch 14/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  6.17it/s]
Epoch 14/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  5.96it/s]
Epoch 14/20, Loss: 358.7949, 0/1 Loss: 0.9998, Hamming Loss: 0.7889, EMR: 0.0002, Acc: 0.5020, F1: 0.4040, Precision: 0.5013, Recall: 0.3924
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.95it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.41it/s]
Evaluation, Loss: 50.9052, 0/1 Loss: 1.0000, Hamming Loss: 0.6330, EMR: 0.0000, Acc: 0.7397, F1: 0.5274, Precision: 0.5059, Recall: 0.5746

Epoch 15/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 15/20, Batch 263/264: 100%|█████████▉| 263/264 [00:43<00:00,  6.05it/s]
Epoch 15/20, Batch 264/264: 100%|█████████▉| 263/264 [00:43<00:00,  6.05it/s]
Epoch 15/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.29it/s]
Epoch 15/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.02it/s]
Epoch 15/20, Loss: 358.5372, 0/1 Loss: 0.9996, Hamming Loss: 0.7886, EMR: 0.0004, Acc: 0.5047, F1: 0.4044, Precision: 0.4994, Recall: 0.3944
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.95it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.41it/s]
Evaluation, Loss: 50.9553, 0/1 Loss: 1.0000, Hamming Loss: 0.7100, EMR: 0.0000, Acc: 0.7397, F1: 0.4951, Precision: 0.4532, Recall: 0.5746

Epoch 16/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 16/20, Batch 263/264: 100%|█████████▉| 263/264 [00:43<00:00,  6.00it/s]
Epoch 16/20, Batch 264/264: 100%|█████████▉| 263/264 [00:43<00:00,  6.00it/s]
Epoch 16/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.21it/s]
Epoch 16/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.01it/s]
Epoch 16/20, Loss: 358.5328, 0/1 Loss: 0.9998, Hamming Loss: 0.7852, EMR: 0.0002, Acc: 0.5066, F1: 0.4068, Precision: 0.5048, Recall: 0.3966
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.97it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.45it/s]
Evaluation, Loss: 50.9136, 0/1 Loss: 0.9992, Hamming Loss: 0.6514, EMR: 0.0008, Acc: 0.7397, F1: 0.5173, Precision: 0.4890, Recall: 0.5746

Epoch 17/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 17/20, Batch 263/264: 100%|█████████▉| 263/264 [00:43<00:00,  6.04it/s]
Epoch 17/20, Batch 264/264: 100%|█████████▉| 263/264 [00:43<00:00,  6.04it/s]
Epoch 17/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.30it/s]
Epoch 17/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.00it/s]
Epoch 17/20, Loss: 358.7551, 0/1 Loss: 0.9999, Hamming Loss: 0.7865, EMR: 0.0001, Acc: 0.5036, F1: 0.4055, Precision: 0.5068, Recall: 0.3935
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.97it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.47it/s]
Evaluation, Loss: 50.8607, 0/1 Loss: 1.0000, Hamming Loss: 0.6353, EMR: 0.0000, Acc: 0.7397, F1: 0.5263, Precision: 0.5044, Recall: 0.5746

Epoch 18/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 18/20, Batch 263/264: 100%|█████████▉| 263/264 [00:43<00:00,  5.89it/s]
Epoch 18/20, Batch 264/264: 100%|█████████▉| 263/264 [00:43<00:00,  5.89it/s]
Epoch 18/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  6.08it/s]
Epoch 18/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  6.00it/s]
Epoch 18/20, Loss: 358.3126, 0/1 Loss: 0.9998, Hamming Loss: 0.7861, EMR: 0.0002, Acc: 0.5070, F1: 0.4069, Precision: 0.5053, Recall: 0.3961
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.93it/s]
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.46it/s]
Evaluation, Loss: 50.8155, 0/1 Loss: 0.9992, Hamming Loss: 0.6709, EMR: 0.0008, Acc: 0.7397, F1: 0.5105, Precision: 0.4812, Recall: 0.5746

Early stopping triggered!


All commands completed!

