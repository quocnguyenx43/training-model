
======== Evaluating LSTM ========

lstm + vinai/phobert-base
[14:16:17] task: task-1                                                                                my_import.py:127
           model_type: lstm                                                                            my_import.py:127
           model_name: vinai/phobert-base                                                              my_import.py:127
           padding_len: 200                                                                            my_import.py:125
           batch_size: 32                                                                              my_import.py:127
           learning_rate: 0.001                                                                        my_import.py:127
           epochs: 10                                                                                  my_import.py:127
           fine_tune: True                                                                             my_import.py:127
           hidden_size: 128                                                                            my_import.py:127
           num_layers: 1                                                                               my_import.py:127
           device: cuda                                                                                my_import.py:127
           saving_path: ./models/task_1/lstm_phobert-base                                              my_import.py:127
           train_shape: (8444, 28)                                                                     my_import.py:127
           dev_shape: (1207, 28)                                                                       my_import.py:127
           test_shape: (2413, 28)                                                                      my_import.py:127

model_weight_path: ./models/task_1/lstm_phobert-base_15.pth
Loading model weight successfully!

Evaluation on dev test
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 35.5187, Acc: 0.5626, Precision: 0.5640, Recall: 0.5241, F1: 0.5289
Confusion Matrix:
[[396  56  67]
 [156 178  47]
 [167  35 105]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.55      0.76      0.64       519
     warning       0.66      0.47      0.55       381
     seeding       0.48      0.34      0.40       307

    accuracy                           0.56      1207
   macro avg       0.56      0.52      0.53      1207
weighted avg       0.57      0.56      0.55      1207

Evaluation on test test
Evaluation:   0%|          | 0/76 [00:00<?, ?it/s]
Evaluation, Loss: 71.1195, Acc: 0.5578, Precision: 0.5640, Recall: 0.5234, F1: 0.5239
Confusion Matrix:
[[789  91 119]
 [340 335 126]
 [318  73 222]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.55      0.79      0.65       999
     warning       0.67      0.42      0.52       801
     seeding       0.48      0.36      0.41       613

    accuracy                           0.56      2413
   macro avg       0.56      0.52      0.52      2413
weighted avg       0.57      0.56      0.54      2413

lstm + uitnlp/visobert
[14:16:47] task: task-1                                                                                my_import.py:127
           model_type: lstm                                                                            my_import.py:127
           model_name: uitnlp/visobert                                                                 my_import.py:127
           padding_len: 400                                                                            my_import.py:125
           batch_size: 32                                                                              my_import.py:127
           learning_rate: 0.001                                                                        my_import.py:127
           epochs: 10                                                                                  my_import.py:127
           fine_tune: True                                                                             my_import.py:127
           hidden_size: 128                                                                            my_import.py:127
           num_layers: 1                                                                               my_import.py:127
           device: cuda                                                                                my_import.py:127
           saving_path: ./models/task_1/lstm_visobert                                                  my_import.py:127
           train_shape: (8444, 28)                                                                     my_import.py:127
           dev_shape: (1207, 28)                                                                       my_import.py:127
           test_shape: (2413, 28)                                                                      my_import.py:127

Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/visobert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

model_weight_path: ./models/task_1/lstm_visobert_15.pth
Loading model weight successfully!

Evaluation on dev test
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 34.2405, Acc: 0.5973, Precision: 0.6031, Recall: 0.5583, F1: 0.5646
Confusion Matrix:
[[416  36  67]
 [146 190  45]
 [155  37 115]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.58      0.80      0.67       519
     warning       0.72      0.50      0.59       381
     seeding       0.51      0.37      0.43       307

    accuracy                           0.60      1207
   macro avg       0.60      0.56      0.56      1207
weighted avg       0.61      0.60      0.59      1207

Evaluation on test test
Evaluation:   0%|          | 0/76 [00:00<?, ?it/s]
Evaluation, Loss: 68.6966, Acc: 0.5877, Precision: 0.5900, Recall: 0.5583, F1: 0.5610
Confusion Matrix:
[[782  90 127]
 [283 380 138]
 [283  74 256]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.58      0.78      0.67       999
     warning       0.70      0.47      0.57       801
     seeding       0.49      0.42      0.45       613

    accuracy                           0.59      2413
   macro avg       0.59      0.56      0.56      2413
weighted avg       0.60      0.59      0.58      2413

lstm + uitnlp/CafeBERT
[14:17:28] task: task-1                                                                                my_import.py:127
           model_type: lstm                                                                            my_import.py:127
           model_name: uitnlp/CafeBERT                                                                 my_import.py:127
           padding_len: 300                                                                            my_import.py:125
           batch_size: 32                                                                              my_import.py:127
           learning_rate: 0.001                                                                        my_import.py:127
           epochs: 10                                                                                  my_import.py:127
           fine_tune: True                                                                             my_import.py:127
           hidden_size: 128                                                                            my_import.py:127
           num_layers: 1                                                                               my_import.py:127
           device: cuda                                                                                my_import.py:127
           saving_path: ./models/task_1/lstm_CafeBERT                                                  my_import.py:127
           train_shape: (8444, 28)                                                                     my_import.py:127
           dev_shape: (1207, 28)                                                                       my_import.py:127
           test_shape: (2413, 28)                                                                      my_import.py:127

Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/CafeBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

model_weight_path: ./models/task_1/lstm_CafeBERT_19.pth
Loading model weight successfully!

Evaluation on dev test
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 37.4139, Acc: 0.5253, Precision: 0.5204, Recall: 0.4546, F1: 0.4309
Confusion Matrix:
[[461  25  33]
 [213 139  29]
 [239  34  34]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.50      0.89      0.64       519
     warning       0.70      0.36      0.48       381
     seeding       0.35      0.11      0.17       307

    accuracy                           0.53      1207
   macro avg       0.52      0.45      0.43      1207
weighted avg       0.53      0.53      0.47      1207

Evaluation on test test
Evaluation:   0%|          | 0/76 [00:00<?, ?it/s]
Evaluation, Loss: 73.4774, Acc: 0.5367, Precision: 0.5631, Recall: 0.4773, F1: 0.4582
Confusion Matrix:
[[903  40  56]
 [442 291  68]
 [448  64 101]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.50      0.90      0.65       999
     warning       0.74      0.36      0.49       801
     seeding       0.45      0.16      0.24       613

    accuracy                           0.54      2413
   macro avg       0.56      0.48      0.46      2413
weighted avg       0.57      0.54      0.49      2413

lstm + xlm-roberta-base
[14:18:36] task: task-1                                                                                my_import.py:127
           model_type: lstm                                                                            my_import.py:127
           model_name: xlm-roberta-base                                                                my_import.py:127
           padding_len: 500                                                                            my_import.py:125
           batch_size: 32                                                                              my_import.py:127
           learning_rate: 0.001                                                                        my_import.py:127
           epochs: 10                                                                                  my_import.py:127
           fine_tune: True                                                                             my_import.py:127
           hidden_size: 128                                                                            my_import.py:127
           num_layers: 1                                                                               my_import.py:127
           device: cuda                                                                                my_import.py:127
           saving_path: ./models/task_1/lstm_xlm-roberta-base                                          my_import.py:127
           train_shape: (8444, 28)                                                                     my_import.py:127
           dev_shape: (1207, 28)                                                                       my_import.py:127
           test_shape: (2413, 28)                                                                      my_import.py:127

model_weight_path: ./models/task_1/lstm_xlm-roberta-base_1.pth
Loading model weight successfully!

Evaluation on dev test
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 40.8815, Acc: 0.4300, Precision: 0.1433, Recall: 0.3333, F1: 0.2005
Confusion Matrix:
[[519   0   0]
 [381   0   0]
 [307   0   0]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.43      1.00      0.60       519
     warning       0.00      0.00      0.00       381
     seeding       0.00      0.00      0.00       307

    accuracy                           0.43      1207
   macro avg       0.14      0.33      0.20      1207
weighted avg       0.18      0.43      0.26      1207

Evaluation on test test
Evaluation:   0%|          | 0/76 [00:00<?, ?it/s]
Evaluation, Loss: 82.0609, Acc: 0.4140, Precision: 0.1380, Recall: 0.3333, F1: 0.1952
Confusion Matrix:
[[999   0   0]
 [801   0   0]
 [613   0   0]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.41      1.00      0.59       999
     warning       0.00      0.00      0.00       801
     seeding       0.00      0.00      0.00       613

    accuracy                           0.41      2413
   macro avg       0.14      0.33      0.20      2413
weighted avg       0.17      0.41      0.24      2413

lstm + bert-base-multilingual-cased
[14:19:20] task: task-1                                                                                my_import.py:127
           model_type: lstm                                                                            my_import.py:127
           model_name: bert-base-multilingual-cased                                                    my_import.py:127
           padding_len: 500                                                                            my_import.py:125
           batch_size: 32                                                                              my_import.py:127
           learning_rate: 0.001                                                                        my_import.py:127
           epochs: 10                                                                                  my_import.py:127
           fine_tune: True                                                                             my_import.py:127
           hidden_size: 128                                                                            my_import.py:127
           num_layers: 1                                                                               my_import.py:127
           device: cuda                                                                                my_import.py:127
           saving_path: ./models/task_1/lstm_bert-base-multilingual-cased                              my_import.py:127
           train_shape: (8444, 28)                                                                     my_import.py:127
           dev_shape: (1207, 28)                                                                       my_import.py:127
           test_shape: (2413, 28)                                                                      my_import.py:127

model_weight_path: ./models/task_1/lstm_bert-base-multilingual-cased_4.pth
Loading model weight successfully!

Evaluation on dev test
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 39.9326, Acc: 0.4598, Precision: 0.2948, Recall: 0.3809, F1: 0.3140
Confusion Matrix:
[[450  69   0]
 [276 105   0]
 [226  81   0]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.47      0.87      0.61       519
     warning       0.41      0.28      0.33       381
     seeding       0.00      0.00      0.00       307

    accuracy                           0.46      1207
   macro avg       0.29      0.38      0.31      1207
weighted avg       0.33      0.46      0.37      1207

Evaluation on test test
Evaluation:   0%|          | 0/76 [00:00<?, ?it/s]
Evaluation, Loss: 79.3514, Acc: 0.4505, Precision: 0.2956, Recall: 0.3806, F1: 0.3103
Confusion Matrix:
[[870 129   0]
 [584 217   0]
 [456 157   0]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.46      0.87      0.60       999
     warning       0.43      0.27      0.33       801
     seeding       0.00      0.00      0.00       613

    accuracy                           0.45      2413
   macro avg       0.30      0.38      0.31      2413
weighted avg       0.33      0.45      0.36      2413

lstm + distilbert-base-multilingual-cased
[14:20:06] task: task-1                                                                                my_import.py:127
           model_type: lstm                                                                            my_import.py:127
           model_name: distilbert-base-multilingual-cased                                              my_import.py:127
           padding_len: 500                                                                            my_import.py:125
           batch_size: 32                                                                              my_import.py:127
           learning_rate: 0.001                                                                        my_import.py:127
           epochs: 10                                                                                  my_import.py:127
           fine_tune: True                                                                             my_import.py:127
           hidden_size: 128                                                                            my_import.py:127
           num_layers: 1                                                                               my_import.py:127
           device: cuda                                                                                my_import.py:127
           saving_path: ./models/task_1/lstm_distilbert-base-multilingual-cased                        my_import.py:127
           train_shape: (8444, 28)                                                                     my_import.py:127
           dev_shape: (1207, 28)                                                                       my_import.py:127
           test_shape: (2413, 28)                                                                      my_import.py:127

model_weight_path: ./models/task_1/lstm_distilbert-base-multilingual-cased_12.pth
Loading model weight successfully!

Evaluation on dev test
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 34.4667, Acc: 0.5750, Precision: 0.5712, Recall: 0.5369, F1: 0.5417
Confusion Matrix:
[[398  49  72]
 [136 190  55]
 [164  37 106]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.57      0.77      0.65       519
     warning       0.69      0.50      0.58       381
     seeding       0.45      0.35      0.39       307

    accuracy                           0.57      1207
   macro avg       0.57      0.54      0.54      1207
weighted avg       0.58      0.57      0.56      1207

Evaluation on test test
Evaluation:   0%|          | 0/76 [00:00<?, ?it/s]
Evaluation, Loss: 68.6397, Acc: 0.5918, Precision: 0.5915, Recall: 0.5610, F1: 0.5639
Confusion Matrix:
[[786  80 133]
 [286 395 120]
 [274  92 247]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.58      0.79      0.67       999
     warning       0.70      0.49      0.58       801
     seeding       0.49      0.40      0.44       613

    accuracy                           0.59      2413
   macro avg       0.59      0.56      0.56      2413
weighted avg       0.60      0.59      0.58      2413



======== Evaluating CNN ========

cnn + vinai/phobert-base
[20:58:03] task: task-1                                                                                my_import.py:127
           model_type: cnn                                                                             my_import.py:127
           model_name: vinai/phobert-base                                                              my_import.py:127
           padding_len: 200                                                                            my_import.py:125
           batch_size: 32                                                                              my_import.py:127
           learning_rate: 0.001                                                                        my_import.py:127
           epochs: 10                                                                                  my_import.py:127
           fine_tune: True                                                                             my_import.py:127
           num_channels: 768                                                                           my_import.py:127
           kernel_size: 256                                                                            my_import.py:127
           padding: 32                                                                                 my_import.py:127
           device: cuda                                                                                my_import.py:127
           saving_path: ./models/task_1/cnn_phobert-base                                               my_import.py:127
           train_shape: (8444, 28)                                                                     my_import.py:127
           dev_shape: (1207, 28)                                                                       my_import.py:127
           test_shape: (2413, 28)                                                                      my_import.py:127

model_weight_path: ./models/task_1/cnn_phobert-base_11.pth
Loading model weight successfully!

Evaluation on dev test
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 36.3678, Acc: 0.5476, Precision: 0.5499, Recall: 0.4855, F1: 0.4729
Confusion Matrix:
[[447  45  27]
 [189 161  31]
 [210  44  53]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.53      0.86      0.65       519
     warning       0.64      0.42      0.51       381
     seeding       0.48      0.17      0.25       307

    accuracy                           0.55      1207
   macro avg       0.55      0.49      0.47      1207
weighted avg       0.55      0.55      0.51      1207

Evaluation on test test
Evaluation:   0%|          | 0/76 [00:00<?, ?it/s]
Evaluation, Loss: 73.4394, Acc: 0.5213, Precision: 0.5141, Recall: 0.4647, F1: 0.4447
Confusion Matrix:
[[845 103  51]
 [399 328  74]
 [444  84  85]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.50      0.85      0.63       999
     warning       0.64      0.41      0.50       801
     seeding       0.40      0.14      0.21       613

    accuracy                           0.52      2413
   macro avg       0.51      0.46      0.44      2413
weighted avg       0.52      0.52      0.48      2413

cnn + uitnlp/visobert
[20:58:40] task: task-1                                                                                my_import.py:127
           model_type: cnn                                                                             my_import.py:127
           model_name: uitnlp/visobert                                                                 my_import.py:127
           padding_len: 400                                                                            my_import.py:125
           batch_size: 32                                                                              my_import.py:127
           learning_rate: 0.001                                                                        my_import.py:127
           epochs: 10                                                                                  my_import.py:127
           fine_tune: True                                                                             my_import.py:127
           num_channels: 768                                                                           my_import.py:127
           kernel_size: 256                                                                            my_import.py:127
           padding: 32                                                                                 my_import.py:127
           device: cuda                                                                                my_import.py:127
           saving_path: ./models/task_1/cnn_visobert                                                   my_import.py:127
           train_shape: (8444, 28)                                                                     my_import.py:127
           dev_shape: (1207, 28)                                                                       my_import.py:127
           test_shape: (2413, 28)                                                                      my_import.py:127

Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/visobert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

model_weight_path: ./models/task_1/cnn_visobert_13.pth
Loading model weight successfully!

Evaluation on dev test
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 34.5105, Acc: 0.5891, Precision: 0.6063, Recall: 0.5649, F1: 0.5677
Confusion Matrix:
[[388  25 106]
 [133 166  82]
 [123  27 157]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.60      0.75      0.67       519
     warning       0.76      0.44      0.55       381
     seeding       0.46      0.51      0.48       307

    accuracy                           0.59      1207
   macro avg       0.61      0.56      0.57      1207
weighted avg       0.62      0.59      0.58      1207

Evaluation on test test
Evaluation:   0%|          | 0/76 [00:00<?, ?it/s]
Evaluation, Loss: 68.8337, Acc: 0.5756, Precision: 0.5956, Recall: 0.5620, F1: 0.5585
Confusion Matrix:
[[722  61 216]
 [280 326 195]
 [220  52 341]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.59      0.72      0.65       999
     warning       0.74      0.41      0.53       801
     seeding       0.45      0.56      0.50       613

    accuracy                           0.58      2413
   macro avg       0.60      0.56      0.56      2413
weighted avg       0.61      0.58      0.57      2413

cnn + uitnlp/CafeBERT
[20:59:22] task: task-1                                                                                my_import.py:127
           model_type: cnn                                                                             my_import.py:127
           model_name: uitnlp/CafeBERT                                                                 my_import.py:127
           padding_len: 300                                                                            my_import.py:125
           batch_size: 32                                                                              my_import.py:127
           learning_rate: 0.001                                                                        my_import.py:127
           epochs: 10                                                                                  my_import.py:127
           fine_tune: True                                                                             my_import.py:127
           num_channels: 768                                                                           my_import.py:127
           kernel_size: 256                                                                            my_import.py:127
           padding: 32                                                                                 my_import.py:127
           device: cuda                                                                                my_import.py:127
           saving_path: ./models/task_1/cnn_CafeBERT                                                   my_import.py:127
           train_shape: (8444, 28)                                                                     my_import.py:127
           dev_shape: (1207, 28)                                                                       my_import.py:127
           test_shape: (2413, 28)                                                                      my_import.py:127

Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/CafeBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

model_weight_path: ./models/task_1/cnn_CafeBERT_0.pth
Loading model weight successfully!

Evaluation on dev test
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 40.6712, Acc: 0.4300, Precision: 0.1433, Recall: 0.3333, F1: 0.2005
Confusion Matrix:
[[519   0   0]
 [381   0   0]
 [307   0   0]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.43      1.00      0.60       519
     warning       0.00      0.00      0.00       381
     seeding       0.00      0.00      0.00       307

    accuracy                           0.43      1207
   macro avg       0.14      0.33      0.20      1207
weighted avg       0.18      0.43      0.26      1207

Evaluation on test test
Evaluation:   0%|          | 0/76 [00:00<?, ?it/s]
Evaluation, Loss: 81.6202, Acc: 0.4140, Precision: 0.1380, Recall: 0.3333, F1: 0.1952
Confusion Matrix:
[[999   0   0]
 [801   0   0]
 [613   0   0]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.41      1.00      0.59       999
     warning       0.00      0.00      0.00       801
     seeding       0.00      0.00      0.00       613

    accuracy                           0.41      2413
   macro avg       0.14      0.33      0.20      2413
weighted avg       0.17      0.41      0.24      2413

cnn + xlm-roberta-base
[21:00:42] task: task-1                                                                                my_import.py:127
           model_type: cnn                                                                             my_import.py:127
           model_name: xlm-roberta-base                                                                my_import.py:127
           padding_len: 500                                                                            my_import.py:125
           batch_size: 32                                                                              my_import.py:127
           learning_rate: 0.001                                                                        my_import.py:127
           epochs: 10                                                                                  my_import.py:127
           fine_tune: True                                                                             my_import.py:127
           num_channels: 768                                                                           my_import.py:127
           kernel_size: 256                                                                            my_import.py:127
           padding: 32                                                                                 my_import.py:127
           device: cuda                                                                                my_import.py:127
           saving_path: ./models/task_1/cnn_xlm-roberta-base                                           my_import.py:127
           train_shape: (8444, 28)                                                                     my_import.py:127
           dev_shape: (1207, 28)                                                                       my_import.py:127
           test_shape: (2413, 28)                                                                      my_import.py:127

model_weight_path: ./models/task_1/cnn_xlm-roberta-base_6.pth
Loading model weight successfully!

Evaluation on dev test
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 40.8991, Acc: 0.4300, Precision: 0.1433, Recall: 0.3333, F1: 0.2005
Confusion Matrix:
[[519   0   0]
 [381   0   0]
 [307   0   0]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.43      1.00      0.60       519
     warning       0.00      0.00      0.00       381
     seeding       0.00      0.00      0.00       307

    accuracy                           0.43      1207
   macro avg       0.14      0.33      0.20      1207
weighted avg       0.18      0.43      0.26      1207

Evaluation on test test
Evaluation:   0%|          | 0/76 [00:00<?, ?it/s]
Evaluation, Loss: 82.1675, Acc: 0.4140, Precision: 0.1380, Recall: 0.3333, F1: 0.1952
Confusion Matrix:
[[999   0   0]
 [801   0   0]
 [613   0   0]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.41      1.00      0.59       999
     warning       0.00      0.00      0.00       801
     seeding       0.00      0.00      0.00       613

    accuracy                           0.41      2413
   macro avg       0.14      0.33      0.20      2413
weighted avg       0.17      0.41      0.24      2413

cnn + bert-base-multilingual-cased
[21:01:33] task: task-1                                                                                my_import.py:127
           model_type: cnn                                                                             my_import.py:127
           model_name: bert-base-multilingual-cased                                                    my_import.py:127
           padding_len: 500                                                                            my_import.py:125
           batch_size: 32                                                                              my_import.py:127
           learning_rate: 0.001                                                                        my_import.py:127
           epochs: 10                                                                                  my_import.py:127
           fine_tune: True                                                                             my_import.py:127
           num_channels: 768                                                                           my_import.py:127
           kernel_size: 256                                                                            my_import.py:127
           padding: 32                                                                                 my_import.py:127
           device: cuda                                                                                my_import.py:127
           saving_path: ./models/task_1/cnn_bert-base-multilingual-cased                               my_import.py:127
           train_shape: (8444, 28)                                                                     my_import.py:127
           dev_shape: (1207, 28)                                                                       my_import.py:127
           test_shape: (2413, 28)                                                                      my_import.py:127

model_weight_path: ./models/task_1/cnn_bert-base-multilingual-cased_14.pth
Loading model weight successfully!

Evaluation on dev test
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 40.1761, Acc: 0.4474, Precision: 0.3455, Recall: 0.3547, F1: 0.2539
Confusion Matrix:
[[506  13   0]
 [347  34   0]
 [297  10   0]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.44      0.97      0.61       519
     warning       0.60      0.09      0.16       381
     seeding       0.00      0.00      0.00       307

    accuracy                           0.45      1207
   macro avg       0.35      0.35      0.25      1207
weighted avg       0.38      0.45      0.31      1207

Evaluation on test test
Evaluation:   0%|          | 0/76 [00:00<?, ?it/s]
Evaluation, Loss: 79.8387, Acc: 0.4393, Precision: 0.3540, Recall: 0.3598, F1: 0.2532
Confusion Matrix:
[[986  13   0]
 [727  74   0]
 [583  30   0]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.43      0.99      0.60       999
     warning       0.63      0.09      0.16       801
     seeding       0.00      0.00      0.00       613

    accuracy                           0.44      2413
   macro avg       0.35      0.36      0.25      2413
weighted avg       0.39      0.44      0.30      2413

cnn + distilbert-base-multilingual-cased
[21:02:20] task: task-1                                                                                my_import.py:127
           model_type: cnn                                                                             my_import.py:127
           model_name: distilbert-base-multilingual-cased                                              my_import.py:127
           padding_len: 500                                                                            my_import.py:125
           batch_size: 32                                                                              my_import.py:127
           learning_rate: 0.001                                                                        my_import.py:127
           epochs: 10                                                                                  my_import.py:127
           fine_tune: True                                                                             my_import.py:127
           num_channels: 768                                                                           my_import.py:127
           kernel_size: 256                                                                            my_import.py:127
           padding: 32                                                                                 my_import.py:127
           device: cuda                                                                                my_import.py:127
           saving_path: ./models/task_1/cnn_distilbert-base-multilingual-cased                         my_import.py:127
           train_shape: (8444, 28)                                                                     my_import.py:127
           dev_shape: (1207, 28)                                                                       my_import.py:127
           test_shape: (2413, 28)                                                                      my_import.py:127

model_weight_path: ./models/task_1/cnn_distilbert-base-multilingual-cased_15.pth
Loading model weight successfully!

Evaluation on dev test
Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 36.7766, Acc: 0.5261, Precision: 0.5170, Recall: 0.4729, F1: 0.4669
Confusion Matrix:
[[408  55  56]
 [179 169  33]
 [211  38  58]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.51      0.79      0.62       519
     warning       0.65      0.44      0.53       381
     seeding       0.39      0.19      0.26       307

    accuracy                           0.53      1207
   macro avg       0.52      0.47      0.47      1207
weighted avg       0.52      0.53      0.50      1207

Evaluation on test test
Evaluation:   0%|          | 0/76 [00:00<?, ?it/s]
Evaluation, Loss: 71.9290, Acc: 0.5557, Precision: 0.5630, Recall: 0.5077, F1: 0.5018
Confusion Matrix:
[[829  93  77]
 [364 371  66]
 [378  94 141]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.53      0.83      0.65       999
     warning       0.66      0.46      0.55       801
     seeding       0.50      0.23      0.31       613

    accuracy                           0.56      2413
   macro avg       0.56      0.51      0.50      2413
weighted avg       0.57      0.56      0.53      2413

