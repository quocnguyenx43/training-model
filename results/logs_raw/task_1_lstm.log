Training LSTM
lstm + vinai/phobert-base

[12:04:46] task: task-1                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: vinai/phobert-base                                                              my_import.py:133
           padding_len: 256                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_1/lstm_phobert-base                                              my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

Training ...
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [00:39<00:00,  6.68it/s]
Epoch 1/20, Loss: 279.7168, Acc: 0.4492, Precision: 0.3342, Recall: 0.3633, F1: 0.2949
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.29it/s]
Evaluation, Loss: 40.3058, Acc: 0.4573, Precision: 0.3285, Recall: 0.3710, F1: 0.2772
Saved the best model to path: ./models/task_1/lstm_phobert-base_0.pth

Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  7.01it/s]
Epoch 2/20, Loss: 270.9827, Acc: 0.4777, Precision: 0.4476, Recall: 0.4058, F1: 0.3713
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.51it/s]
Evaluation, Loss: 39.0012, Acc: 0.4788, Precision: 0.5160, Recall: 0.4058, F1: 0.3552
Saved the best model to path: ./models/task_1/lstm_phobert-base_1.pth

Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  6.98it/s]
Epoch 3/20, Loss: 267.4346, Acc: 0.4871, Precision: 0.4894, Recall: 0.4306, F1: 0.4017
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.50it/s]
Evaluation, Loss: 39.4139, Acc: 0.4921, Precision: 0.5518, Recall: 0.4316, F1: 0.3951

Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  6.97it/s]
Epoch 4/20, Loss: 264.0891, Acc: 0.5003, Precision: 0.4966, Recall: 0.4484, F1: 0.4334
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.52it/s]
Evaluation, Loss: 37.4292, Acc: 0.4971, Precision: 0.4947, Recall: 0.4465, F1: 0.4307
Saved the best model to path: ./models/task_1/lstm_phobert-base_3.pth

Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  6.98it/s]
Epoch 5/20, Loss: 257.7671, Acc: 0.5272, Precision: 0.5145, Recall: 0.4822, F1: 0.4821
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.46it/s]
Evaluation, Loss: 36.8271, Acc: 0.5394, Precision: 0.5926, Recall: 0.4799, F1: 0.4680
Saved the best model to path: ./models/task_1/lstm_phobert-base_4.pth

Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.92it/s]
Epoch 6/20, Loss: 255.8279, Acc: 0.5341, Precision: 0.5267, Recall: 0.4887, F1: 0.4890
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.29it/s]
Evaluation, Loss: 36.7886, Acc: 0.5494, Precision: 0.5794, Recall: 0.4970, F1: 0.4925
Saved the best model to path: ./models/task_1/lstm_phobert-base_5.pth

Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.91it/s]
Epoch 7/20, Loss: 252.0427, Acc: 0.5500, Precision: 0.5451, Recall: 0.5015, F1: 0.5017
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.44it/s]
Evaluation, Loss: 36.4715, Acc: 0.5320, Precision: 0.5226, Recall: 0.5167, F1: 0.5188
Saved the best model to path: ./models/task_1/lstm_phobert-base_6.pth

Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.92it/s]
Epoch 8/20, Loss: 251.9120, Acc: 0.5490, Precision: 0.5393, Recall: 0.5026, F1: 0.5035
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.32it/s]
Evaluation, Loss: 36.9164, Acc: 0.5344, Precision: 0.6181, Recall: 0.4650, F1: 0.4393

Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  6.95it/s]
Epoch 9/20, Loss: 250.6884, Acc: 0.5495, Precision: 0.5421, Recall: 0.5037, F1: 0.5044
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.48it/s]
Evaluation, Loss: 35.7367, Acc: 0.5510, Precision: 0.5631, Recall: 0.5057, F1: 0.5064
Saved the best model to path: ./models/task_1/lstm_phobert-base_8.pth

Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  6.95it/s]
Epoch 10/20, Loss: 248.8185, Acc: 0.5567, Precision: 0.5526, Recall: 0.5116, F1: 0.5133
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.46it/s]
Evaluation, Loss: 36.2334, Acc: 0.5237, Precision: 0.5140, Recall: 0.5026, F1: 0.5058

Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.95it/s]
Epoch 11/20, Loss: 247.8767, Acc: 0.5594, Precision: 0.5546, Recall: 0.5149, F1: 0.5177
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.46it/s]
Evaluation, Loss: 35.7141, Acc: 0.5577, Precision: 0.5693, Recall: 0.5197, F1: 0.5242
Saved the best model to path: ./models/task_1/lstm_phobert-base_10.pth

Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  6.96it/s]
Epoch 12/20, Loss: 248.0716, Acc: 0.5583, Precision: 0.5542, Recall: 0.5117, F1: 0.5140
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.43it/s]
Evaluation, Loss: 36.8633, Acc: 0.5286, Precision: 0.5683, Recall: 0.4721, F1: 0.4594

Epoch 13/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  6.98it/s]
Epoch 13/20, Loss: 246.6584, Acc: 0.5643, Precision: 0.5595, Recall: 0.5170, F1: 0.5192
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.49it/s]
Evaluation, Loss: 35.4094, Acc: 0.5627, Precision: 0.5686, Recall: 0.5302, F1: 0.5343
Saved the best model to path: ./models/task_1/lstm_phobert-base_12.pth

Epoch 14/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  6.97it/s]
Epoch 14/20, Loss: 245.6410, Acc: 0.5639, Precision: 0.5591, Recall: 0.5211, F1: 0.5238
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.46it/s]
Evaluation, Loss: 36.1505, Acc: 0.5527, Precision: 0.5894, Recall: 0.4971, F1: 0.4890

Epoch 15/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  6.97it/s]
Epoch 15/20, Loss: 245.3756, Acc: 0.5644, Precision: 0.5609, Recall: 0.5194, F1: 0.5222
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.48it/s]
Evaluation, Loss: 36.5422, Acc: 0.5527, Precision: 0.5842, Recall: 0.5041, F1: 0.5042

Epoch 16/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  6.98it/s]
Epoch 16/20, Loss: 245.8284, Acc: 0.5693, Precision: 0.5667, Recall: 0.5264, F1: 0.5298
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.47it/s]
Evaluation, Loss: 35.2904, Acc: 0.5577, Precision: 0.5772, Recall: 0.5166, F1: 0.5204
Saved the best model to path: ./models/task_1/lstm_phobert-base_15.pth

Epoch 17/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  6.96it/s]
Epoch 17/20, Loss: 243.5018, Acc: 0.5695, Precision: 0.5678, Recall: 0.5237, F1: 0.5270
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.45it/s]
Evaluation, Loss: 35.5614, Acc: 0.5568, Precision: 0.5910, Recall: 0.5050, F1: 0.5026

Epoch 18/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  6.96it/s]
Epoch 18/20, Loss: 243.3952, Acc: 0.5751, Precision: 0.5724, Recall: 0.5331, F1: 0.5367
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.12it/s]
Evaluation, Loss: 35.4406, Acc: 0.5660, Precision: 0.5963, Recall: 0.5191, F1: 0.5210

Epoch 19/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  6.97it/s]
Epoch 19/20, Loss: 242.0248, Acc: 0.5746, Precision: 0.5718, Recall: 0.5307, F1: 0.5345
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.21it/s]
Evaluation, Loss: 36.6184, Acc: 0.5394, Precision: 0.5890, Recall: 0.4798, F1: 0.4673

Epoch 20/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  6.95it/s]
Epoch 20/20, Loss: 242.9236, Acc: 0.5745, Precision: 0.5750, Recall: 0.5273, F1: 0.5305
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.46it/s]
Evaluation, Loss: 36.3016, Acc: 0.5461, Precision: 0.5897, Recall: 0.4891, F1: 0.4771

lstm + uitnlp/visobert

[12:19:34] task: task-1                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: uitnlp/visobert                                                                 my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_1/lstm_visobert                                                  my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/visobert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Training ...
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.38it/s]
Epoch 1/20, Loss: 278.3730, Acc: 0.4426, Precision: 0.3879, Recall: 0.3550, F1: 0.2792
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.73it/s]
Evaluation, Loss: 38.9317, Acc: 0.4506, Precision: 0.6198, Recall: 0.3863, F1: 0.3309
Saved the best model to path: ./models/task_1/lstm_visobert_0.pth

Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.39it/s]
Epoch 2/20, Loss: 269.1826, Acc: 0.4810, Precision: 0.4598, Recall: 0.4116, F1: 0.3827
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.72it/s]
Evaluation, Loss: 38.5822, Acc: 0.4797, Precision: 0.5477, Recall: 0.4117, F1: 0.3587
Saved the best model to path: ./models/task_1/lstm_visobert_1.pth

Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.38it/s]
Epoch 3/20, Loss: 259.1014, Acc: 0.5233, Precision: 0.5116, Recall: 0.4784, F1: 0.4762
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.74it/s]
Evaluation, Loss: 37.3534, Acc: 0.5178, Precision: 0.5781, Recall: 0.4829, F1: 0.4671
Saved the best model to path: ./models/task_1/lstm_visobert_2.pth

Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [01:16<00:00,  3.45it/s]
Epoch 4/20, Loss: 254.5029, Acc: 0.5383, Precision: 0.5351, Recall: 0.5011, F1: 0.5006
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.79it/s]
Evaluation, Loss: 36.7736, Acc: 0.5544, Precision: 0.6239, Recall: 0.4955, F1: 0.4845
Saved the best model to path: ./models/task_1/lstm_visobert_3.pth

Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.36it/s]
Epoch 5/20, Loss: 248.7684, Acc: 0.5528, Precision: 0.5546, Recall: 0.5120, F1: 0.5135
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.67it/s]
Evaluation, Loss: 35.6423, Acc: 0.5469, Precision: 0.5862, Recall: 0.5176, F1: 0.5156
Saved the best model to path: ./models/task_1/lstm_visobert_4.pth

Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [01:16<00:00,  3.44it/s]
Epoch 6/20, Loss: 245.4886, Acc: 0.5700, Precision: 0.5705, Recall: 0.5328, F1: 0.5357
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.79it/s]
Evaluation, Loss: 35.5653, Acc: 0.5651, Precision: 0.5942, Recall: 0.5144, F1: 0.5131
Saved the best model to path: ./models/task_1/lstm_visobert_5.pth

Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [01:16<00:00,  3.44it/s]
Epoch 7/20, Loss: 245.5661, Acc: 0.5683, Precision: 0.5636, Recall: 0.5317, F1: 0.5356
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.79it/s]
Evaluation, Loss: 35.6229, Acc: 0.5544, Precision: 0.5943, Recall: 0.5205, F1: 0.5199

Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [01:16<00:00,  3.45it/s]
Epoch 8/20, Loss: 243.7885, Acc: 0.5708, Precision: 0.5655, Recall: 0.5324, F1: 0.5357
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.79it/s]
Evaluation, Loss: 35.6312, Acc: 0.5610, Precision: 0.6069, Recall: 0.5287, F1: 0.5238

Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [01:16<00:00,  3.45it/s]
Epoch 9/20, Loss: 242.5250, Acc: 0.5734, Precision: 0.5752, Recall: 0.5353, F1: 0.5387
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.80it/s]
Evaluation, Loss: 34.7668, Acc: 0.5651, Precision: 0.5962, Recall: 0.5396, F1: 0.5411
Saved the best model to path: ./models/task_1/lstm_visobert_8.pth

Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [01:16<00:00,  3.45it/s]
Epoch 10/20, Loss: 240.8145, Acc: 0.5796, Precision: 0.5762, Recall: 0.5389, F1: 0.5431
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.79it/s]
Evaluation, Loss: 34.5300, Acc: 0.5734, Precision: 0.5726, Recall: 0.5386, F1: 0.5428
Saved the best model to path: ./models/task_1/lstm_visobert_9.pth

Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [01:16<00:00,  3.45it/s]
Epoch 11/20, Loss: 240.2176, Acc: 0.5777, Precision: 0.5746, Recall: 0.5412, F1: 0.5448
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.79it/s]
Evaluation, Loss: 34.5910, Acc: 0.5784, Precision: 0.6026, Recall: 0.5478, F1: 0.5492

Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [01:16<00:00,  3.45it/s]
Epoch 12/20, Loss: 238.3574, Acc: 0.5868, Precision: 0.5823, Recall: 0.5496, F1: 0.5534
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.79it/s]
Evaluation, Loss: 34.1515, Acc: 0.5934, Precision: 0.6006, Recall: 0.5589, F1: 0.5630
Saved the best model to path: ./models/task_1/lstm_visobert_11.pth

Epoch 13/20, Batch 264/264: 100%|██████████| 264/264 [01:16<00:00,  3.44it/s]
Epoch 13/20, Loss: 239.7746, Acc: 0.5822, Precision: 0.5811, Recall: 0.5457, F1: 0.5494
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.79it/s]
Evaluation, Loss: 34.3389, Acc: 0.5834, Precision: 0.6114, Recall: 0.5498, F1: 0.5525

Epoch 14/20, Batch 264/264: 100%|██████████| 264/264 [01:16<00:00,  3.46it/s]
Epoch 14/20, Loss: 237.8279, Acc: 0.5858, Precision: 0.5822, Recall: 0.5510, F1: 0.5550
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.80it/s]
Evaluation, Loss: 34.5327, Acc: 0.5817, Precision: 0.6030, Recall: 0.5435, F1: 0.5470

Epoch 15/20, Batch 264/264: 100%|██████████| 264/264 [01:16<00:00,  3.45it/s]
Epoch 15/20, Loss: 237.7481, Acc: 0.5850, Precision: 0.5838, Recall: 0.5475, F1: 0.5521
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.81it/s]
Evaluation, Loss: 34.6296, Acc: 0.5676, Precision: 0.6004, Recall: 0.5519, F1: 0.5514

Epoch 16/20, Batch 264/264: 100%|██████████| 264/264 [01:16<00:00,  3.46it/s]
Epoch 16/20, Loss: 236.7658, Acc: 0.5863, Precision: 0.5831, Recall: 0.5486, F1: 0.5525
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.81it/s]
Evaluation, Loss: 34.4818, Acc: 0.5776, Precision: 0.6278, Recall: 0.5269, F1: 0.5272

Epoch 17/20, Batch 264/264: 100%|██████████| 264/264 [01:16<00:00,  3.46it/s]
Epoch 17/20, Loss: 235.5905, Acc: 0.5971, Precision: 0.5986, Recall: 0.5599, F1: 0.5649
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.71it/s]
Evaluation, Loss: 34.7089, Acc: 0.5793, Precision: 0.6237, Recall: 0.5253, F1: 0.5243
Early stopping triggered
lstm + uitnlp/CafeBERT

[12:44:40] task: task-1                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: uitnlp/CafeBERT                                                                 my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_1/lstm_CafeBERT                                                  my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/CafeBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Training ...
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.14it/s]
Epoch 1/20, Loss: 283.4526, Acc: 0.4319, Precision: 0.3708, Recall: 0.3341, F1: 0.2127
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 40.9403, Acc: 0.4224, Precision: 0.1408, Recall: 0.3333, F1: 0.1980
Saved the best model to path: ./models/task_1/lstm_CafeBERT_0.pth

Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [03:52<00:00,  1.14it/s]
Epoch 2/20, Loss: 281.7789, Acc: 0.4382, Precision: 0.2797, Recall: 0.3425, F1: 0.2389
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 40.5777, Acc: 0.4432, Precision: 0.2997, Recall: 0.3600, F1: 0.2708
Saved the best model to path: ./models/task_1/lstm_CafeBERT_1.pth

Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [03:51<00:00,  1.14it/s]
Epoch 3/20, Loss: 278.1730, Acc: 0.4675, Precision: 0.3030, Recall: 0.3844, F1: 0.3230
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 40.8053, Acc: 0.4415, Precision: 0.3197, Recall: 0.3551, F1: 0.2518

Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [03:53<00:00,  1.13it/s]
Epoch 4/20, Loss: 276.7273, Acc: 0.4676, Precision: 0.3017, Recall: 0.3830, F1: 0.3195
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.23it/s]
Evaluation, Loss: 39.9857, Acc: 0.4598, Precision: 0.3134, Recall: 0.3775, F1: 0.2973
Saved the best model to path: ./models/task_1/lstm_CafeBERT_3.pth

Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.14it/s]
Epoch 5/20, Loss: 274.3060, Acc: 0.4811, Precision: 0.3136, Recall: 0.3988, F1: 0.3389
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 40.1081, Acc: 0.4647, Precision: 0.3103, Recall: 0.3855, F1: 0.3130

Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.14it/s]
Epoch 6/20, Loss: 273.4351, Acc: 0.4839, Precision: 0.3161, Recall: 0.4023, F1: 0.3430
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 39.3938, Acc: 0.4722, Precision: 0.3554, Recall: 0.3870, F1: 0.3037
Saved the best model to path: ./models/task_1/lstm_CafeBERT_5.pth

Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.14it/s]
Epoch 7/20, Loss: 272.0010, Acc: 0.4929, Precision: 0.3297, Recall: 0.4081, F1: 0.3474
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 38.9738, Acc: 0.4880, Precision: 0.3377, Recall: 0.4078, F1: 0.3383
Saved the best model to path: ./models/task_1/lstm_CafeBERT_6.pth

Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.15it/s]
Epoch 8/20, Loss: 269.9187, Acc: 0.5023, Precision: 0.3386, Recall: 0.4186, F1: 0.3596
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 38.6615, Acc: 0.4979, Precision: 0.3469, Recall: 0.4174, F1: 0.3488
Saved the best model to path: ./models/task_1/lstm_CafeBERT_7.pth

Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.15it/s]
Epoch 9/20, Loss: 266.8615, Acc: 0.5076, Precision: 0.4547, Recall: 0.4240, F1: 0.3707
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 38.7378, Acc: 0.4880, Precision: 0.3931, Recall: 0.4021, F1: 0.3238

Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [03:49<00:00,  1.15it/s]
Epoch 10/20, Loss: 265.5746, Acc: 0.5108, Precision: 0.4708, Recall: 0.4320, F1: 0.3915
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 37.5828, Acc: 0.5087, Precision: 0.5430, Recall: 0.4269, F1: 0.3611
Saved the best model to path: ./models/task_1/lstm_CafeBERT_9.pth

Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.14it/s]
Epoch 11/20, Loss: 262.4127, Acc: 0.5127, Precision: 0.4754, Recall: 0.4396, F1: 0.4112
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 37.2502, Acc: 0.5112, Precision: 0.5263, Recall: 0.4504, F1: 0.4326
Saved the best model to path: ./models/task_1/lstm_CafeBERT_10.pth

Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [03:49<00:00,  1.15it/s]
Epoch 12/20, Loss: 261.0448, Acc: 0.5220, Precision: 0.5024, Recall: 0.4593, F1: 0.4488
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 40.1272, Acc: 0.4871, Precision: 0.5623, Recall: 0.4082, F1: 0.3503

Epoch 13/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.14it/s]
Epoch 13/20, Loss: 260.5249, Acc: 0.5132, Precision: 0.4862, Recall: 0.4467, F1: 0.4311
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 37.1091, Acc: 0.5295, Precision: 0.5482, Recall: 0.4782, F1: 0.4702
Saved the best model to path: ./models/task_1/lstm_CafeBERT_12.pth

Epoch 14/20, Batch 264/264: 100%|██████████| 264/264 [03:51<00:00,  1.14it/s]
Epoch 14/20, Loss: 260.2612, Acc: 0.5153, Precision: 0.4805, Recall: 0.4418, F1: 0.4164
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 36.9338, Acc: 0.5278, Precision: 0.5635, Recall: 0.4559, F1: 0.4256
Saved the best model to path: ./models/task_1/lstm_CafeBERT_13.pth

Epoch 15/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.15it/s]
Epoch 15/20, Loss: 260.4479, Acc: 0.5150, Precision: 0.4971, Recall: 0.4495, F1: 0.4355
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 37.3512, Acc: 0.5120, Precision: 0.5817, Recall: 0.4510, F1: 0.4268

Epoch 16/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.14it/s]
Epoch 16/20, Loss: 257.9497, Acc: 0.5223, Precision: 0.5059, Recall: 0.4576, F1: 0.4457
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 37.8612, Acc: 0.5137, Precision: 0.5575, Recall: 0.4533, F1: 0.4321

Epoch 17/20, Batch 264/264: 100%|██████████| 264/264 [03:52<00:00,  1.13it/s]p
Epoch 17/20, Loss: 257.5582, Acc: 0.5271, Precision: 0.5115, Recall: 0.4641, F1: 0.4541
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 36.3975, Acc: 0.5353, Precision: 0.5513, Recall: 0.4765, F1: 0.4642
Saved the best model to path: ./models/task_1/lstm_CafeBERT_16.pth

Epoch 18/20, Batch 264/264: 100%|██████████| 264/264 [03:51<00:00,  1.14it/s]
Epoch 18/20, Loss: 260.4192, Acc: 0.5202, Precision: 0.5084, Recall: 0.4556, F1: 0.4437
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 38.0986, Acc: 0.5187, Precision: 0.5691, Recall: 0.4415, F1: 0.3984

Epoch 19/20, Batch 264/264: 100%|██████████| 264/264 [03:51<00:00,  1.14it/s]
Epoch 19/20, Loss: 257.6739, Acc: 0.5221, Precision: 0.5109, Recall: 0.4600, F1: 0.4511
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 36.5743, Acc: 0.5270, Precision: 0.5210, Recall: 0.4717, F1: 0.4608

Epoch 20/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.14it/s]
Epoch 20/20, Loss: 257.1523, Acc: 0.5259, Precision: 0.5133, Recall: 0.4684, F1: 0.4630
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 37.3903, Acc: 0.5195, Precision: 0.5827, Recall: 0.4622, F1: 0.4445

lstm + xlm-roberta-base

[14:12:46] task: task-1                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: xlm-roberta-base                                                                my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_1/lstm_xlm-roberta-base                                          my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

Training ...
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.38it/s]
Epoch 1/20, Loss: 283.4475, Acc: 0.4327, Precision: 0.1443, Recall: 0.3331, F1: 0.2014
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.75it/s]
Evaluation, Loss: 41.0445, Acc: 0.4224, Precision: 0.1408, Recall: 0.3333, F1: 0.1980
Saved the best model to path: ./models/task_1/lstm_xlm-roberta-base_0.pth

Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.42it/s]
Epoch 2/20, Loss: 283.0410, Acc: 0.4331, Precision: 0.1444, Recall: 0.3333, F1: 0.2015
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.71it/s]
Evaluation, Loss: 40.9269, Acc: 0.4224, Precision: 0.1408, Recall: 0.3333, F1: 0.1980
Saved the best model to path: ./models/task_1/lstm_xlm-roberta-base_1.pth

Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.36it/s]
Epoch 3/20, Loss: 282.8360, Acc: 0.4331, Precision: 0.1444, Recall: 0.3333, F1: 0.2015
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.76it/s]
Evaluation, Loss: 41.0891, Acc: 0.4224, Precision: 0.1408, Recall: 0.3333, F1: 0.1980

Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.39it/s]
Epoch 4/20, Loss: 282.9328, Acc: 0.4331, Precision: 0.1444, Recall: 0.3333, F1: 0.2015
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.76it/s]
Evaluation, Loss: 40.9142, Acc: 0.4224, Precision: 0.1408, Recall: 0.3333, F1: 0.1980
Saved the best model to path: ./models/task_1/lstm_xlm-roberta-base_3.pth

Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.42it/s]
Epoch 5/20, Loss: 282.8454, Acc: 0.4331, Precision: 0.1444, Recall: 0.3333, F1: 0.2015
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.76it/s]
Evaluation, Loss: 40.9645, Acc: 0.4224, Precision: 0.1408, Recall: 0.3333, F1: 0.1980

Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.42it/s]
Epoch 6/20, Loss: 282.6613, Acc: 0.4331, Precision: 0.1444, Recall: 0.3333, F1: 0.2015
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.76it/s]
Evaluation, Loss: 40.8773, Acc: 0.4224, Precision: 0.1408, Recall: 0.3333, F1: 0.1980
Saved the best model to path: ./models/task_1/lstm_xlm-roberta-base_5.pth

Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [01:20<00:00,  3.29it/s]
Epoch 7/20, Loss: 282.6582, Acc: 0.4331, Precision: 0.1444, Recall: 0.3333, F1: 0.2015
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.65it/s]
Evaluation, Loss: 40.9392, Acc: 0.4224, Precision: 0.1408, Recall: 0.3333, F1: 0.1980

Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [01:21<00:00,  3.23it/s]
Epoch 8/20, Loss: 282.3958, Acc: 0.4331, Precision: 0.1444, Recall: 0.3333, F1: 0.2015
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.64it/s]
Evaluation, Loss: 40.8381, Acc: 0.4224, Precision: 0.1408, Recall: 0.3333, F1: 0.1980
Saved the best model to path: ./models/task_1/lstm_xlm-roberta-base_7.pth

Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [01:20<00:00,  3.26it/s]
Epoch 9/20, Loss: 282.0291, Acc: 0.4331, Precision: 0.1444, Recall: 0.3333, F1: 0.2015
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.68it/s]
Evaluation, Loss: 40.7507, Acc: 0.4224, Precision: 0.1408, Recall: 0.3333, F1: 0.1980
Saved the best model to path: ./models/task_1/lstm_xlm-roberta-base_8.pth

Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.37it/s]
Epoch 10/20, Loss: 280.9308, Acc: 0.4376, Precision: 0.2760, Recall: 0.3430, F1: 0.2436
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.72it/s]
Evaluation, Loss: 40.6060, Acc: 0.4332, Precision: 0.4341, Recall: 0.3445, F1: 0.2222
Saved the best model to path: ./models/task_1/lstm_xlm-roberta-base_9.pth

Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.42it/s]
Epoch 11/20, Loss: 279.7864, Acc: 0.4449, Precision: 0.2832, Recall: 0.3551, F1: 0.2762
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.77it/s]
Evaluation, Loss: 41.0791, Acc: 0.3826, Precision: 0.2647, Recall: 0.3461, F1: 0.2924

Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.37it/s]
Epoch 12/20, Loss: 279.2001, Acc: 0.4404, Precision: 0.2766, Recall: 0.3520, F1: 0.2750
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.68it/s]
Evaluation, Loss: 40.0110, Acc: 0.4373, Precision: 0.2882, Recall: 0.3512, F1: 0.2468
Saved the best model to path: ./models/task_1/lstm_xlm-roberta-base_11.pth

Epoch 13/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.41it/s]
Epoch 13/20, Loss: 278.1885, Acc: 0.4432, Precision: 0.3778, Recall: 0.3546, F1: 0.2788
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.77it/s]
Evaluation, Loss: 39.8583, Acc: 0.4448, Precision: 0.3219, Recall: 0.3587, F1: 0.2586
Saved the best model to path: ./models/task_1/lstm_xlm-roberta-base_12.pth

Epoch 14/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.41it/s]
Epoch 14/20, Loss: 275.2259, Acc: 0.4556, Precision: 0.4046, Recall: 0.3712, F1: 0.3092
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.75it/s]
Evaluation, Loss: 40.0509, Acc: 0.4432, Precision: 0.3890, Recall: 0.3725, F1: 0.3208

Epoch 15/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.36it/s]
Epoch 15/20, Loss: 276.4099, Acc: 0.4518, Precision: 0.4458, Recall: 0.3648, F1: 0.2952
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.69it/s]
Evaluation, Loss: 39.4831, Acc: 0.4515, Precision: 0.3306, Recall: 0.3650, F1: 0.2678
Saved the best model to path: ./models/task_1/lstm_xlm-roberta-base_14.pth

Epoch 16/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.41it/s]
Epoch 16/20, Loss: 274.4223, Acc: 0.4589, Precision: 0.4545, Recall: 0.3743, F1: 0.3118
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.76it/s]
Evaluation, Loss: 39.2403, Acc: 0.4656, Precision: 0.6376, Recall: 0.3863, F1: 0.3152
Saved the best model to path: ./models/task_1/lstm_xlm-roberta-base_15.pth

Epoch 17/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.41it/s]
Epoch 17/20, Loss: 273.8189, Acc: 0.4567, Precision: 0.4078, Recall: 0.3719, F1: 0.3116
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.76it/s]
Evaluation, Loss: 38.9268, Acc: 0.4780, Precision: 0.4619, Recall: 0.4057, F1: 0.3590
Saved the best model to path: ./models/task_1/lstm_xlm-roberta-base_16.pth

Epoch 18/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.42it/s]
Epoch 18/20, Loss: 273.5062, Acc: 0.4626, Precision: 0.4247, Recall: 0.3831, F1: 0.3369
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.76it/s]
Evaluation, Loss: 38.9492, Acc: 0.4722, Precision: 0.3052, Recall: 0.3937, F1: 0.3228

Epoch 19/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.41it/s]
Epoch 19/20, Loss: 271.6528, Acc: 0.4662, Precision: 0.4406, Recall: 0.3869, F1: 0.3423
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.76it/s]
Evaluation, Loss: 38.9713, Acc: 0.4697, Precision: 0.5031, Recall: 0.3920, F1: 0.3287

Epoch 20/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.43it/s]
Epoch 20/20, Loss: 272.8921, Acc: 0.4664, Precision: 0.4270, Recall: 0.3853, F1: 0.3365
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.76it/s]
Evaluation, Loss: 38.8362, Acc: 0.4689, Precision: 0.5282, Recall: 0.3875, F1: 0.3148
Saved the best model to path: ./models/task_1/lstm_xlm-roberta-base_19.pth

lstm + bert-base-multilingual-cased

[14:42:45] task: task-1                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: bert-base-multilingual-cased                                                    my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_1/lstm_bert-base-multilingual-cased                              my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

Training ...
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [01:10<00:00,  3.76it/s]
Epoch 1/20, Loss: 282.1285, Acc: 0.4321, Precision: 0.2657, Recall: 0.3345, F1: 0.2149
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.94it/s]
Evaluation, Loss: 40.4354, Acc: 0.4224, Precision: 0.1408, Recall: 0.3333, F1: 0.1980
Saved the best model to path: ./models/task_1/lstm_bert-base-multilingual-cased_0.pth

Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [01:09<00:00,  3.79it/s]
Epoch 2/20, Loss: 279.2801, Acc: 0.4326, Precision: 0.2611, Recall: 0.3413, F1: 0.2519
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.89it/s]
Evaluation, Loss: 40.4105, Acc: 0.4224, Precision: 0.1408, Recall: 0.3333, F1: 0.1980
Saved the best model to path: ./models/task_1/lstm_bert-base-multilingual-cased_1.pth

Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [01:09<00:00,  3.79it/s]
Epoch 3/20, Loss: 279.1136, Acc: 0.4330, Precision: 0.2635, Recall: 0.3401, F1: 0.2453
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.94it/s]
Evaluation, Loss: 40.3201, Acc: 0.4282, Precision: 0.2647, Recall: 0.3532, F1: 0.2813
Saved the best model to path: ./models/task_1/lstm_bert-base-multilingual-cased_2.pth

Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [01:10<00:00,  3.73it/s]
Epoch 4/20, Loss: 278.6760, Acc: 0.4337, Precision: 0.3320, Recall: 0.3447, F1: 0.2638
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.89it/s]
Evaluation, Loss: 40.2540, Acc: 0.4232, Precision: 0.4743, Recall: 0.3342, F1: 0.1998
Saved the best model to path: ./models/task_1/lstm_bert-base-multilingual-cased_3.pth

Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [01:10<00:00,  3.74it/s]
Epoch 5/20, Loss: 277.7915, Acc: 0.4407, Precision: 0.2728, Recall: 0.3536, F1: 0.2799
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.99it/s]
Evaluation, Loss: 40.4570, Acc: 0.4266, Precision: 0.3797, Recall: 0.3376, F1: 0.2071

Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [01:09<00:00,  3.81it/s]
Epoch 6/20, Loss: 278.0103, Acc: 0.4493, Precision: 0.5067, Recall: 0.3606, F1: 0.2861
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.98it/s]
Evaluation, Loss: 39.9273, Acc: 0.4523, Precision: 0.2971, Recall: 0.3726, F1: 0.2962
Saved the best model to path: ./models/task_1/lstm_bert-base-multilingual-cased_5.pth

Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [01:10<00:00,  3.76it/s]
Epoch 7/20, Loss: 275.9526, Acc: 0.4587, Precision: 0.4268, Recall: 0.3711, F1: 0.3029
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.91it/s]
Evaluation, Loss: 39.8604, Acc: 0.4490, Precision: 0.2908, Recall: 0.3723, F1: 0.3013
Saved the best model to path: ./models/task_1/lstm_bert-base-multilingual-cased_6.pth

Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [01:11<00:00,  3.69it/s]
Epoch 8/20, Loss: 275.7817, Acc: 0.4566, Precision: 0.3901, Recall: 0.3672, F1: 0.2946
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.84it/s]
Evaluation, Loss: 39.4926, Acc: 0.4548, Precision: 0.4248, Recall: 0.3869, F1: 0.3423
Saved the best model to path: ./models/task_1/lstm_bert-base-multilingual-cased_7.pth

Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [01:11<00:00,  3.70it/s]
Epoch 9/20, Loss: 274.1175, Acc: 0.4627, Precision: 0.4269, Recall: 0.3757, F1: 0.3124
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.79it/s]
Evaluation, Loss: 39.7230, Acc: 0.4531, Precision: 0.3693, Recall: 0.3656, F1: 0.2648

Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [01:11<00:00,  3.70it/s]
Epoch 10/20, Loss: 274.7119, Acc: 0.4608, Precision: 0.4283, Recall: 0.3736, F1: 0.3090
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.83it/s]
Evaluation, Loss: 39.9054, Acc: 0.4456, Precision: 0.4001, Recall: 0.3572, F1: 0.2474

Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [01:11<00:00,  3.70it/s]
Epoch 11/20, Loss: 273.2554, Acc: 0.4665, Precision: 0.4364, Recall: 0.3808, F1: 0.3223
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.83it/s]
Evaluation, Loss: 39.4875, Acc: 0.4647, Precision: 0.4925, Recall: 0.3902, F1: 0.3299
Saved the best model to path: ./models/task_1/lstm_bert-base-multilingual-cased_10.pth

Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [01:11<00:00,  3.68it/s]
Epoch 12/20, Loss: 272.9750, Acc: 0.4645, Precision: 0.4375, Recall: 0.3873, F1: 0.3433
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.82it/s]
Evaluation, Loss: 39.4922, Acc: 0.4639, Precision: 0.4820, Recall: 0.3790, F1: 0.2937

Epoch 13/20, Batch 264/264: 100%|██████████| 264/264 [01:11<00:00,  3.69it/s]
Epoch 13/20, Loss: 273.6615, Acc: 0.4653, Precision: 0.4400, Recall: 0.3828, F1: 0.3303
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.78it/s]
Evaluation, Loss: 39.3505, Acc: 0.4647, Precision: 0.6224, Recall: 0.3796, F1: 0.2935
Saved the best model to path: ./models/task_1/lstm_bert-base-multilingual-cased_12.pth

Epoch 14/20, Batch 264/264: 100%|██████████| 264/264 [01:12<00:00,  3.66it/s]
Epoch 14/20, Loss: 273.1578, Acc: 0.4681, Precision: 0.4464, Recall: 0.3853, F1: 0.3331
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.77it/s]
Evaluation, Loss: 39.3045, Acc: 0.4672, Precision: 0.5185, Recall: 0.3845, F1: 0.3074
Saved the best model to path: ./models/task_1/lstm_bert-base-multilingual-cased_13.pth

Epoch 15/20, Batch 264/264: 100%|██████████| 264/264 [01:10<00:00,  3.73it/s]
Epoch 15/20, Loss: 273.1026, Acc: 0.4698, Precision: 0.4671, Recall: 0.3900, F1: 0.3425
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.94it/s]
Evaluation, Loss: 39.8142, Acc: 0.4647, Precision: 0.5289, Recall: 0.3832, F1: 0.3076

Epoch 16/20, Batch 264/264: 100%|██████████| 264/264 [01:09<00:00,  3.79it/s]
Epoch 16/20, Loss: 272.6431, Acc: 0.4702, Precision: 0.4472, Recall: 0.3880, F1: 0.3381
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.93it/s]
Evaluation, Loss: 39.2089, Acc: 0.4730, Precision: 0.4856, Recall: 0.3958, F1: 0.3356
Saved the best model to path: ./models/task_1/lstm_bert-base-multilingual-cased_15.pth

Epoch 17/20, Batch 264/264: 100%|██████████| 264/264 [01:10<00:00,  3.76it/s]
Epoch 17/20, Loss: 272.5119, Acc: 0.4686, Precision: 0.4435, Recall: 0.3856, F1: 0.3338
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.84it/s]
Evaluation, Loss: 39.3646, Acc: 0.4672, Precision: 0.4849, Recall: 0.3866, F1: 0.3158

Epoch 18/20, Batch 264/264: 100%|██████████| 264/264 [01:10<00:00,  3.77it/s]
Epoch 18/20, Loss: 272.3492, Acc: 0.4654, Precision: 0.4508, Recall: 0.3884, F1: 0.3450
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.98it/s]
Evaluation, Loss: 39.0515, Acc: 0.4639, Precision: 0.3363, Recall: 0.3787, F1: 0.2920
Saved the best model to path: ./models/task_1/lstm_bert-base-multilingual-cased_17.pth

Epoch 19/20, Batch 264/264: 100%|██████████| 264/264 [01:09<00:00,  3.80it/s]
Epoch 19/20, Loss: 272.4804, Acc: 0.4652, Precision: 0.4361, Recall: 0.3779, F1: 0.3152
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.94it/s]
Evaluation, Loss: 39.3398, Acc: 0.4614, Precision: 0.4770, Recall: 0.3765, F1: 0.2899

Epoch 20/20, Batch 264/264: 100%|██████████| 264/264 [01:09<00:00,  3.81it/s]
Epoch 20/20, Loss: 270.9964, Acc: 0.4720, Precision: 0.4627, Recall: 0.3968, F1: 0.3574
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.91it/s]
Evaluation, Loss: 39.0930, Acc: 0.4614, Precision: 0.4224, Recall: 0.3784, F1: 0.2990

lstm + distilbert-base-multilingual-cased

[15:09:56] task: task-1                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: distilbert-base-multilingual-cased                                              my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_1/lstm_distilbert-base-multilingual-cased                        my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

Training ...
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.15it/s]
Epoch 1/20, Loss: 279.5465, Acc: 0.4457, Precision: 0.3678, Recall: 0.3551, F1: 0.2747
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.76it/s]
Evaluation, Loss: 39.9335, Acc: 0.4606, Precision: 0.3071, Recall: 0.3826, F1: 0.3117
Saved the best model to path: ./models/task_1/lstm_distilbert-base-multilingual-cased_0.pth

Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.03it/s]
Epoch 2/20, Loss: 272.1282, Acc: 0.4733, Precision: 0.4530, Recall: 0.3932, F1: 0.3481
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.62it/s]
Evaluation, Loss: 39.2897, Acc: 0.4672, Precision: 0.4835, Recall: 0.3959, F1: 0.3425
Saved the best model to path: ./models/task_1/lstm_distilbert-base-multilingual-cased_1.pth

Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.23it/s]
Epoch 3/20, Loss: 269.3076, Acc: 0.4773, Precision: 0.4640, Recall: 0.4004, F1: 0.3607
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.80it/s]
Evaluation, Loss: 40.0686, Acc: 0.4606, Precision: 0.4317, Recall: 0.3955, F1: 0.3601

Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.24it/s]
Epoch 4/20, Loss: 265.5023, Acc: 0.4826, Precision: 0.4628, Recall: 0.4174, F1: 0.3980
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.79it/s]
Evaluation, Loss: 37.9633, Acc: 0.5071, Precision: 0.4993, Recall: 0.4802, F1: 0.4827
Saved the best model to path: ./models/task_1/lstm_distilbert-base-multilingual-cased_3.pth

Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.23it/s]
Epoch 5/20, Loss: 257.2003, Acc: 0.5229, Precision: 0.5057, Recall: 0.4616, F1: 0.4529
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.81it/s]
Evaluation, Loss: 36.3923, Acc: 0.5361, Precision: 0.5165, Recall: 0.4939, F1: 0.4920
Saved the best model to path: ./models/task_1/lstm_distilbert-base-multilingual-cased_4.pth

Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.18it/s]
Epoch 6/20, Loss: 254.3777, Acc: 0.5306, Precision: 0.5178, Recall: 0.4720, F1: 0.4667
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.46it/s]
Evaluation, Loss: 36.8013, Acc: 0.5187, Precision: 0.5106, Recall: 0.4499, F1: 0.4191

Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.11it/s]
Epoch 7/20, Loss: 250.9590, Acc: 0.5452, Precision: 0.5352, Recall: 0.4892, F1: 0.4867
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.67it/s]
Evaluation, Loss: 37.5874, Acc: 0.5220, Precision: 0.5075, Recall: 0.4844, F1: 0.4830

Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.13it/s]
Epoch 8/20, Loss: 246.9329, Acc: 0.5616, Precision: 0.5553, Recall: 0.5100, F1: 0.5109
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.54it/s]
Evaluation, Loss: 35.6963, Acc: 0.5585, Precision: 0.5663, Recall: 0.5076, F1: 0.5044
Saved the best model to path: ./models/task_1/lstm_distilbert-base-multilingual-cased_7.pth

Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.25it/s]
Epoch 9/20, Loss: 246.7760, Acc: 0.5592, Precision: 0.5569, Recall: 0.5125, F1: 0.5142
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.81it/s]
Evaluation, Loss: 37.7218, Acc: 0.5079, Precision: 0.5400, Recall: 0.5006, F1: 0.4924

Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [00:41<00:00,  6.29it/s]
Epoch 10/20, Loss: 245.4347, Acc: 0.5679, Precision: 0.5683, Recall: 0.5201, F1: 0.5222
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.77it/s]
Evaluation, Loss: 36.4185, Acc: 0.5344, Precision: 0.5839, Recall: 0.4616, F1: 0.4315

Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.28it/s]
Epoch 11/20, Loss: 244.7024, Acc: 0.5692, Precision: 0.5707, Recall: 0.5258, F1: 0.5281
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.81it/s]
Evaluation, Loss: 34.9024, Acc: 0.5710, Precision: 0.5903, Recall: 0.5227, F1: 0.5219
Saved the best model to path: ./models/task_1/lstm_distilbert-base-multilingual-cased_10.pth

Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.08it/s]
Epoch 12/20, Loss: 242.6315, Acc: 0.5741, Precision: 0.5785, Recall: 0.5313, F1: 0.5342
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.72it/s]
Evaluation, Loss: 36.6562, Acc: 0.5394, Precision: 0.5477, Recall: 0.5175, F1: 0.5123

Epoch 13/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.26it/s]
Epoch 13/20, Loss: 241.5696, Acc: 0.5725, Precision: 0.5746, Recall: 0.5287, F1: 0.5309
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.51it/s]
Evaluation, Loss: 34.9670, Acc: 0.5809, Precision: 0.5937, Recall: 0.5364, F1: 0.5389

Epoch 14/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.20it/s]
Epoch 14/20, Loss: 240.7193, Acc: 0.5821, Precision: 0.5853, Recall: 0.5399, F1: 0.5431
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.68it/s]
Evaluation, Loss: 34.8610, Acc: 0.5801, Precision: 0.6075, Recall: 0.5332, F1: 0.5343
Saved the best model to path: ./models/task_1/lstm_distilbert-base-multilingual-cased_13.pth

Epoch 15/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.24it/s]
Epoch 15/20, Loss: 239.8100, Acc: 0.5818, Precision: 0.5837, Recall: 0.5404, F1: 0.5436
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.83it/s]
Evaluation, Loss: 34.9654, Acc: 0.5726, Precision: 0.5783, Recall: 0.5517, F1: 0.5531

Epoch 16/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.15it/s]
Epoch 16/20, Loss: 238.3658, Acc: 0.5884, Precision: 0.5898, Recall: 0.5426, F1: 0.5466
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.82it/s]
Evaluation, Loss: 35.1803, Acc: 0.5726, Precision: 0.6055, Recall: 0.5332, F1: 0.5350

Epoch 17/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.15it/s]
Epoch 17/20, Loss: 237.8111, Acc: 0.5863, Precision: 0.5861, Recall: 0.5425, F1: 0.5462
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.82it/s]
Evaluation, Loss: 34.5877, Acc: 0.5851, Precision: 0.5883, Recall: 0.5546, F1: 0.5593
Saved the best model to path: ./models/task_1/lstm_distilbert-base-multilingual-cased_16.pth

Epoch 18/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.08it/s]
Epoch 18/20, Loss: 237.5680, Acc: 0.5923, Precision: 0.5923, Recall: 0.5496, F1: 0.5535
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.76it/s]
Evaluation, Loss: 34.5659, Acc: 0.5950, Precision: 0.6199, Recall: 0.5528, F1: 0.5559
Saved the best model to path: ./models/task_1/lstm_distilbert-base-multilingual-cased_17.pth

Epoch 19/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  5.93it/s]
Epoch 19/20, Loss: 237.7346, Acc: 0.5898, Precision: 0.5910, Recall: 0.5480, F1: 0.5523
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:06<00:00,  6.32it/s]
Evaluation, Loss: 35.0636, Acc: 0.5793, Precision: 0.6218, Recall: 0.5331, F1: 0.5315

Epoch 20/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.09it/s]
Epoch 20/20, Loss: 235.6680, Acc: 0.5980, Precision: 0.5938, Recall: 0.5579, F1: 0.5624
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.64it/s]
Evaluation, Loss: 35.0500, Acc: 0.5710, Precision: 0.5743, Recall: 0.5377, F1: 0.5424


(quocenv) ➜  training-model git:(master) ✗ ./coms/task_1_lstm_cnn_eval_commands.sh
Evaluating LSTM
lstm + vinai/phobert-base

[18:02:12] task: task-1                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: vinai/phobert-base                                                              my_import.py:133
           padding_len: 256                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_1/lstm_phobert-base                                              my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

model_weight_path: ./models/task_1/lstm_phobert-base_15.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:06<00:00,  6.03it/s]
Evaluation, Loss: 35.2904, Acc: 0.5577, Precision: 0.5772, Recall: 0.5166, F1: 0.5204
Confusion Matrix:
[[404  67  38]
 [189 170  35]
 [171  33  98]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.53      0.79      0.63       509
     warning       0.63      0.43      0.51       394
     seeding       0.57      0.32      0.41       302

    accuracy                           0.56      1205
   macro avg       0.58      0.52      0.52      1205
weighted avg       0.57      0.56      0.54      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:11<00:00,  6.88it/s]
Evaluation, Loss: 69.4002, Acc: 0.5672, Precision: 0.5812, Recall: 0.5205, F1: 0.5226
Confusion Matrix:
[[838 107  80]
 [365 352  72]
 [349  71 178]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.54      0.82      0.65      1025
     warning       0.66      0.45      0.53       789
     seeding       0.54      0.30      0.38       598

    accuracy                           0.57      2412
   macro avg       0.58      0.52      0.52      2412
weighted avg       0.58      0.57      0.55      2412

lstm + uitnlp/visobert

[18:02:52] task: task-1                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: uitnlp/visobert                                                                 my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_1/lstm_visobert                                                  my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/visobert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

model_weight_path: ./models/task_1/lstm_visobert_11.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.61it/s]
Evaluation, Loss: 34.1515, Acc: 0.5934, Precision: 0.6006, Recall: 0.5589, F1: 0.5630
Confusion Matrix:
[[408  29  72]
 [158 183  53]
 [137  41 124]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.58      0.80      0.67       509
     warning       0.72      0.46      0.57       394
     seeding       0.50      0.41      0.45       302

    accuracy                           0.59      1205
   macro avg       0.60      0.56      0.56      1205
weighted avg       0.61      0.59      0.58      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:20<00:00,  3.79it/s]
Evaluation, Loss: 68.1213, Acc: 0.5825, Precision: 0.5816, Recall: 0.5493, F1: 0.5525
Confusion Matrix:
[[799  84 142]
 [314 358 117]
 [261  89 248]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.58      0.78      0.67      1025
     warning       0.67      0.45      0.54       789
     seeding       0.49      0.41      0.45       598

    accuracy                           0.58      2412
   macro avg       0.58      0.55      0.55      2412
weighted avg       0.59      0.58      0.57      2412

lstm + uitnlp/CafeBERT

[18:03:45] task: task-1                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: uitnlp/CafeBERT                                                                 my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_1/lstm_CafeBERT                                                  my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/CafeBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

model_weight_path: ./models/task_1/lstm_CafeBERT_16.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.23it/s]
Evaluation, Loss: 36.3975, Acc: 0.5353, Precision: 0.5513, Recall: 0.4765, F1: 0.4642
Confusion Matrix:
[[443  30  36]
 [221 142  31]
 [202  40  60]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.51      0.87      0.64       509
     warning       0.67      0.36      0.47       394
     seeding       0.47      0.20      0.28       302

    accuracy                           0.54      1205
   macro avg       0.55      0.48      0.46      1205
weighted avg       0.55      0.54      0.50      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [01:01<00:00,  1.24it/s]
Evaluation, Loss: 72.7282, Acc: 0.5319, Precision: 0.5467, Recall: 0.4714, F1: 0.4597
Confusion Matrix:
[[879  79  67]
 [435 294  60]
 [422  66 110]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.51      0.86      0.64      1025
     warning       0.67      0.37      0.48       789
     seeding       0.46      0.18      0.26       598

    accuracy                           0.53      2412
   macro avg       0.55      0.47      0.46      2412
weighted avg       0.55      0.53      0.49      2412

lstm + xlm-roberta-base

[18:05:40] task: task-1                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: xlm-roberta-base                                                                my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_1/lstm_xlm-roberta-base                                          my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

model_weight_path: ./models/task_1/lstm_xlm-roberta-base_19.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.54it/s]
Evaluation, Loss: 38.8362, Acc: 0.4689, Precision: 0.5282, Recall: 0.3875, F1: 0.3148
Confusion Matrix:
[[491  14   4]
 [325  61   8]
 [264  25  13]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.45      0.96      0.62       509
     warning       0.61      0.15      0.25       394
     seeding       0.52      0.04      0.08       302

    accuracy                           0.47      1205
   macro avg       0.53      0.39      0.31      1205
weighted avg       0.52      0.47      0.36      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:20<00:00,  3.74it/s]
Evaluation, Loss: 76.7453, Acc: 0.4722, Precision: 0.4812, Recall: 0.3891, F1: 0.3206
Confusion Matrix:
[[976  36  13]
 [626 142  21]
 [519  58  21]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.46      0.95      0.62      1025
     warning       0.60      0.18      0.28       789
     seeding       0.38      0.04      0.06       598

    accuracy                           0.47      2412
   macro avg       0.48      0.39      0.32      2412
weighted avg       0.49      0.47      0.37      2412

lstm + bert-base-multilingual-cased

[18:06:30] task: task-1                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: bert-base-multilingual-cased                                                    my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_1/lstm_bert-base-multilingual-cased                              my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

model_weight_path: ./models/task_1/lstm_bert-base-multilingual-cased_17.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.68it/s]
Evaluation, Loss: 39.0515, Acc: 0.4639, Precision: 0.3363, Recall: 0.3787, F1: 0.2920
Confusion Matrix:
[[493  15   1]
 [328  66   0]
 [264  38   0]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.45      0.97      0.62       509
     warning       0.55      0.17      0.26       394
     seeding       0.00      0.00      0.00       302

    accuracy                           0.46      1205
   macro avg       0.34      0.38      0.29      1205
weighted avg       0.37      0.46      0.35      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:19<00:00,  3.96it/s]
Evaluation, Loss: 77.9649, Acc: 0.4602, Precision: 0.4995, Recall: 0.3742, F1: 0.2911
Confusion Matrix:
[[977  47   1]
 [657 131   1]
 [535  61   2]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.45      0.95      0.61      1025
     warning       0.55      0.17      0.25       789
     seeding       0.50      0.00      0.01       598

    accuracy                           0.46      2412
   macro avg       0.50      0.37      0.29      2412
weighted avg       0.49      0.46      0.34      2412

lstm + distilbert-base-multilingual-cased

[18:07:15] task: task-1                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: distilbert-base-multilingual-cased                                              my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_1/lstm_distilbert-base-multilingual-cased                        my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

model_weight_path: ./models/task_1/lstm_distilbert-base-multilingual-cased_17.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:06<00:00,  6.08it/s]
Evaluation, Loss: 34.5659, Acc: 0.5950, Precision: 0.6199, Recall: 0.5528, F1: 0.5559
Confusion Matrix:
[[435  25  49]
 [166 168  60]
 [166  22 114]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.57      0.85      0.68       509
     warning       0.78      0.43      0.55       394
     seeding       0.51      0.38      0.43       302

    accuracy                           0.60      1205
   macro avg       0.62      0.55      0.56      1205
weighted avg       0.62      0.60      0.58      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:11<00:00,  6.66it/s]
Evaluation, Loss: 68.2798, Acc: 0.5891, Precision: 0.6117, Recall: 0.5433, F1: 0.5450
Confusion Matrix:
[[883  56  86]
 [336 324 129]
 [344  40 214]]
Classification Report:
              precision    recall  f1-score   support

       clean       0.56      0.86      0.68      1025
     warning       0.77      0.41      0.54       789
     seeding       0.50      0.36      0.42       598

    accuracy                           0.59      2412
   macro avg       0.61      0.54      0.55      2412
weighted avg       0.62      0.59      0.57      2412