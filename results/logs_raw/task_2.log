vinai/phobert-base

[13:05:21] task: task-2                                                                                   my_import.py:132
           model_type: simple                                                                             my_import.py:132
           model_name: vinai/phobert-base                                                                 my_import.py:132
           padding_len: 256                                                                               my_import.py:130
           batch_size: 32                                                                                 my_import.py:132
           learning_rate: 0.001                                                                           my_import.py:132
           epochs: 20                                                                                     my_import.py:132
           fine_tune: True                                                                                my_import.py:132
           device: cuda                                                                                   my_import.py:132
           saving_path: ./models/task_2/simple_phobert-base                                               my_import.py:132
           train_shape: (8437, 27)                                                                        my_import.py:132
           dev_shape: (1205, 27)                                                                          my_import.py:132
           test_shape: (2412, 27)                                                                         my_import.py:132

Training ...
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [00:40<00:00,  6.45it/s]
Epoch 1/20, Loss: 360.0212, 0/1 Loss: 0.9941, Hamming Loss: 0.6852, EMR: 0.0059, Acc: 0.6375, F1: 0.4937, Precision: 0.5554, Recall: 0.4976
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.23it/s]
Evaluation, Loss: 51.3066, 0/1 Loss: 0.9859, Hamming Loss: 0.5683, EMR: 0.0141, Acc: 0.8826, F1: 0.5835, Precision: 0.5249, Recall: 0.6887
Saved the best model to path: ./models/task_2/simple_phobert-base_0.pth

Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.92it/s]
Epoch 2/20, Loss: 358.6567, 0/1 Loss: 0.9902, Hamming Loss: 0.6540, EMR: 0.0098, Acc: 0.6883, F1: 0.5214, Precision: 0.5696, Recall: 0.5381
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.94it/s]
Evaluation, Loss: 51.0042, 0/1 Loss: 0.9054, Hamming Loss: 0.4707, EMR: 0.0946, Acc: 0.9641, F1: 0.6543, Precision: 0.6049, Recall: 0.7545
Saved the best model to path: ./models/task_2/simple_phobert-base_1.pth

Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.81it/s]
Epoch 3/20, Loss: 357.3591, 0/1 Loss: 0.9864, Hamming Loss: 0.6413, EMR: 0.0136, Acc: 0.7052, F1: 0.5341, Precision: 0.5838, Recall: 0.5519
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.46it/s]
Evaluation, Loss: 50.7192, 0/1 Loss: 0.9154, Hamming Loss: 0.5004, EMR: 0.0846, Acc: 0.9861, F1: 0.6371, Precision: 0.5688, Recall: 0.7715
Saved the best model to path: ./models/task_2/simple_phobert-base_2.pth

Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.86it/s]
Epoch 4/20, Loss: 357.0882, 0/1 Loss: 0.9873, Hamming Loss: 0.6634, EMR: 0.0127, Acc: 0.6715, F1: 0.5117, Precision: 0.5681, Recall: 0.5249
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.42it/s]
Evaluation, Loss: 50.7548, 0/1 Loss: 0.9544, Hamming Loss: 0.4820, EMR: 0.0456, Acc: 0.9861, F1: 0.6391, Precision: 0.5669, Recall: 0.7715

Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.82it/s]
Epoch 5/20, Loss: 356.6175, 0/1 Loss: 0.9836, Hamming Loss: 0.6535, EMR: 0.0164, Acc: 0.7025, F1: 0.5251, Precision: 0.5707, Recall: 0.5493
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.34it/s]
Evaluation, Loss: 50.6574, 0/1 Loss: 0.9336, Hamming Loss: 0.5790, EMR: 0.0664, Acc: 0.9861, F1: 0.6003, Precision: 0.5128, Recall: 0.7715
Saved the best model to path: ./models/task_2/simple_phobert-base_4.pth

Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.86it/s]
Epoch 6/20, Loss: 356.4764, 0/1 Loss: 0.9874, Hamming Loss: 0.6564, EMR: 0.0126, Acc: 0.7093, F1: 0.5263, Precision: 0.5666, Recall: 0.5551
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.41it/s]
Evaluation, Loss: 50.5932, 0/1 Loss: 0.9494, Hamming Loss: 0.5670, EMR: 0.0506, Acc: 0.9861, F1: 0.6054, Precision: 0.5198, Recall: 0.7715
Saved the best model to path: ./models/task_2/simple_phobert-base_5.pth

Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.85it/s]
Epoch 7/20, Loss: 356.3071, 0/1 Loss: 0.9825, Hamming Loss: 0.6545, EMR: 0.0175, Acc: 0.6935, F1: 0.5229, Precision: 0.5748, Recall: 0.5437
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.42it/s]
Evaluation, Loss: 50.3931, 0/1 Loss: 0.9220, Hamming Loss: 0.5595, EMR: 0.0780, Acc: 0.9861, F1: 0.5999, Precision: 0.5154, Recall: 0.7715
Saved the best model to path: ./models/task_2/simple_phobert-base_6.pth

Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.84it/s]
Epoch 8/20, Loss: 355.9091, 0/1 Loss: 0.9840, Hamming Loss: 0.6537, EMR: 0.0160, Acc: 0.7071, F1: 0.5273, Precision: 0.5727, Recall: 0.5541
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.43it/s]
Evaluation, Loss: 50.1629, 0/1 Loss: 0.9104, Hamming Loss: 0.5122, EMR: 0.0896, Acc: 0.9861, F1: 0.6223, Precision: 0.5452, Recall: 0.7715
Saved the best model to path: ./models/task_2/simple_phobert-base_7.pth

Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.83it/s]
Epoch 9/20, Loss: 355.5466, 0/1 Loss: 0.9826, Hamming Loss: 0.6540, EMR: 0.0174, Acc: 0.7070, F1: 0.5245, Precision: 0.5648, Recall: 0.5539
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.40it/s]
Evaluation, Loss: 50.4575, 0/1 Loss: 0.8954, Hamming Loss: 0.4548, EMR: 0.1046, Acc: 0.9861, F1: 0.6620, Precision: 0.6087, Recall: 0.7715

Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.85it/s]
Epoch 10/20, Loss: 356.0020, 0/1 Loss: 0.9839, Hamming Loss: 0.6560, EMR: 0.0161, Acc: 0.7018, F1: 0.5247, Precision: 0.5699, Recall: 0.5503
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.42it/s]
Evaluation, Loss: 50.2951, 0/1 Loss: 0.9029, Hamming Loss: 0.5033, EMR: 0.0971, Acc: 0.9861, F1: 0.6288, Precision: 0.5587, Recall: 0.7715

Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.84it/s]
Epoch 11/20, Loss: 356.0994, 0/1 Loss: 0.9838, Hamming Loss: 0.6624, EMR: 0.0162, Acc: 0.6984, F1: 0.5212, Precision: 0.5692, Recall: 0.5470
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.43it/s]
Evaluation, Loss: 50.4290, 0/1 Loss: 0.9087, Hamming Loss: 0.5409, EMR: 0.0913, Acc: 0.9861, F1: 0.6126, Precision: 0.5357, Recall: 0.7715

Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.83it/s]
Epoch 12/20, Loss: 355.3906, 0/1 Loss: 0.9844, Hamming Loss: 0.6656, EMR: 0.0156, Acc: 0.6944, F1: 0.5164, Precision: 0.5636, Recall: 0.5429
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.44it/s]
Evaluation, Loss: 50.4118, 0/1 Loss: 0.9154, Hamming Loss: 0.5670, EMR: 0.0846, Acc: 0.9838, F1: 0.6105, Precision: 0.5315, Recall: 0.7694

Epoch 13/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.91it/s]
Epoch 13/20, Loss: 355.5230, 0/1 Loss: 0.9863, Hamming Loss: 0.6594, EMR: 0.0137, Acc: 0.7024, F1: 0.5228, Precision: 0.5671, Recall: 0.5500
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.44it/s]
Evaluation, Loss: 50.0811, 0/1 Loss: 0.9095, Hamming Loss: 0.5320, EMR: 0.0905, Acc: 0.9861, F1: 0.6144, Precision: 0.5364, Recall: 0.7715
Saved the best model to path: ./models/task_2/simple_phobert-base_12.pth

Epoch 14/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.90it/s]
Epoch 14/20, Loss: 355.3685, 0/1 Loss: 0.9801, Hamming Loss: 0.6529, EMR: 0.0199, Acc: 0.7017, F1: 0.5247, Precision: 0.5731, Recall: 0.5499
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.43it/s]
Evaluation, Loss: 50.0716, 0/1 Loss: 0.9120, Hamming Loss: 0.5135, EMR: 0.0880, Acc: 0.9861, F1: 0.6240, Precision: 0.5484, Recall: 0.7715
Saved the best model to path: ./models/task_2/simple_phobert-base_13.pth

Epoch 15/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.92it/s]
Epoch 15/20, Loss: 355.3614, 0/1 Loss: 0.9828, Hamming Loss: 0.6573, EMR: 0.0172, Acc: 0.6928, F1: 0.5208, Precision: 0.5733, Recall: 0.5428
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.90it/s]
Evaluation, Loss: 50.0953, 0/1 Loss: 0.9071, Hamming Loss: 0.5566, EMR: 0.0929, Acc: 0.9861, F1: 0.6055, Precision: 0.5241, Recall: 0.7712

Epoch 16/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.89it/s]
Epoch 16/20, Loss: 355.0875, 0/1 Loss: 0.9828, Hamming Loss: 0.6605, EMR: 0.0172, Acc: 0.6837, F1: 0.5171, Precision: 0.5739, Recall: 0.5355
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.44it/s]
Evaluation, Loss: 50.1367, 0/1 Loss: 0.9112, Hamming Loss: 0.5400, EMR: 0.0888, Acc: 0.9859, F1: 0.6112, Precision: 0.5318, Recall: 0.7713

Epoch 17/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.90it/s]
Epoch 17/20, Loss: 355.1125, 0/1 Loss: 0.9822, Hamming Loss: 0.6602, EMR: 0.0178, Acc: 0.6881, F1: 0.5171, Precision: 0.5689, Recall: 0.5390
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.43it/s]
Evaluation, Loss: 50.1213, 0/1 Loss: 0.8855, Hamming Loss: 0.4774, EMR: 0.1145, Acc: 0.9728, F1: 0.6396, Precision: 0.5827, Recall: 0.7599

Epoch 18/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.93it/s]
Epoch 18/20, Loss: 354.9221, 0/1 Loss: 0.9822, Hamming Loss: 0.6599, EMR: 0.0178, Acc: 0.6908, F1: 0.5174, Precision: 0.5667, Recall: 0.5409
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.44it/s]
Evaluation, Loss: 50.0374, 0/1 Loss: 0.8905, Hamming Loss: 0.5100, EMR: 0.1095, Acc: 0.9824, F1: 0.6347, Precision: 0.5680, Recall: 0.7682
Saved the best model to path: ./models/task_2/simple_phobert-base_17.pth

Epoch 19/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.90it/s]
Epoch 19/20, Loss: 355.1177, 0/1 Loss: 0.9853, Hamming Loss: 0.6600, EMR: 0.0147, Acc: 0.6991, F1: 0.5216, Precision: 0.5672, Recall: 0.5476
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.40it/s]
Evaluation, Loss: 50.1114, 0/1 Loss: 0.9054, Hamming Loss: 0.5114, EMR: 0.0946, Acc: 0.9664, F1: 0.6231, Precision: 0.5586, Recall: 0.7541

Epoch 20/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.91it/s]
Epoch 20/20, Loss: 354.9832, 0/1 Loss: 0.9816, Hamming Loss: 0.6557, EMR: 0.0184, Acc: 0.6902, F1: 0.5199, Precision: 0.5704, Recall: 0.5396
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.43it/s]
Evaluation, Loss: 50.0131, 0/1 Loss: 0.8797, Hamming Loss: 0.4913, EMR: 0.1203, Acc: 0.9820, F1: 0.6333, Precision: 0.5672, Recall: 0.7675
Saved the best model to path: ./models/task_2/simple_phobert-base_19.pth

uitnlp/visobert

[13:20:28] task: task-2                                                                                   my_import.py:132
           model_type: simple                                                                             my_import.py:132
           model_name: uitnlp/visobert                                                                    my_import.py:132
           padding_len: 512                                                                               my_import.py:130
           batch_size: 32                                                                                 my_import.py:132
           learning_rate: 0.001                                                                           my_import.py:132
           epochs: 20                                                                                     my_import.py:132
           fine_tune: True                                                                                my_import.py:132
           device: cuda                                                                                   my_import.py:132
           saving_path: ./models/task_2/simple_visobert                                                   my_import.py:132
           train_shape: (8437, 27)                                                                        my_import.py:132
           dev_shape: (1205, 27)                                                                          my_import.py:132
           test_shape: (2412, 27)                                                                         my_import.py:132
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/visobert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Training ...
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [01:19<00:00,  3.32it/s]
Epoch 1/20, Loss: 359.5919, 0/1 Loss: 0.9998, Hamming Loss: 0.7854, EMR: 0.0002, Acc: 0.5026, F1: 0.4065, Precision: 0.4970, Recall: 0.3914
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.72it/s]
Evaluation, Loss: 51.4019, 0/1 Loss: 0.9992, Hamming Loss: 0.6598, EMR: 0.0008, Acc: 0.7407, F1: 0.5207, Precision: 0.4869, Recall: 0.5771
Saved the best model to path: ./models/task_2/simple_visobert_0.pth

Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.41it/s]
Epoch 2/20, Loss: 359.5695, 0/1 Loss: 0.9999, Hamming Loss: 0.7986, EMR: 0.0001, Acc: 0.4837, F1: 0.3900, Precision: 0.4916, Recall: 0.3765
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.70it/s]
Evaluation, Loss: 50.8326, 0/1 Loss: 1.0000, Hamming Loss: 0.6716, EMR: 0.0000, Acc: 0.7327, F1: 0.5066, Precision: 0.4737, Recall: 0.5709
Saved the best model to path: ./models/task_2/simple_visobert_1.pth

Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.37it/s]
Epoch 3/20, Loss: 358.7205, 0/1 Loss: 0.9998, Hamming Loss: 0.7900, EMR: 0.0002, Acc: 0.4849, F1: 0.3932, Precision: 0.4988, Recall: 0.3779
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.69it/s]
Evaluation, Loss: 50.8303, 0/1 Loss: 1.0000, Hamming Loss: 0.6357, EMR: 0.0000, Acc: 0.7274, F1: 0.5209, Precision: 0.4991, Recall: 0.5661
Saved the best model to path: ./models/task_2/simple_visobert_2.pth

Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [01:19<00:00,  3.34it/s]
Epoch 4/20, Loss: 358.2506, 0/1 Loss: 1.0000, Hamming Loss: 0.7894, EMR: 0.0000, Acc: 0.4791, F1: 0.3910, Precision: 0.5003, Recall: 0.3727
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.68it/s]
Evaluation, Loss: 50.8252, 0/1 Loss: 1.0000, Hamming Loss: 0.6284, EMR: 0.0000, Acc: 0.7267, F1: 0.5266, Precision: 0.5101, Recall: 0.5664
Saved the best model to path: ./models/task_2/simple_visobert_3.pth

Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [01:21<00:00,  3.25it/s]
Epoch 5/20, Loss: 358.3380, 0/1 Loss: 0.9999, Hamming Loss: 0.7901, EMR: 0.0001, Acc: 0.4851, F1: 0.3925, Precision: 0.4950, Recall: 0.3776
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.71it/s]
Evaluation, Loss: 50.7855, 0/1 Loss: 0.9992, Hamming Loss: 0.6662, EMR: 0.0008, Acc: 0.7373, F1: 0.5120, Precision: 0.4785, Recall: 0.5741
Saved the best model to path: ./models/task_2/simple_visobert_4.pth

Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [01:19<00:00,  3.33it/s]
Epoch 6/20, Loss: 358.4399, 0/1 Loss: 0.9999, Hamming Loss: 0.7935, EMR: 0.0001, Acc: 0.4845, F1: 0.3911, Precision: 0.4932, Recall: 0.3780
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.72it/s]
Evaluation, Loss: 50.6999, 0/1 Loss: 1.0000, Hamming Loss: 0.6616, EMR: 0.0000, Acc: 0.7377, F1: 0.5127, Precision: 0.4803, Recall: 0.5748
Saved the best model to path: ./models/task_2/simple_visobert_5.pth

Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [01:20<00:00,  3.28it/s]
Epoch 7/20, Loss: 358.1844, 0/1 Loss: 0.9998, Hamming Loss: 0.7896, EMR: 0.0002, Acc: 0.4773, F1: 0.3903, Precision: 0.5034, Recall: 0.3726
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.74it/s]
Evaluation, Loss: 50.6286, 0/1 Loss: 1.0000, Hamming Loss: 0.6469, EMR: 0.0000, Acc: 0.7369, F1: 0.5201, Precision: 0.4940, Recall: 0.5740
Saved the best model to path: ./models/task_2/simple_visobert_6.pth

Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [01:20<00:00,  3.29it/s]
Epoch 8/20, Loss: 357.6095, 0/1 Loss: 0.9995, Hamming Loss: 0.7817, EMR: 0.0005, Acc: 0.4883, F1: 0.3973, Precision: 0.5077, Recall: 0.3806
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.66it/s]
Evaluation, Loss: 50.6505, 0/1 Loss: 1.0000, Hamming Loss: 0.7071, EMR: 0.0000, Acc: 0.7407, F1: 0.4946, Precision: 0.4523, Recall: 0.5771

Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [01:19<00:00,  3.33it/s]
Epoch 9/20, Loss: 357.5517, 0/1 Loss: 0.9999, Hamming Loss: 0.7788, EMR: 0.0001, Acc: 0.4888, F1: 0.3984, Precision: 0.5070, Recall: 0.3801
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.67it/s]
Evaluation, Loss: 50.6852, 0/1 Loss: 1.0000, Hamming Loss: 0.6805, EMR: 0.0000, Acc: 0.7351, F1: 0.5040, Precision: 0.4697, Recall: 0.5724

Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [01:20<00:00,  3.26it/s]
Epoch 10/20, Loss: 357.7003, 0/1 Loss: 0.9998, Hamming Loss: 0.7896, EMR: 0.0002, Acc: 0.4793, F1: 0.3897, Precision: 0.4979, Recall: 0.3734
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.68it/s]
Evaluation, Loss: 50.6774, 0/1 Loss: 1.0000, Hamming Loss: 0.6913, EMR: 0.0000, Acc: 0.7378, F1: 0.4992, Precision: 0.4612, Recall: 0.5749

Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [01:19<00:00,  3.33it/s]
Epoch 11/20, Loss: 358.0110, 0/1 Loss: 0.9998, Hamming Loss: 0.7880, EMR: 0.0002, Acc: 0.4834, F1: 0.3934, Precision: 0.5014, Recall: 0.3773
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.66it/s]
Evaluation, Loss: 50.6147, 0/1 Loss: 1.0000, Hamming Loss: 0.6662, EMR: 0.0000, Acc: 0.7402, F1: 0.5110, Precision: 0.4777, Recall: 0.5767
Saved the best model to path: ./models/task_2/simple_visobert_10.pth

Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [01:20<00:00,  3.28it/s]
Epoch 12/20, Loss: 357.8535, 0/1 Loss: 0.9998, Hamming Loss: 0.7858, EMR: 0.0002, Acc: 0.4812, F1: 0.3929, Precision: 0.5026, Recall: 0.3760
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.66it/s]
Evaluation, Loss: 50.7304, 0/1 Loss: 1.0000, Hamming Loss: 0.7060, EMR: 0.0000, Acc: 0.7326, F1: 0.4931, Precision: 0.4552, Recall: 0.5707

Epoch 13/20, Batch 264/264: 100%|██████████| 264/264 [01:20<00:00,  3.28it/s]
Epoch 13/20, Loss: 357.7530, 0/1 Loss: 0.9995, Hamming Loss: 0.7894, EMR: 0.0005, Acc: 0.4871, F1: 0.3925, Precision: 0.4907, Recall: 0.3796
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.66it/s]
Evaluation, Loss: 50.6862, 0/1 Loss: 0.9992, Hamming Loss: 0.7046, EMR: 0.0008, Acc: 0.7407, F1: 0.4951, Precision: 0.4528, Recall: 0.5771

Epoch 14/20, Batch 264/264: 100%|██████████| 264/264 [01:19<00:00,  3.30it/s]
Epoch 14/20, Loss: 357.6228, 0/1 Loss: 0.9998, Hamming Loss: 0.7874, EMR: 0.0002, Acc: 0.4848, F1: 0.3930, Precision: 0.4989, Recall: 0.3783
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.67it/s]
Evaluation, Loss: 50.7362, 0/1 Loss: 1.0000, Hamming Loss: 0.6900, EMR: 0.0000, Acc: 0.7398, F1: 0.5025, Precision: 0.4662, Recall: 0.5763

Epoch 15/20, Batch 264/264: 100%|██████████| 264/264 [01:19<00:00,  3.31it/s]
Epoch 15/20, Loss: 357.6120, 0/1 Loss: 0.9998, Hamming Loss: 0.7826, EMR: 0.0002, Acc: 0.4884, F1: 0.3963, Precision: 0.5047, Recall: 0.3815
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.67it/s]
Evaluation, Loss: 50.6622, 0/1 Loss: 1.0000, Hamming Loss: 0.6929, EMR: 0.0000, Acc: 0.7385, F1: 0.5004, Precision: 0.4633, Recall: 0.5752

Epoch 16/20, Batch 264/264: 100%|██████████| 264/264 [01:19<00:00,  3.34it/s]
Epoch 16/20, Loss: 357.6723, 0/1 Loss: 0.9998, Hamming Loss: 0.7880, EMR: 0.0002, Acc: 0.4787, F1: 0.3897, Precision: 0.4971, Recall: 0.3737
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.64it/s]
Evaluation, Loss: 50.5980, 0/1 Loss: 0.9992, Hamming Loss: 0.6834, EMR: 0.0008, Acc: 0.7400, F1: 0.5037, Precision: 0.4666, Recall: 0.5766
Saved the best model to path: ./models/task_2/simple_visobert_15.pth

Epoch 17/20, Batch 264/264: 100%|██████████| 264/264 [01:20<00:00,  3.27it/s]
Epoch 17/20, Loss: 357.6458, 0/1 Loss: 0.9999, Hamming Loss: 0.7884, EMR: 0.0001, Acc: 0.4779, F1: 0.3911, Precision: 0.5020, Recall: 0.3734
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.68it/s]
Evaluation, Loss: 50.6608, 0/1 Loss: 1.0000, Hamming Loss: 0.7008, EMR: 0.0000, Acc: 0.7407, F1: 0.4978, Precision: 0.4580, Recall: 0.5771

Epoch 18/20, Batch 264/264: 100%|██████████| 264/264 [01:19<00:00,  3.31it/s]
Epoch 18/20, Loss: 357.1806, 0/1 Loss: 0.9996, Hamming Loss: 0.7867, EMR: 0.0004, Acc: 0.4928, F1: 0.3960, Precision: 0.4969, Recall: 0.3846
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.67it/s]
Evaluation, Loss: 50.5692, 0/1 Loss: 1.0000, Hamming Loss: 0.6880, EMR: 0.0000, Acc: 0.7407, F1: 0.5028, Precision: 0.4655, Recall: 0.5771
Saved the best model to path: ./models/task_2/simple_visobert_17.pth

Epoch 19/20, Batch 264/264: 100%|██████████| 264/264 [01:19<00:00,  3.30it/s]
Epoch 19/20, Loss: 357.4908, 0/1 Loss: 0.9999, Hamming Loss: 0.7870, EMR: 0.0001, Acc: 0.4856, F1: 0.3948, Precision: 0.5050, Recall: 0.3789
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.67it/s]
Evaluation, Loss: 50.6696, 0/1 Loss: 1.0000, Hamming Loss: 0.7255, EMR: 0.0000, Acc: 0.7296, F1: 0.4842, Precision: 0.4418, Recall: 0.5682

Epoch 20/20, Batch 264/264: 100%|██████████| 264/264 [01:19<00:00,  3.32it/s]
Epoch 20/20, Loss: 357.4009, 0/1 Loss: 0.9998, Hamming Loss: 0.7903, EMR: 0.0002, Acc: 0.4814, F1: 0.3909, Precision: 0.4939, Recall: 0.3756
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.69it/s]
Evaluation, Loss: 50.7079, 0/1 Loss: 1.0000, Hamming Loss: 0.7286, EMR: 0.0000, Acc: 0.7384, F1: 0.4859, Precision: 0.4404, Recall: 0.5753

uitnlp/CafeBERT

[13:50:58] task: task-2                                                                                   my_import.py:132
           model_type: simple                                                                             my_import.py:132
           model_name: uitnlp/CafeBERT                                                                    my_import.py:132
           padding_len: 512                                                                               my_import.py:130
           batch_size: 32                                                                                 my_import.py:132
           learning_rate: 0.001                                                                           my_import.py:132
           epochs: 20                                                                                     my_import.py:132
           fine_tune: True                                                                                my_import.py:132
           device: cuda                                                                                   my_import.py:132
           saving_path: ./models/task_2/simple_CafeBERT                                                   my_import.py:132
           train_shape: (8437, 27)                                                                        my_import.py:132
           dev_shape: (1205, 27)                                                                          my_import.py:132
           test_shape: (2412, 27)                                                                         my_import.py:132
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/CafeBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Training ...
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [03:52<00:00,  1.14it/s]
Epoch 1/20, Loss: 361.5518, 0/1 Loss: 0.9999, Hamming Loss: 0.8027, EMR: 0.0001, Acc: 0.4681, F1: 0.3849, Precision: 0.4963, Recall: 0.3640
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 51.6295, 0/1 Loss: 1.0000, Hamming Loss: 0.6973, EMR: 0.0000, Acc: 0.6058, F1: 0.4691, Precision: 0.4900, Recall: 0.4710
Saved the best model to path: ./models/task_2/simple_CafeBERT_0.pth

Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.14it/s]
Epoch 2/20, Loss: 360.8752, 0/1 Loss: 0.9999, Hamming Loss: 0.7951, EMR: 0.0001, Acc: 0.4958, F1: 0.3999, Precision: 0.5021, Recall: 0.3863
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 51.5744, 0/1 Loss: 1.0000, Hamming Loss: 0.7073, EMR: 0.0000, Acc: 0.5809, F1: 0.4589, Precision: 0.4893, Recall: 0.4519
Saved the best model to path: ./models/task_2/simple_CafeBERT_1.pth

Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [03:52<00:00,  1.14it/s]
Epoch 3/20, Loss: 360.5711, 0/1 Loss: 1.0000, Hamming Loss: 0.7915, EMR: 0.0000, Acc: 0.4912, F1: 0.4000, Precision: 0.5109, Recall: 0.3820
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 51.4310, 0/1 Loss: 0.9992, Hamming Loss: 0.6560, EMR: 0.0008, Acc: 0.7253, F1: 0.5180, Precision: 0.4920, Recall: 0.5653
Saved the best model to path: ./models/task_2/simple_CafeBERT_2.pth

Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [03:52<00:00,  1.14it/s]
Epoch 4/20, Loss: 360.2852, 0/1 Loss: 0.9999, Hamming Loss: 0.7911, EMR: 0.0001, Acc: 0.4905, F1: 0.3988, Precision: 0.5108, Recall: 0.3819
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 51.3907, 0/1 Loss: 0.9992, Hamming Loss: 0.6544, EMR: 0.0008, Acc: 0.7295, F1: 0.5196, Precision: 0.4920, Recall: 0.5686
Saved the best model to path: ./models/task_2/simple_CafeBERT_3.pth

Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [03:52<00:00,  1.14it/s]
Epoch 5/20, Loss: 360.2019, 0/1 Loss: 1.0000, Hamming Loss: 0.8023, EMR: 0.0000, Acc: 0.4847, F1: 0.3921, Precision: 0.5000, Recall: 0.3781
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 51.4131, 0/1 Loss: 1.0000, Hamming Loss: 0.6388, EMR: 0.0000, Acc: 0.7402, F1: 0.5379, Precision: 0.5246, Recall: 0.5766

Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.15it/s]
Epoch 6/20, Loss: 359.7317, 0/1 Loss: 0.9998, Hamming Loss: 0.7902, EMR: 0.0002, Acc: 0.5012, F1: 0.4033, Precision: 0.5076, Recall: 0.3899
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 51.3564, 0/1 Loss: 1.0000, Hamming Loss: 0.7942, EMR: 0.0000, Acc: 0.7407, F1: 0.4690, Precision: 0.4101, Recall: 0.5771
Saved the best model to path: ./models/task_2/simple_CafeBERT_5.pth

Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.15it/s]
Epoch 7/20, Loss: 359.6921, 0/1 Loss: 1.0000, Hamming Loss: 0.8010, EMR: 0.0000, Acc: 0.5031, F1: 0.3995, Precision: 0.4916, Recall: 0.3916
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 51.2579, 0/1 Loss: 0.9992, Hamming Loss: 0.7164, EMR: 0.0008, Acc: 0.7328, F1: 0.4948, Precision: 0.4550, Recall: 0.5710
Saved the best model to path: ./models/task_2/simple_CafeBERT_6.pth

Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.15it/s]
Epoch 8/20, Loss: 360.0140, 0/1 Loss: 0.9999, Hamming Loss: 0.8057, EMR: 0.0001, Acc: 0.4986, F1: 0.3965, Precision: 0.4896, Recall: 0.3885
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 51.3449, 0/1 Loss: 1.0000, Hamming Loss: 0.7293, EMR: 0.0000, Acc: 0.7407, F1: 0.4945, Precision: 0.4530, Recall: 0.5771

Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.15it/s]
Epoch 9/20, Loss: 359.7653, 0/1 Loss: 0.9999, Hamming Loss: 0.7922, EMR: 0.0001, Acc: 0.5096, F1: 0.4068, Precision: 0.5043, Recall: 0.3973
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 51.2806, 0/1 Loss: 1.0000, Hamming Loss: 0.6380, EMR: 0.0000, Acc: 0.7407, F1: 0.5310, Precision: 0.5059, Recall: 0.5771

Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [03:51<00:00,  1.14it/s]
Epoch 10/20, Loss: 359.6144, 0/1 Loss: 0.9999, Hamming Loss: 0.7914, EMR: 0.0001, Acc: 0.5094, F1: 0.4068, Precision: 0.5023, Recall: 0.3967
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:31<00:00,  1.22it/s]
Evaluation, Loss: 51.3477, 0/1 Loss: 1.0000, Hamming Loss: 0.6981, EMR: 0.0000, Acc: 0.7407, F1: 0.5083, Precision: 0.4763, Recall: 0.5771

Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [03:53<00:00,  1.13it/s]
Epoch 11/20, Loss: 359.6272, 0/1 Loss: 0.9996, Hamming Loss: 0.7963, EMR: 0.0004, Acc: 0.5133, F1: 0.4058, Precision: 0.4960, Recall: 0.3996
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 51.1845, 0/1 Loss: 0.9992, Hamming Loss: 0.7562, EMR: 0.0008, Acc: 0.7407, F1: 0.4811, Precision: 0.4307, Recall: 0.5771
Saved the best model to path: ./models/task_2/simple_CafeBERT_10.pth

Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [03:52<00:00,  1.13it/s]
Epoch 12/20, Loss: 360.0340, 0/1 Loss: 0.9995, Hamming Loss: 0.7803, EMR: 0.0005, Acc: 0.5209, F1: 0.4179, Precision: 0.5166, Recall: 0.4060
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.23it/s]
Evaluation, Loss: 51.3178, 0/1 Loss: 1.0000, Hamming Loss: 0.6239, EMR: 0.0000, Acc: 0.7407, F1: 0.5384, Precision: 0.5201, Recall: 0.5771

Epoch 13/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.15it/s]
Epoch 13/20, Loss: 359.7631, 0/1 Loss: 0.9998, Hamming Loss: 0.7696, EMR: 0.0002, Acc: 0.5167, F1: 0.4196, Precision: 0.5305, Recall: 0.4017
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 51.3040, 0/1 Loss: 1.0000, Hamming Loss: 0.6295, EMR: 0.0000, Acc: 0.7407, F1: 0.5356, Precision: 0.5149, Recall: 0.5771

Epoch 14/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.15it/s]
Epoch 14/20, Loss: 359.7398, 0/1 Loss: 0.9999, Hamming Loss: 0.7774, EMR: 0.0001, Acc: 0.5174, F1: 0.4167, Precision: 0.5167, Recall: 0.4033
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 51.2986, 0/1 Loss: 1.0000, Hamming Loss: 0.7459, EMR: 0.0000, Acc: 0.7407, F1: 0.4877, Precision: 0.4426, Recall: 0.5771

Epoch 15/20, Batch 264/264: 100%|██████████| 264/264 [03:49<00:00,  1.15it/s]
Epoch 15/20, Loss: 359.4157, 0/1 Loss: 0.9999, Hamming Loss: 0.7995, EMR: 0.0001, Acc: 0.5149, F1: 0.4053, Precision: 0.4891, Recall: 0.4009
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 51.1499, 0/1 Loss: 1.0000, Hamming Loss: 0.6670, EMR: 0.0000, Acc: 0.7407, F1: 0.5219, Precision: 0.4980, Recall: 0.5771
Saved the best model to path: ./models/task_2/simple_CafeBERT_14.pth

Epoch 16/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.14it/s]
Epoch 16/20, Loss: 359.3830, 0/1 Loss: 0.9999, Hamming Loss: 0.7991, EMR: 0.0001, Acc: 0.5131, F1: 0.4045, Precision: 0.4910, Recall: 0.4004
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.23it/s]
Evaluation, Loss: 51.2860, 0/1 Loss: 1.0000, Hamming Loss: 0.8004, EMR: 0.0000, Acc: 0.7407, F1: 0.4657, Precision: 0.4050, Recall: 0.5771

Epoch 17/20, Batch 264/264: 100%|██████████| 264/264 [03:51<00:00,  1.14it/s]
Epoch 17/20, Loss: 359.4419, 0/1 Loss: 0.9998, Hamming Loss: 0.7926, EMR: 0.0002, Acc: 0.5179, F1: 0.4095, Precision: 0.4958, Recall: 0.4038
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 51.2606, 0/1 Loss: 1.0000, Hamming Loss: 0.7676, EMR: 0.0000, Acc: 0.7407, F1: 0.4779, Precision: 0.4261, Recall: 0.5771

Epoch 18/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.14it/s]
Epoch 18/20, Loss: 359.5932, 0/1 Loss: 1.0000, Hamming Loss: 0.7929, EMR: 0.0000, Acc: 0.5156, F1: 0.4091, Precision: 0.4976, Recall: 0.4011
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 51.3559, 0/1 Loss: 1.0000, Hamming Loss: 0.7151, EMR: 0.0000, Acc: 0.7407, F1: 0.5042, Precision: 0.4707, Recall: 0.5771

Epoch 19/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.14it/s]
Epoch 19/20, Loss: 359.3535, 0/1 Loss: 0.9998, Hamming Loss: 0.7960, EMR: 0.0002, Acc: 0.5124, F1: 0.4063, Precision: 0.4991, Recall: 0.3994
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 51.3135, 0/1 Loss: 1.0000, Hamming Loss: 0.7110, EMR: 0.0000, Acc: 0.7407, F1: 0.5064, Precision: 0.4743, Recall: 0.5771

Epoch 20/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.15it/s]
Epoch 20/20, Loss: 359.4013, 0/1 Loss: 0.9998, Hamming Loss: 0.7979, EMR: 0.0002, Acc: 0.5074, F1: 0.4030, Precision: 0.4934, Recall: 0.3956
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.26it/s]
Evaluation, Loss: 51.1226, 0/1 Loss: 1.0000, Hamming Loss: 0.7541, EMR: 0.0000, Acc: 0.7407, F1: 0.4820, Precision: 0.4332, Recall: 0.5771
Saved the best model to path: ./models/task_2/simple_CafeBERT_19.pth

xlm-roberta-base

[15:19:18] task: task-2                                                                                   my_import.py:132
           model_type: simple                                                                             my_import.py:132
           model_name: xlm-roberta-base                                                                   my_import.py:132
           padding_len: 512                                                                               my_import.py:130
           batch_size: 32                                                                                 my_import.py:132
           learning_rate: 0.001                                                                           my_import.py:132
           epochs: 20                                                                                     my_import.py:132
           fine_tune: True                                                                                my_import.py:132
           device: cuda                                                                                   my_import.py:132
           saving_path: ./models/task_2/simple_xlm-roberta-base                                           my_import.py:132
           train_shape: (8437, 27)                                                                        my_import.py:132
           dev_shape: (1205, 27)                                                                          my_import.py:132
           test_shape: (2412, 27)                                                                         my_import.py:132

Training ...
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.38it/s]
Epoch 1/20, Loss: 363.0146, 0/1 Loss: 1.0000, Hamming Loss: 0.9144, EMR: 0.0000, Acc: 0.3115, F1: 0.2705, Precision: 0.3438, Recall: 0.2416
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.72it/s]
Evaluation, Loss: 51.8693, 0/1 Loss: 1.0000, Hamming Loss: 0.8662, EMR: 0.0000, Acc: 0.4899, F1: 0.3805, Precision: 0.3907, Recall: 0.3805
Saved the best model to path: ./models/task_2/simple_xlm-roberta-base_0.pth

Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.42it/s]
Epoch 2/20, Loss: 362.7481, 0/1 Loss: 1.0000, Hamming Loss: 0.9187, EMR: 0.0000, Acc: 0.2864, F1: 0.2526, Precision: 0.3286, Recall: 0.2221
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.64it/s]
Evaluation, Loss: 51.7229, 0/1 Loss: 1.0000, Hamming Loss: 0.8662, EMR: 0.0000, Acc: 0.4899, F1: 0.3805, Precision: 0.3907, Recall: 0.3805
Saved the best model to path: ./models/task_2/simple_xlm-roberta-base_1.pth

Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.42it/s]
Epoch 3/20, Loss: 362.7950, 0/1 Loss: 1.0000, Hamming Loss: 0.9176, EMR: 0.0000, Acc: 0.2941, F1: 0.2582, Precision: 0.3338, Recall: 0.2283
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.78it/s]
Evaluation, Loss: 51.8190, 0/1 Loss: 1.0000, Hamming Loss: 0.8662, EMR: 0.0000, Acc: 0.4899, F1: 0.3805, Precision: 0.3907, Recall: 0.3805

Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [01:16<00:00,  3.43it/s]
Epoch 4/20, Loss: 362.6646, 0/1 Loss: 1.0000, Hamming Loss: 0.9172, EMR: 0.0000, Acc: 0.2923, F1: 0.2575, Precision: 0.3349, Recall: 0.2267
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.78it/s]
Evaluation, Loss: 51.8400, 0/1 Loss: 1.0000, Hamming Loss: 0.8662, EMR: 0.0000, Acc: 0.4899, F1: 0.3805, Precision: 0.3907, Recall: 0.3805

Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.38it/s]
Epoch 5/20, Loss: 362.6920, 0/1 Loss: 1.0000, Hamming Loss: 0.9169, EMR: 0.0000, Acc: 0.2933, F1: 0.2592, Precision: 0.3382, Recall: 0.2278
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.72it/s]
Evaluation, Loss: 51.7606, 0/1 Loss: 1.0000, Hamming Loss: 0.8662, EMR: 0.0000, Acc: 0.4899, F1: 0.3805, Precision: 0.3907, Recall: 0.3805

Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.36it/s]
Epoch 6/20, Loss: 362.7761, 0/1 Loss: 1.0000, Hamming Loss: 0.9184, EMR: 0.0000, Acc: 0.2923, F1: 0.2578, Precision: 0.3353, Recall: 0.2268
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.68it/s]
Evaluation, Loss: 51.7834, 0/1 Loss: 1.0000, Hamming Loss: 0.8662, EMR: 0.0000, Acc: 0.4899, F1: 0.3805, Precision: 0.3907, Recall: 0.3805

Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [01:19<00:00,  3.34it/s]
Epoch 7/20, Loss: 362.4700, 0/1 Loss: 1.0000, Hamming Loss: 0.9155, EMR: 0.0000, Acc: 0.2937, F1: 0.2584, Precision: 0.3350, Recall: 0.2275
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.67it/s]
Evaluation, Loss: 51.7753, 0/1 Loss: 1.0000, Hamming Loss: 0.8662, EMR: 0.0000, Acc: 0.4899, F1: 0.3805, Precision: 0.3907, Recall: 0.3805
Early stopping triggered
bert-base-multilingual-cased

[15:29:54] task: task-2                                                                                   my_import.py:132
           model_type: simple                                                                             my_import.py:132
           model_name: bert-base-multilingual-cased                                                       my_import.py:132
           padding_len: 512                                                                               my_import.py:130
           batch_size: 32                                                                                 my_import.py:132
           learning_rate: 0.001                                                                           my_import.py:132
           epochs: 20                                                                                     my_import.py:132
           fine_tune: True                                                                                my_import.py:132
           device: cuda                                                                                   my_import.py:132
           saving_path: ./models/task_2/simple_bert-base-multilingual-cased                               my_import.py:132
           train_shape: (8437, 27)                                                                        my_import.py:132
           dev_shape: (1205, 27)                                                                          my_import.py:132
           test_shape: (2412, 27)                                                                         my_import.py:132

Training ...
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [01:12<00:00,  3.63it/s]
Epoch 1/20, Loss: 361.5374, 0/1 Loss: 0.9979, Hamming Loss: 0.7384, EMR: 0.0021, Acc: 0.5150, F1: 0.4231, Precision: 0.5417, Recall: 0.4006
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.87it/s]
Evaluation, Loss: 51.3764, 0/1 Loss: 0.9834, Hamming Loss: 0.4807, EMR: 0.0166, Acc: 0.9729, F1: 0.6380, Precision: 0.5647, Recall: 0.7611
Saved the best model to path: ./models/task_2/simple_bert-base-multilingual-cased_0.pth

Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [01:10<00:00,  3.74it/s]
Epoch 2/20, Loss: 361.2473, 0/1 Loss: 0.9960, Hamming Loss: 0.7006, EMR: 0.0040, Acc: 0.5892, F1: 0.4707, Precision: 0.5702, Recall: 0.4590
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.99it/s]
Evaluation, Loss: 51.5314, 0/1 Loss: 0.9851, Hamming Loss: 0.5164, EMR: 0.0149, Acc: 0.8695, F1: 0.6076, Precision: 0.5722, Recall: 0.6790

Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [01:09<00:00,  3.81it/s]
Epoch 3/20, Loss: 360.8629, 0/1 Loss: 0.9942, Hamming Loss: 0.6941, EMR: 0.0058, Acc: 0.6188, F1: 0.4832, Precision: 0.5692, Recall: 0.4823
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.99it/s]
Evaluation, Loss: 51.4184, 0/1 Loss: 0.9851, Hamming Loss: 0.4927, EMR: 0.0149, Acc: 0.9324, F1: 0.6276, Precision: 0.5709, Recall: 0.7293

Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [01:10<00:00,  3.75it/s]
Epoch 4/20, Loss: 360.8466, 0/1 Loss: 0.9981, Hamming Loss: 0.7502, EMR: 0.0019, Acc: 0.5483, F1: 0.4357, Precision: 0.5327, Recall: 0.4268
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.98it/s]
Evaluation, Loss: 51.4395, 0/1 Loss: 0.9992, Hamming Loss: 0.6853, EMR: 0.0008, Acc: 0.6369, F1: 0.4819, Precision: 0.4906, Recall: 0.4945

Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [01:09<00:00,  3.81it/s]
Epoch 5/20, Loss: 360.7947, 0/1 Loss: 0.9995, Hamming Loss: 0.8001, EMR: 0.0005, Acc: 0.4714, F1: 0.3861, Precision: 0.4966, Recall: 0.3657
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  4.00it/s]
Evaluation, Loss: 51.3349, 0/1 Loss: 1.0000, Hamming Loss: 0.6894, EMR: 0.0000, Acc: 0.6306, F1: 0.4774, Precision: 0.4869, Recall: 0.4896
Saved the best model to path: ./models/task_2/simple_bert-base-multilingual-cased_4.pth

Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [01:14<00:00,  3.55it/s]
Epoch 6/20, Loss: 360.3808, 0/1 Loss: 0.9994, Hamming Loss: 0.7718, EMR: 0.0006, Acc: 0.5046, F1: 0.4083, Precision: 0.5158, Recall: 0.3922
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.63it/s]
Evaluation, Loss: 51.3147, 0/1 Loss: 0.9967, Hamming Loss: 0.6432, EMR: 0.0033, Acc: 0.7431, F1: 0.5269, Precision: 0.4994, Recall: 0.5784
Saved the best model to path: ./models/task_2/simple_bert-base-multilingual-cased_5.pth

Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.41it/s]
Epoch 7/20, Loss: 360.6250, 0/1 Loss: 0.9977, Hamming Loss: 0.7498, EMR: 0.0023, Acc: 0.5422, F1: 0.4320, Precision: 0.5331, Recall: 0.4216
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.54it/s]
Evaluation, Loss: 51.3316, 0/1 Loss: 0.9635, Hamming Loss: 0.5832, EMR: 0.0365, Acc: 0.9645, F1: 0.5913, Precision: 0.5000, Recall: 0.7550

Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.37it/s]
Epoch 8/20, Loss: 360.4595, 0/1 Loss: 0.9968, Hamming Loss: 0.7113, EMR: 0.0032, Acc: 0.6018, F1: 0.4677, Precision: 0.5571, Recall: 0.4679
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.57it/s]
Evaluation, Loss: 51.3480, 0/1 Loss: 0.9851, Hamming Loss: 0.5054, EMR: 0.0149, Acc: 0.9058, F1: 0.6160, Precision: 0.5641, Recall: 0.7081

Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [01:16<00:00,  3.44it/s]
Epoch 9/20, Loss: 360.6389, 0/1 Loss: 0.9986, Hamming Loss: 0.7483, EMR: 0.0014, Acc: 0.5351, F1: 0.4300, Precision: 0.5439, Recall: 0.4157
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.61it/s]
Evaluation, Loss: 51.3255, 0/1 Loss: 0.9983, Hamming Loss: 0.6340, EMR: 0.0017, Acc: 0.7369, F1: 0.5308, Precision: 0.5094, Recall: 0.5727

Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [01:11<00:00,  3.68it/s]
Epoch 10/20, Loss: 360.4973, 0/1 Loss: 0.9982, Hamming Loss: 0.7491, EMR: 0.0018, Acc: 0.5301, F1: 0.4289, Precision: 0.5450, Recall: 0.4125
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  3.92it/s]
Evaluation, Loss: 51.3523, 0/1 Loss: 0.9851, Hamming Loss: 0.5137, EMR: 0.0149, Acc: 0.8720, F1: 0.6093, Precision: 0.5718, Recall: 0.6809

Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [01:10<00:00,  3.75it/s]
Epoch 11/20, Loss: 360.3657, 0/1 Loss: 0.9968, Hamming Loss: 0.7135, EMR: 0.0032, Acc: 0.5824, F1: 0.4613, Precision: 0.5636, Recall: 0.4523
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.75it/s]
Evaluation, Loss: 51.3206, 0/1 Loss: 0.9851, Hamming Loss: 0.5154, EMR: 0.0149, Acc: 0.8699, F1: 0.6074, Precision: 0.5698, Recall: 0.6794
Early stopping triggered
distilbert-base-multilingual-cased

[15:45:26] task: task-2                                                                                   my_import.py:132
           model_type: simple                                                                             my_import.py:132
           model_name: distilbert-base-multilingual-cased                                                 my_import.py:132
           padding_len: 512                                                                               my_import.py:130
           batch_size: 32                                                                                 my_import.py:132
           learning_rate: 0.001                                                                           my_import.py:132
           epochs: 20                                                                                     my_import.py:132
           fine_tune: True                                                                                my_import.py:132
           device: cuda                                                                                   my_import.py:132
           saving_path: ./models/task_2/simple_distilbert-base-multilingual-cased                         my_import.py:132
           train_shape: (8437, 27)                                                                        my_import.py:132
           dev_shape: (1205, 27)                                                                          my_import.py:132
           test_shape: (2412, 27)                                                                         my_import.py:132

Training ...
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [00:44<00:00,  5.97it/s]
Epoch 1/20, Loss: 360.6009, 0/1 Loss: 0.9998, Hamming Loss: 0.7822, EMR: 0.0002, Acc: 0.4984, F1: 0.4061, Precision: 0.5092, Recall: 0.3880
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.89it/s]
Evaluation, Loss: 51.2502, 0/1 Loss: 1.0000, Hamming Loss: 0.6774, EMR: 0.0000, Acc: 0.6589, F1: 0.4907, Precision: 0.4906, Recall: 0.5125
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_0.pth

Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [00:41<00:00,  6.32it/s]
Epoch 2/20, Loss: 359.8073, 0/1 Loss: 1.0000, Hamming Loss: 0.8071, EMR: 0.0000, Acc: 0.4657, F1: 0.3808, Precision: 0.4932, Recall: 0.3611
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.77it/s]
Evaluation, Loss: 51.2111, 0/1 Loss: 1.0000, Hamming Loss: 0.6876, EMR: 0.0000, Acc: 0.6296, F1: 0.4788, Precision: 0.4902, Recall: 0.4891
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_1.pth

Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [00:41<00:00,  6.29it/s]
Epoch 3/20, Loss: 360.0351, 0/1 Loss: 0.9999, Hamming Loss: 0.8086, EMR: 0.0001, Acc: 0.4575, F1: 0.3770, Precision: 0.4947, Recall: 0.3555
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.85it/s]
Evaluation, Loss: 51.1201, 0/1 Loss: 0.9992, Hamming Loss: 0.6575, EMR: 0.0008, Acc: 0.7407, F1: 0.5210, Precision: 0.4873, Recall: 0.5771
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_2.pth

Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [00:41<00:00,  6.32it/s]
Epoch 4/20, Loss: 359.7485, 0/1 Loss: 0.9998, Hamming Loss: 0.8162, EMR: 0.0002, Acc: 0.4754, F1: 0.3818, Precision: 0.4778, Recall: 0.3701
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.72it/s]
Evaluation, Loss: 51.1368, 0/1 Loss: 1.0000, Hamming Loss: 0.6900, EMR: 0.0000, Acc: 0.6209, F1: 0.4752, Precision: 0.4900, Recall: 0.4821

Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [00:41<00:00,  6.33it/s]
Epoch 5/20, Loss: 359.4607, 0/1 Loss: 0.9999, Hamming Loss: 0.8135, EMR: 0.0001, Acc: 0.4653, F1: 0.3774, Precision: 0.4869, Recall: 0.3618
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.84it/s]
Evaluation, Loss: 51.0966, 0/1 Loss: 0.9992, Hamming Loss: 0.7232, EMR: 0.0008, Acc: 0.7407, F1: 0.4921, Precision: 0.4443, Recall: 0.5771
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_4.pth

Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [00:41<00:00,  6.30it/s]
Epoch 6/20, Loss: 359.2959, 0/1 Loss: 0.9996, Hamming Loss: 0.8214, EMR: 0.0004, Acc: 0.4526, F1: 0.3676, Precision: 0.4714, Recall: 0.3519
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.87it/s]
Evaluation, Loss: 51.1247, 0/1 Loss: 1.0000, Hamming Loss: 0.7071, EMR: 0.0000, Acc: 0.6184, F1: 0.4642, Precision: 0.4711, Recall: 0.4797

Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [00:41<00:00,  6.34it/s]
Epoch 7/20, Loss: 359.1180, 0/1 Loss: 0.9995, Hamming Loss: 0.8183, EMR: 0.0005, Acc: 0.4578, F1: 0.3706, Precision: 0.4747, Recall: 0.3556
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.86it/s]
Evaluation, Loss: 50.9927, 0/1 Loss: 0.9992, Hamming Loss: 0.7087, EMR: 0.0008, Acc: 0.7239, F1: 0.4921, Precision: 0.4533, Recall: 0.5642
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_6.pth

Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [00:41<00:00,  6.31it/s]
Epoch 8/20, Loss: 359.1115, 0/1 Loss: 0.9999, Hamming Loss: 0.8201, EMR: 0.0001, Acc: 0.4638, F1: 0.3731, Precision: 0.4726, Recall: 0.3610
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.90it/s]
Evaluation, Loss: 50.9363, 0/1 Loss: 0.9992, Hamming Loss: 0.7081, EMR: 0.0008, Acc: 0.7039, F1: 0.4853, Precision: 0.4547, Recall: 0.5488
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_7.pth

Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [00:43<00:00,  6.05it/s]
Epoch 9/20, Loss: 359.0384, 0/1 Loss: 0.9998, Hamming Loss: 0.8231, EMR: 0.0002, Acc: 0.4573, F1: 0.3693, Precision: 0.4691, Recall: 0.3560
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.90it/s]
Evaluation, Loss: 51.0098, 0/1 Loss: 1.0000, Hamming Loss: 0.7282, EMR: 0.0000, Acc: 0.6583, F1: 0.4641, Precision: 0.4476, Recall: 0.5120

Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.26it/s]
Epoch 10/20, Loss: 359.0243, 0/1 Loss: 1.0000, Hamming Loss: 0.8204, EMR: 0.0000, Acc: 0.4615, F1: 0.3721, Precision: 0.4728, Recall: 0.3591
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.90it/s]
Evaluation, Loss: 50.8772, 0/1 Loss: 0.9992, Hamming Loss: 0.7191, EMR: 0.0008, Acc: 0.7293, F1: 0.4889, Precision: 0.4463, Recall: 0.5682
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_9.pth

Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.27it/s]
Epoch 11/20, Loss: 358.8532, 0/1 Loss: 0.9998, Hamming Loss: 0.8209, EMR: 0.0002, Acc: 0.4610, F1: 0.3713, Precision: 0.4745, Recall: 0.3584
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.45it/s]
Evaluation, Loss: 50.9172, 0/1 Loss: 0.9992, Hamming Loss: 0.6724, EMR: 0.0008, Acc: 0.7055, F1: 0.5013, Precision: 0.4774, Recall: 0.5504

Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [00:41<00:00,  6.29it/s]
Epoch 12/20, Loss: 358.7160, 0/1 Loss: 0.9995, Hamming Loss: 0.8196, EMR: 0.0005, Acc: 0.4611, F1: 0.3712, Precision: 0.4669, Recall: 0.3586
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.47it/s]
Evaluation, Loss: 50.8768, 0/1 Loss: 0.9992, Hamming Loss: 0.6722, EMR: 0.0008, Acc: 0.7263, F1: 0.5077, Precision: 0.4754, Recall: 0.5661
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_11.pth

Epoch 13/20, Batch 264/264: 100%|██████████| 264/264 [00:41<00:00,  6.34it/s]
Epoch 13/20, Loss: 358.6076, 0/1 Loss: 0.9999, Hamming Loss: 0.8254, EMR: 0.0001, Acc: 0.4601, F1: 0.3690, Precision: 0.4634, Recall: 0.3591
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.80it/s]
Evaluation, Loss: 50.8790, 0/1 Loss: 1.0000, Hamming Loss: 0.7037, EMR: 0.0000, Acc: 0.6666, F1: 0.4751, Precision: 0.4608, Recall: 0.5186

Epoch 14/20, Batch 264/264: 100%|██████████| 264/264 [00:41<00:00,  6.33it/s]
Epoch 14/20, Loss: 358.8275, 0/1 Loss: 0.9996, Hamming Loss: 0.8251, EMR: 0.0004, Acc: 0.4536, F1: 0.3660, Precision: 0.4672, Recall: 0.3537
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.87it/s]
Evaluation, Loss: 50.8766, 0/1 Loss: 0.9992, Hamming Loss: 0.6859, EMR: 0.0008, Acc: 0.7270, F1: 0.5022, Precision: 0.4670, Recall: 0.5666
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_13.pth

Epoch 15/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.28it/s]
Epoch 15/20, Loss: 358.7143, 0/1 Loss: 1.0000, Hamming Loss: 0.8256, EMR: 0.0000, Acc: 0.4585, F1: 0.3681, Precision: 0.4643, Recall: 0.3572
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.40it/s]
Evaluation, Loss: 50.8346, 0/1 Loss: 1.0000, Hamming Loss: 0.6934, EMR: 0.0000, Acc: 0.6783, F1: 0.4832, Precision: 0.4663, Recall: 0.5283
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_14.pth

Epoch 16/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.28it/s]
Epoch 16/20, Loss: 358.7082, 0/1 Loss: 0.9998, Hamming Loss: 0.8211, EMR: 0.0002, Acc: 0.4650, F1: 0.3726, Precision: 0.4651, Recall: 0.3623
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.84it/s]
Evaluation, Loss: 50.8284, 0/1 Loss: 1.0000, Hamming Loss: 0.7114, EMR: 0.0000, Acc: 0.7001, F1: 0.4833, Precision: 0.4537, Recall: 0.5460
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_15.pth

Epoch 17/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.26it/s]
Epoch 17/20, Loss: 358.3544, 0/1 Loss: 1.0000, Hamming Loss: 0.8232, EMR: 0.0000, Acc: 0.4603, F1: 0.3685, Precision: 0.4635, Recall: 0.3581
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.87it/s]
Evaluation, Loss: 50.8099, 0/1 Loss: 0.9992, Hamming Loss: 0.7158, EMR: 0.0008, Acc: 0.6860, F1: 0.4764, Precision: 0.4511, Recall: 0.5343
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_16.pth

Epoch 18/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.27it/s]
Epoch 18/20, Loss: 358.3781, 0/1 Loss: 0.9999, Hamming Loss: 0.8223, EMR: 0.0001, Acc: 0.4517, F1: 0.3661, Precision: 0.4678, Recall: 0.3521
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.82it/s]
Evaluation, Loss: 50.9186, 0/1 Loss: 1.0000, Hamming Loss: 0.6988, EMR: 0.0000, Acc: 0.6183, F1: 0.4655, Precision: 0.4751, Recall: 0.4806

Epoch 19/20, Batch 264/264: 100%|██████████| 264/264 [00:41<00:00,  6.33it/s]
Epoch 19/20, Loss: 358.4401, 0/1 Loss: 0.9999, Hamming Loss: 0.8250, EMR: 0.0001, Acc: 0.4462, F1: 0.3617, Precision: 0.4657, Recall: 0.3478
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.72it/s]
Evaluation, Loss: 50.7656, 0/1 Loss: 0.9992, Hamming Loss: 0.6844, EMR: 0.0008, Acc: 0.7035, F1: 0.4950, Precision: 0.4694, Recall: 0.5490
Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_18.pth

Epoch 20/20, Batch 264/264: 100%|██████████| 264/264 [00:42<00:00,  6.28it/s]
Epoch 20/20, Loss: 357.8453, 0/1 Loss: 0.9999, Hamming Loss: 0.7890, EMR: 0.0001, Acc: 0.4951, F1: 0.3980, Precision: 0.4966, Recall: 0.3861
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.87it/s]
Evaluation, Loss: 50.8061, 0/1 Loss: 1.0000, Hamming Loss: 0.7330, EMR: 0.0000, Acc: 0.7407, F1: 0.4853, Precision: 0.4372, Recall: 0.5771



vinai/phobert-base

[20:20:52] task: task-2                                                                                   my_import.py:133
           model_type: simple                                                                             my_import.py:133
           model_name: vinai/phobert-base                                                                 my_import.py:133
           padding_len: 256                                                                               my_import.py:131
           batch_size: 32                                                                                 my_import.py:133
           learning_rate: 0.001                                                                           my_import.py:133
           epochs: 10                                                                                     my_import.py:133
           fine_tune: True                                                                                my_import.py:133
           device: cuda                                                                                   my_import.py:133
           saving_path: ./models/task_2/simple_phobert-base                                               my_import.py:133
           train_shape: (8437, 27)                                                                        my_import.py:133
           dev_shape: (1205, 27)                                                                          my_import.py:133
           test_shape: (2412, 27)                                                                         my_import.py:133

model_weight_path: ./models/task_2/simple_phobert-base_19.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.51it/s]
Evaluation, Loss: 50.0131,

accs: 0.6149, 0.5602, 0.3610, 0.4988,  => 0.5087

precs (macro): 0.3555, 0.6012, 0.2041, 0.3026,  => 0.3659
recalls (macro): 0.3601, 0.5977, 0.3987, 0.3593,  => 0.4289
f1s (macro): 0.3575, 0.5597, 0.2525, 0.2505,  => 0.3551

precs (micro): 0.6149, 0.5602, 0.3610, 0.4988,  => 0.5087
recalls (micro): 0.6149, 0.5602, 0.3610, 0.4988,  => 0.5087
f1s (micro): 0.6149, 0.5602, 0.3610, 0.4988,  => 0.5087

precs (weighed): 0.6191, 0.6331, 0.2017, 0.7477,  => 0.5504
recalls (weighed): 0.6149, 0.5602, 0.3610, 0.4988,  => 0.5087
f1s (weighed): 0.6167, 0.5565, 0.2505, 0.5886,  => 0.5031

Confusion Matrix of title aspect
[[  0   8   3]
 [  0 622 241]
 [  0 212 119]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.74      0.72      0.73       863
           2       0.33      0.36      0.34       331

    accuracy                           0.61      1205
   macro avg       0.36      0.36      0.36      1205
weighted avg       0.62      0.61      0.62      1205

Confusion Matrix of desc aspect
[[318 420]
 [110 357]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.74      0.43      0.55       738
           2       0.46      0.76      0.57       467

    accuracy                           0.56      1205
   macro avg       0.60      0.60      0.56      1205
weighted avg       0.63      0.56      0.56      1205

Confusion Matrix of company aspect
[[  2   0  31  23]
 [ 20   0 230 335]
 [  2   0 298  97]
 [  0   0  32 135]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.08      0.04      0.05        56
           1       0.00      0.00      0.00       585
           2       0.50      0.75      0.60       397
           3       0.23      0.81      0.36       167

    accuracy                           0.36      1205
   macro avg       0.20      0.40      0.25      1205
weighted avg       0.20      0.36      0.25      1205

Confusion Matrix of other aspect
[[582   0 449]
 [ 67   0  70]
 [ 18   0  19]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.87      0.56      0.69      1031
           2       0.00      0.00      0.00       137
           3       0.04      0.51      0.07        37

    accuracy                           0.50      1205
   macro avg       0.30      0.36      0.25      1205
weighted avg       0.75      0.50      0.59      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:10<00:00,  7.06it/s]
Evaluation, Loss: 100.2821,

accs: 0.6053, 0.5535, 0.3491, 0.4954,  => 0.5008

precs (macro): 0.2644, 0.4021, 0.1816, 0.3037,  => 0.2880
recalls (macro): 0.2695, 0.4000, 0.4054, 0.3578,  => 0.3582
f1s (macro): 0.2665, 0.3689, 0.2404, 0.2538,  => 0.2824

precs (micro): 0.6053, 0.5535, 0.3491, 0.4954,  => 0.5008
recalls (micro): 0.6053, 0.5535, 0.3491, 0.4954,  => 0.5008
f1s (micro): 0.6053, 0.5535, 0.3491, 0.4954,  => 0.5008

precs (weighed): 0.6106, 0.6435, 0.1855, 0.7398,  => 0.5449
recalls (weighed): 0.6053, 0.5535, 0.3491, 0.4954,  => 0.5008
f1s (weighed): 0.6074, 0.5499, 0.2353, 0.5813,  => 0.4935

Confusion Matrix of title aspect
[[   0   26    3    0]
 [   0 1215  503    0]
 [   0  416  245    0]
 [   0    3    1    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.73      0.71      0.72      1718
           2       0.33      0.37      0.35       661
           3       0.00      0.00      0.00         4

    accuracy                           0.61      2412
   macro avg       0.26      0.27      0.27      2412
weighted avg       0.61      0.61      0.61      2412

Confusion Matrix of desc aspect
[[  0   0   2]
 [  0 635 878]
 [  0 197 700]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.76      0.42      0.54      1513
           2       0.44      0.78      0.57       897

    accuracy                           0.55      2412
   macro avg       0.40      0.40      0.37      2412
weighted avg       0.64      0.55      0.55      2412

Confusion Matrix of company aspect
[[  1   0  65  33]
 [ 68   0 512 655]
 [ 10   0 582 186]
 [  1   0  40 259]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.01      0.01      0.01        99
           1       0.00      0.00      0.00      1235
           2       0.49      0.75      0.59       778
           3       0.23      0.86      0.36       300

    accuracy                           0.35      2412
   macro avg       0.18      0.41      0.24      2412
weighted avg       0.19      0.35      0.24      2412

Confusion Matrix of other aspect
[[1146    0  909]
 [ 131    0  131]
 [  46    0   49]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.87      0.56      0.68      2055
           2       0.00      0.00      0.00       262
           3       0.04      0.52      0.08        95

    accuracy                           0.50      2412
   macro avg       0.30      0.36      0.25      2412
weighted avg       0.74      0.50      0.58      2412

uitnlp/visobert

[20:21:30] task: task-2                                                                                   my_import.py:133
           model_type: simple                                                                             my_import.py:133
           model_name: uitnlp/visobert                                                                    my_import.py:133
           padding_len: 512                                                                               my_import.py:131
           batch_size: 32                                                                                 my_import.py:133
           learning_rate: 0.001                                                                           my_import.py:133
           epochs: 10                                                                                     my_import.py:133
           fine_tune: True                                                                                my_import.py:133
           device: cuda                                                                                   my_import.py:133
           saving_path: ./models/task_2/simple_visobert                                                   my_import.py:133
           train_shape: (8437, 27)                                                                        my_import.py:133
           dev_shape: (1205, 27)                                                                          my_import.py:133
           test_shape: (2412, 27)                                                                         my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/visobert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

model_weight_path: ./models/task_2/simple_visobert_17.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:12<00:00,  2.98it/s]
Evaluation, Loss: 50.5692,

accs: 0.0091, 0.5627, 0.1386, 0.5378,  => 0.3120

precs (macro): 0.0030, 0.6226, 0.0346, 0.3483,  => 0.2521
recalls (macro): 0.3333, 0.6107, 0.2500, 0.4461,  => 0.4100
f1s (macro): 0.0060, 0.5601, 0.0609, 0.2775,  => 0.2261

precs (micro): 0.0091, 0.5627, 0.1386, 0.5378,  => 0.3120
recalls (micro): 0.0091, 0.5627, 0.1386, 0.5378,  => 0.3120
f1s (micro): 0.0091, 0.5627, 0.1386, 0.5378,  => 0.3120

precs (weighed): 0.0001, 0.6583, 0.0192, 0.7752,  => 0.3632
recalls (weighed): 0.0091, 0.5627, 0.1386, 0.5378,  => 0.3120
f1s (weighed): 0.0002, 0.5525, 0.0337, 0.6190,  => 0.3014

Confusion Matrix of title aspect
[[ 11   0   0]
 [863   0   0]
 [331   0   0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.01      1.00      0.02        11
           1       0.00      0.00      0.00       863
           2       0.00      0.00      0.00       331

    accuracy                           0.01      1205
   macro avg       0.00      0.33      0.01      1205
weighted avg       0.00      0.01      0.00      1205

Confusion Matrix of desc aspect
[[293 445]
 [ 82 385]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.78      0.40      0.53       738
           2       0.46      0.82      0.59       467

    accuracy                           0.56      1205
   macro avg       0.62      0.61      0.56      1205
weighted avg       0.66      0.56      0.55      1205

Confusion Matrix of company aspect
[[  0   0   0  56]
 [  0   0   0 585]
 [  0   0   0 397]
 [  0   0   0 167]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.00      0.00      0.00       585
           2       0.00      0.00      0.00       397
           3       0.14      1.00      0.24       167

    accuracy                           0.14      1205
   macro avg       0.03      0.25      0.06      1205
weighted avg       0.02      0.14      0.03      1205

Confusion Matrix of other aspect
[[620   9 402]
 [ 66   1  70]
 [ 10   0  27]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.89      0.60      0.72      1031
           2       0.10      0.01      0.01       137
           3       0.05      0.73      0.10        37

    accuracy                           0.54      1205
   macro avg       0.35      0.45      0.28      1205
weighted avg       0.78      0.54      0.62      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:20<00:00,  3.75it/s]
Evaluation, Loss: 101.5459,

accs: 0.0120, 0.5502, 0.1244, 0.5332,  => 0.3049

precs (macro): 0.0030, 0.4095, 0.0311, 0.3869,  => 0.2076
recalls (macro): 0.2500, 0.4036, 0.2500, 0.3948,  => 0.3246
f1s (macro): 0.0059, 0.3659, 0.0553, 0.2882,  => 0.1788

precs (micro): 0.0120, 0.5502, 0.1244, 0.5332,  => 0.3049
recalls (micro): 0.0120, 0.5502, 0.1244, 0.5332,  => 0.3049
f1s (micro): 0.0120, 0.5502, 0.1244, 0.5332,  => 0.3049

precs (weighed): 0.0001, 0.6574, 0.0155, 0.7852,  => 0.3645
recalls (weighed): 0.0120, 0.5502, 0.1244, 0.5332,  => 0.3049
f1s (weighed): 0.0003, 0.5416, 0.0275, 0.6184,  => 0.2969

Confusion Matrix of title aspect
[[  29    0    0    0]
 [1718    0    0    0]
 [ 661    0    0    0]
 [   4    0    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.01      1.00      0.02        29
           1       0.00      0.00      0.00      1718
           2       0.00      0.00      0.00       661
           3       0.00      0.00      0.00         4

    accuracy                           0.01      2412
   macro avg       0.00      0.25      0.01      2412
weighted avg       0.00      0.01      0.00      2412

Confusion Matrix of desc aspect
[[  0   0   2]
 [  0 592 921]
 [  0 162 735]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.79      0.39      0.52      1513
           2       0.44      0.82      0.58       897

    accuracy                           0.55      2412
   macro avg       0.41      0.40      0.37      2412
weighted avg       0.66      0.55      0.54      2412

Confusion Matrix of company aspect
[[   0    0    0   99]
 [   0    0    0 1235]
 [   0    0    0  778]
 [   0    0    0  300]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.00      0.00      0.00      1235
           2       0.00      0.00      0.00       778
           3       0.12      1.00      0.22       300

    accuracy                           0.12      2412
   macro avg       0.03      0.25      0.06      2412
weighted avg       0.02      0.12      0.03      2412

Confusion Matrix of other aspect
[[1225   27  803]
 [ 109    8  145]
 [  40    2   53]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.89      0.60      0.71      2055
           2       0.22      0.03      0.05       262
           3       0.05      0.56      0.10        95

    accuracy                           0.53      2412
   macro avg       0.39      0.39      0.29      2412
weighted avg       0.79      0.53      0.62      2412

uitnlp/CafeBERT

[20:22:22] task: task-2                                                                                                                                       my_import.py:133
           model_type: simple                                                                                                                                 my_import.py:133
           model_name: uitnlp/CafeBERT                                                                                                                        my_import.py:133
           padding_len: 512                                                                                                                                   my_import.py:131
           batch_size: 32                                                                                                                                     my_import.py:133
           learning_rate: 0.001                                                                                                                               my_import.py:133
           epochs: 10                                                                                                                                         my_import.py:133
           fine_tune: True                                                                                                                                    my_import.py:133
           device: cuda                                                                                                                                       my_import.py:133
           saving_path: ./models/task_2/simple_CafeBERT                                                                                                       my_import.py:133
           train_shape: (8437, 27)                                                                                                                            my_import.py:133
           dev_shape: (1205, 27)                                                                                                                              my_import.py:133
           test_shape: (2412, 27)                                                                                                                             my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/CafeBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

model_weight_path: ./models/task_2/simple_CafeBERT_19.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:32<00:00,  1.18it/s]
Evaluation, Loss: 51.1226,

accs: 0.0091, 0.4672, 0.1386, 0.3685,  => 0.2459

precs (macro): 0.0030, 0.5636, 0.0346, 0.2941,  => 0.2238
recalls (macro): 0.3333, 0.5403, 0.2500, 0.3520,  => 0.3689
f1s (macro): 0.0060, 0.4442, 0.0609, 0.2049,  => 0.1790

precs (micro): 0.0091, 0.4672, 0.1386, 0.3685,  => 0.2459
recalls (micro): 0.0091, 0.4672, 0.1386, 0.3685,  => 0.2459
f1s (micro): 0.0091, 0.4672, 0.1386, 0.3685,  => 0.2459

precs (weighed): 0.0001, 0.5979, 0.0192, 0.7270,  => 0.3361
recalls (weighed): 0.0091, 0.4672, 0.1386, 0.3685,  => 0.2459
f1s (weighed): 0.0002, 0.4188, 0.0337, 0.4729,  => 0.2314

Confusion Matrix of title aspect
[[ 11   0   0]
 [863   0   0]
 [331   0   0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.01      1.00      0.02        11
           1       0.00      0.00      0.00       863
           2       0.00      0.00      0.00       331

    accuracy                           0.01      1205
   macro avg       0.00      0.33      0.01      1205
weighted avg       0.00      0.01      0.00      1205

Confusion Matrix of desc aspect
[[159 579]
 [ 63 404]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.72      0.22      0.33       738
           2       0.41      0.87      0.56       467

    accuracy                           0.47      1205
   macro avg       0.56      0.54      0.44      1205
weighted avg       0.60      0.47      0.42      1205

Confusion Matrix of company aspect
[[  0   0   0  56]
 [  0   0   0 585]
 [  0   0   0 397]
 [  0   0   0 167]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.00      0.00      0.00       585
           2       0.00      0.00      0.00       397
           3       0.14      1.00      0.24       167

    accuracy                           0.14      1205
   macro avg       0.03      0.25      0.06      1205
weighted avg       0.02      0.14      0.03      1205

Confusion Matrix of other aspect
[[420   0 611]
 [ 62   0  75]
 [ 13   0  24]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.85      0.41      0.55      1031
           2       0.00      0.00      0.00       137
           3       0.03      0.65      0.06        37

    accuracy                           0.37      1205
   macro avg       0.29      0.35      0.20      1205
weighted avg       0.73      0.37      0.47      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [01:00<00:00,  1.25it/s]
Evaluation, Loss: 102.5942,

accs: 0.0120, 0.4507, 0.1244, 0.3437,  => 0.2327

precs (macro): 0.0030, 0.3745, 0.0311, 0.2943,  => 0.1757
recalls (macro): 0.2500, 0.3584, 0.2500, 0.3085,  => 0.2917
f1s (macro): 0.0059, 0.2851, 0.0553, 0.1961,  => 0.1356

precs (micro): 0.0120, 0.4507, 0.1244, 0.3437,  => 0.2327
recalls (micro): 0.0120, 0.4507, 0.1244, 0.3437,  => 0.2327
f1s (micro): 0.0120, 0.4507, 0.1244, 0.3437,  => 0.2327

precs (weighed): 0.0001, 0.6044, 0.0155, 0.7241,  => 0.3360
recalls (weighed): 0.0120, 0.4507, 0.1244, 0.3437,  => 0.2327
f1s (weighed): 0.0003, 0.3980, 0.0275, 0.4482,  => 0.2185

Confusion Matrix of title aspect
[[  29    0    0    0]
 [1718    0    0    0]
 [ 661    0    0    0]
 [   4    0    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.01      1.00      0.02        29
           1       0.00      0.00      0.00      1718
           2       0.00      0.00      0.00       661
           3       0.00      0.00      0.00         4

    accuracy                           0.01      2412
   macro avg       0.00      0.25      0.01      2412
weighted avg       0.00      0.01      0.00      2412

Confusion Matrix of desc aspect
[[   0    0    2]
 [   0  301 1212]
 [   0  111  786]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.73      0.20      0.31      1513
           2       0.39      0.88      0.54       897

    accuracy                           0.45      2412
   macro avg       0.37      0.36      0.29      2412
weighted avg       0.60      0.45      0.40      2412

Confusion Matrix of company aspect
[[   0    0    0   99]
 [   0    0    0 1235]
 [   0    0    0  778]
 [   0    0    0  300]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.00      0.00      0.00      1235
           2       0.00      0.00      0.00       778
           3       0.12      1.00      0.22       300

    accuracy                           0.12      2412
   macro avg       0.03      0.25      0.06      2412
weighted avg       0.02      0.12      0.03      2412

Confusion Matrix of other aspect
[[ 777    0 1278]
 [  96    0  166]
 [  43    0   52]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.85      0.38      0.52      2055
           2       0.00      0.00      0.00       262
           3       0.03      0.55      0.07        95

    accuracy                           0.34      2412
   macro avg       0.29      0.31      0.20      2412
weighted avg       0.72      0.34      0.45      2412

xlm-roberta-base

[20:24:15] task: task-2                                                                                                                                       my_import.py:133
           model_type: simple                                                                                                                                 my_import.py:133
           model_name: xlm-roberta-base                                                                                                                       my_import.py:133
           padding_len: 512                                                                                                                                   my_import.py:131
           batch_size: 32                                                                                                                                     my_import.py:133
           learning_rate: 0.001                                                                                                                               my_import.py:133
           epochs: 10                                                                                                                                         my_import.py:133
           fine_tune: True                                                                                                                                    my_import.py:133
           device: cuda                                                                                                                                       my_import.py:133
           saving_path: ./models/task_2/simple_xlm-roberta-base                                                                                               my_import.py:133
           train_shape: (8437, 27)                                                                                                                            my_import.py:133
           dev_shape: (1205, 27)                                                                                                                              my_import.py:133
           test_shape: (2412, 27)                                                                                                                             my_import.py:133

model_weight_path: ./models/task_2/simple_xlm-roberta-base_1.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:13<00:00,  2.91it/s]
Evaluation, Loss: 51.7229,

accs: 0.0091, 0.3876, 0.1386, 0.0000,  => 0.1338

precs (macro): 0.0030, 0.1938, 0.0346, 0.0000,  => 0.0579
recalls (macro): 0.3333, 0.5000, 0.2500, 0.0000,  => 0.2708
f1s (macro): 0.0060, 0.2793, 0.0609, 0.0000,  => 0.0865

precs (micro): 0.0091, 0.3876, 0.1386, 0.0000,  => 0.1338
recalls (micro): 0.0091, 0.3876, 0.1386, 0.0000,  => 0.1338
f1s (micro): 0.0091, 0.3876, 0.1386, 0.0000,  => 0.1338

precs (weighed): 0.0001, 0.1502, 0.0192, 0.0000,  => 0.0424
recalls (weighed): 0.0091, 0.3876, 0.1386, 0.0000,  => 0.1338
f1s (weighed): 0.0002, 0.2165, 0.0337, 0.0000,  => 0.0626

Confusion Matrix of title aspect
[[ 11   0   0]
 [863   0   0]
 [331   0   0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.01      1.00      0.02        11
           1       0.00      0.00      0.00       863
           2       0.00      0.00      0.00       331

    accuracy                           0.01      1205
   macro avg       0.00      0.33      0.01      1205
weighted avg       0.00      0.01      0.00      1205

Confusion Matrix of desc aspect
[[  0 738]
 [  0 467]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.00      0.00      0.00       738
           2       0.39      1.00      0.56       467

    accuracy                           0.39      1205
   macro avg       0.19      0.50      0.28      1205
weighted avg       0.15      0.39      0.22      1205

Confusion Matrix of company aspect
[[  0   0   0  56]
 [  0   0   0 585]
 [  0   0   0 397]
 [  0   0   0 167]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.00      0.00      0.00       585
           2       0.00      0.00      0.00       397
           3       0.14      1.00      0.24       167

    accuracy                           0.14      1205
   macro avg       0.03      0.25      0.06      1205
weighted avg       0.02      0.14      0.03      1205

Confusion Matrix of other aspect
[[   0    0    0    0]
 [1031    0    0    0]
 [ 137    0    0    0]
 [  37    0    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       0.0
           1       0.00      0.00      0.00    1031.0
           2       0.00      0.00      0.00     137.0
           3       0.00      0.00      0.00      37.0

    accuracy                           0.00    1205.0
   macro avg       0.00      0.00      0.00    1205.0
weighted avg       0.00      0.00      0.00    1205.0

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:20<00:00,  3.76it/s]
Evaluation, Loss: 103.8393,

accs: 0.0120, 0.3719, 0.1244, 0.0000,  => 0.1271

precs (macro): 0.0030, 0.1240, 0.0311, 0.0000,  => 0.0395
recalls (macro): 0.2500, 0.3333, 0.2500, 0.0000,  => 0.2083
f1s (macro): 0.0059, 0.1807, 0.0553, 0.0000,  => 0.0605

precs (micro): 0.0120, 0.3719, 0.1244, 0.0000,  => 0.1271
recalls (micro): 0.0120, 0.3719, 0.1244, 0.0000,  => 0.1271
f1s (micro): 0.0120, 0.3719, 0.1244, 0.0000,  => 0.1271

precs (weighed): 0.0001, 0.1383, 0.0155, 0.0000,  => 0.0385
recalls (weighed): 0.0120, 0.3719, 0.1244, 0.0000,  => 0.1271
f1s (weighed): 0.0003, 0.2016, 0.0275, 0.0000,  => 0.0574

Confusion Matrix of title aspect
[[  29    0    0    0]
 [1718    0    0    0]
 [ 661    0    0    0]
 [   4    0    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.01      1.00      0.02        29
           1       0.00      0.00      0.00      1718
           2       0.00      0.00      0.00       661
           3       0.00      0.00      0.00         4

    accuracy                           0.01      2412
   macro avg       0.00      0.25      0.01      2412
weighted avg       0.00      0.01      0.00      2412

Confusion Matrix of desc aspect
[[   0    0    2]
 [   0    0 1513]
 [   0    0  897]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.00      0.00      0.00      1513
           2       0.37      1.00      0.54       897

    accuracy                           0.37      2412
   macro avg       0.12      0.33      0.18      2412
weighted avg       0.14      0.37      0.20      2412

Confusion Matrix of company aspect
[[   0    0    0   99]
 [   0    0    0 1235]
 [   0    0    0  778]
 [   0    0    0  300]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.00      0.00      0.00      1235
           2       0.00      0.00      0.00       778
           3       0.12      1.00      0.22       300

    accuracy                           0.12      2412
   macro avg       0.03      0.25      0.06      2412
weighted avg       0.02      0.12      0.03      2412

Confusion Matrix of other aspect
[[   0    0    0    0]
 [2055    0    0    0]
 [ 262    0    0    0]
 [  95    0    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       0.0
           1       0.00      0.00      0.00    2055.0
           2       0.00      0.00      0.00     262.0
           3       0.00      0.00      0.00      95.0

    accuracy                           0.00    2412.0
   macro avg       0.00      0.00      0.00    2412.0
weighted avg       0.00      0.00      0.00    2412.0

bert-base-multilingual-cased

[20:25:05] task: task-2                                                                                                                                       my_import.py:133
           model_type: simple                                                                                                                                 my_import.py:133
           model_name: bert-base-multilingual-cased                                                                                                           my_import.py:133
           padding_len: 512                                                                                                                                   my_import.py:131
           batch_size: 32                                                                                                                                     my_import.py:133
           learning_rate: 0.001                                                                                                                               my_import.py:133
           epochs: 10                                                                                                                                         my_import.py:133
           fine_tune: True                                                                                                                                    my_import.py:133
           device: cuda                                                                                                                                       my_import.py:133
           saving_path: ./models/task_2/simple_bert-base-multilingual-cased                                                                                   my_import.py:133
           train_shape: (8437, 27)                                                                                                                            my_import.py:133
           dev_shape: (1205, 27)                                                                                                                              my_import.py:133
           test_shape: (2412, 27)                                                                                                                             my_import.py:133

model_weight_path: ./models/task_2/simple_bert-base-multilingual-cased_5.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:12<00:00,  3.01it/s]
Evaluation, Loss: 51.3147,

accs: 0.1054, 0.3436, 0.1386, 0.8398,  => 0.3568

precs (macro): 0.2248, 0.1323, 0.0346, 0.3098,  => 0.1754
recalls (macro): 0.3482, 0.2955, 0.2500, 0.3446,  => 0.3096
f1s (macro): 0.0815, 0.1828, 0.0609, 0.3255,  => 0.1626

precs (micro): 0.1054, 0.3436, 0.1386, 0.8398,  => 0.3568
recalls (micro): 0.1054, 0.3436, 0.1386, 0.8398,  => 0.3568
f1s (micro): 0.1054, 0.3436, 0.1386, 0.8398,  => 0.3568

precs (weighed): 0.4762, 0.1538, 0.0192, 0.7364,  => 0.3464
recalls (weighed): 0.1054, 0.3436, 0.1386, 0.8398,  => 0.3568
f1s (weighed): 0.1615, 0.2125, 0.0337, 0.7846,  => 0.2981

Confusion Matrix of title aspect
[[ 10   1   0]
 [746 117   0]
 [273  58   0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.01      0.91      0.02        11
           1       0.66      0.14      0.23       863
           2       0.00      0.00      0.00       331

    accuracy                           0.11      1205
   macro avg       0.22      0.35      0.08      1205
weighted avg       0.48      0.11      0.16      1205

Confusion Matrix of desc aspect
[[  0   0   0]
 [109   0 629]
 [ 53   0 414]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.00      0.00      0.00       738
           2       0.40      0.89      0.55       467

    accuracy                           0.34      1205
   macro avg       0.13      0.30      0.18      1205
weighted avg       0.15      0.34      0.21      1205

Confusion Matrix of company aspect
[[  0   0   0  56]
 [  0   0   0 585]
 [  0   0   0 397]
 [  0   0   0 167]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.00      0.00      0.00       585
           2       0.00      0.00      0.00       397
           3       0.14      1.00      0.24       167

    accuracy                           0.14      1205
   macro avg       0.03      0.25      0.06      1205
weighted avg       0.02      0.14      0.03      1205

Confusion Matrix of other aspect
[[1010    0   21]
 [ 132    0    5]
 [  35    0    2]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.86      0.98      0.91      1031
           2       0.00      0.00      0.00       137
           3       0.07      0.05      0.06        37

    accuracy                           0.84      1205
   macro avg       0.31      0.34      0.33      1205
weighted avg       0.74      0.84      0.78      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:19<00:00,  3.91it/s]
Evaluation, Loss: 102.9673,

accs: 0.1119, 0.3416, 0.1244, 0.8354,  => 0.3533

precs (macro): 0.1678, 0.1318, 0.0311, 0.2910,  => 0.1554
recalls (macro): 0.2427, 0.4725, 0.2500, 0.3302,  => 0.3238
f1s (macro): 0.0646, 0.1853, 0.0553, 0.3085,  => 0.1534

precs (micro): 0.1119, 0.3416, 0.1244, 0.8354,  => 0.3533
recalls (micro): 0.1119, 0.3416, 0.1244, 0.8354,  => 0.3533
f1s (micro): 0.1119, 0.3416, 0.1244, 0.8354,  => 0.3533

precs (weighed): 0.4699, 0.1459, 0.0155, 0.7264,  => 0.3394
recalls (weighed): 0.1119, 0.3416, 0.1244, 0.8354,  => 0.3533
f1s (weighed): 0.1679, 0.2044, 0.0275, 0.7770,  => 0.2942

Confusion Matrix of title aspect
[[  24    5    0    0]
 [1472  246    0    0]
 [ 540  121    0    0]
 [   3    1    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.01      0.83      0.02        29
           1       0.66      0.14      0.24      1718
           2       0.00      0.00      0.00       661
           3       0.00      0.00      0.00         4

    accuracy                           0.11      2412
   macro avg       0.17      0.24      0.06      2412
weighted avg       0.47      0.11      0.17      2412

Confusion Matrix of desc aspect
[[   1    0    1]
 [ 239    0 1274]
 [  74    0  823]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.50      0.01         2
           1       0.00      0.00      0.00      1513
           2       0.39      0.92      0.55       897

    accuracy                           0.34      2412
   macro avg       0.13      0.47      0.19      2412
weighted avg       0.15      0.34      0.20      2412

Confusion Matrix of company aspect
[[   0    0    0   99]
 [   1    0    0 1234]
 [   0    0    0  778]
 [   0    0    0  300]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.00      0.00      0.00      1235
           2       0.00      0.00      0.00       778
           3       0.12      1.00      0.22       300

    accuracy                           0.12      2412
   macro avg       0.03      0.25      0.06      2412
weighted avg       0.02      0.12      0.03      2412

Confusion Matrix of other aspect
[[2014    0   41]
 [ 257    0    5]
 [  94    0    1]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.85      0.98      0.91      2055
           2       0.00      0.00      0.00       262
           3       0.02      0.01      0.01        95

    accuracy                           0.84      2412
   macro avg       0.29      0.33      0.31      2412
weighted avg       0.73      0.84      0.78      2412

distilbert-base-multilingual-cased

[20:25:51] task: task-2                                                                                                                                       my_import.py:133
           model_type: simple                                                                                                                                 my_import.py:133
           model_name: distilbert-base-multilingual-cased                                                                                                     my_import.py:133
           padding_len: 512                                                                                                                                   my_import.py:131
           batch_size: 32                                                                                                                                     my_import.py:133
           learning_rate: 0.001                                                                                                                               my_import.py:133
           epochs: 10                                                                                                                                         my_import.py:133
           fine_tune: True                                                                                                                                    my_import.py:133
           device: cuda                                                                                                                                       my_import.py:133
           saving_path: ./models/task_2/simple_distilbert-base-multilingual-cased                                                                             my_import.py:133
           train_shape: (8437, 27)                                                                                                                            my_import.py:133
           dev_shape: (1205, 27)                                                                                                                              my_import.py:133
           test_shape: (2412, 27)                                                                                                                             my_import.py:133

model_weight_path: ./models/task_2/simple_distilbert-base-multilingual-cased_18.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.39it/s]
Evaluation, Loss: 50.7656,

accs: 0.0091, 0.3552, 0.1386, 0.7593,  => 0.3156

precs (macro): 0.0030, 0.1391, 0.0346, 0.3297,  => 0.1266
recalls (macro): 0.3333, 0.3055, 0.2500, 0.4609,  => 0.3374
f1s (macro): 0.0060, 0.1911, 0.0609, 0.3464,  => 0.1511

precs (micro): 0.0091, 0.3552, 0.1386, 0.7593,  => 0.3156
recalls (micro): 0.0091, 0.3552, 0.1386, 0.7593,  => 0.3156
f1s (micro): 0.0091, 0.3552, 0.1386, 0.7593,  => 0.3156

precs (weighed): 0.0001, 0.1617, 0.0192, 0.7702,  => 0.2378
recalls (weighed): 0.0091, 0.3552, 0.1386, 0.7593,  => 0.3156
f1s (weighed): 0.0002, 0.2222, 0.0337, 0.7601,  => 0.2540

Confusion Matrix of title aspect
[[ 11   0   0]
 [863   0   0]
 [331   0   0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.01      1.00      0.02        11
           1       0.00      0.00      0.00       863
           2       0.00      0.00      0.00       331

    accuracy                           0.01      1205
   macro avg       0.00      0.33      0.01      1205
weighted avg       0.00      0.01      0.00      1205

Confusion Matrix of desc aspect
[[  0   0   0]
 [140   0 598]
 [ 39   0 428]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.00      0.00      0.00       738
           2       0.42      0.92      0.57       467

    accuracy                           0.36      1205
   macro avg       0.14      0.31      0.19      1205
weighted avg       0.16      0.36      0.22      1205

Confusion Matrix of company aspect
[[  0   0   0  56]
 [  0   0   0 585]
 [  0   0   0 397]
 [  0   0   0 167]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.00      0.00      0.00       585
           2       0.00      0.00      0.00       397
           3       0.14      1.00      0.24       167

    accuracy                           0.14      1205
   macro avg       0.03      0.25      0.06      1205
weighted avg       0.02      0.14      0.03      1205

Confusion Matrix of other aspect
[[896   0 135]
 [ 85   0  52]
 [ 18   0  19]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.90      0.87      0.88      1031
           2       0.00      0.00      0.00       137
           3       0.09      0.51      0.16        37

    accuracy                           0.76      1205
   macro avg       0.33      0.46      0.35      1205
weighted avg       0.77      0.76      0.76      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:11<00:00,  6.72it/s]
Evaluation, Loss: 102.0505,

accs: 0.0120, 0.3503, 0.1244, 0.7608,  => 0.3119

precs (macro): 0.0030, 0.1364, 0.0311, 0.3246,  => 0.1238
recalls (macro): 0.2500, 0.3140, 0.2500, 0.4014,  => 0.3039
f1s (macro): 0.0059, 0.1902, 0.0553, 0.3375,  => 0.1472

precs (micro): 0.0120, 0.3503, 0.1244, 0.7608,  => 0.3119
recalls (micro): 0.0120, 0.3503, 0.1244, 0.7608,  => 0.3119
f1s (micro): 0.0120, 0.3503, 0.1244, 0.7608,  => 0.3119

precs (weighed): 0.0001, 0.1522, 0.0155, 0.7666,  => 0.2336
recalls (weighed): 0.0120, 0.3503, 0.1244, 0.7608,  => 0.3119
f1s (weighed): 0.0003, 0.2122, 0.0275, 0.7606,  => 0.2501

Confusion Matrix of title aspect
[[  29    0    0    0]
 [1718    0    0    0]
 [ 661    0    0    0]
 [   4    0    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.01      1.00      0.02        29
           1       0.00      0.00      0.00      1718
           2       0.00      0.00      0.00       661
           3       0.00      0.00      0.00         4

    accuracy                           0.01      2412
   macro avg       0.00      0.25      0.01      2412
weighted avg       0.00      0.01      0.00      2412

Confusion Matrix of desc aspect
[[   0    0    2]
 [ 295    0 1218]
 [  52    0  845]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.00      0.00      0.00      1513
           2       0.41      0.94      0.57       897

    accuracy                           0.35      2412
   macro avg       0.14      0.31      0.19      2412
weighted avg       0.15      0.35      0.21      2412

Confusion Matrix of company aspect
[[   0    0    0   99]
 [   0    0    0 1235]
 [   0    0    0  778]
 [   0    0    0  300]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.00      0.00      0.00      1235
           2       0.00      0.00      0.00       778
           3       0.12      1.00      0.22       300

    accuracy                           0.12      2412
   macro avg       0.03      0.25      0.06      2412
weighted avg       0.02      0.12      0.03      2412

Confusion Matrix of other aspect
[[1804    0  251]
 [ 145    0  117]
 [  64    0   31]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.90      0.88      0.89      2055
           2       0.00      0.00      0.00       262
           3       0.08      0.33      0.13        95

    accuracy                           0.76      2412
   macro avg       0.32      0.40      0.34      2412
weighted avg       0.77      0.76      0.76      2412

All commands completed!