################# TRAINING #################


vinai/phobert-base

[12:09:53] task: task-2                                                                                my_import.py:133
           model_type: simple                                                                          my_import.py:133
           model_name: vinai/phobert-base                                                              my_import.py:133
           padding_len: 256                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/simple_phobert-base                                            my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

Training ...
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [00:41<00:00,  6.36it/s]
Epoch 1/20, Loss: 1182.0938,

accs: 0.4903, 0.3638, 0.3849, 0.5057,  => 0.4362

precs (macro): 0.2538, 0.1516, 0.2871, 0.2277,  => 0.2301
recalls (macro): 0.2612, 0.2301, 0.2639, 0.2116,  => 0.2417
f1s (macro): 0.2323, 0.1493, 0.2412, 0.1755,  => 0.1996

precs (micro): 0.4903, 0.3638, 0.3849, 0.5057,  => 0.4362
recalls (micro): 0.4903, 0.3638, 0.3849, 0.5057,  => 0.4362
f1s (micro): 0.4903, 0.3638, 0.3849, 0.5057,  => 0.4362

precs (weighed): 0.5957, 0.3756, 0.4105, 0.7271,  => 0.5272
recalls (weighed): 0.4903, 0.3638, 0.3849, 0.5057,  => 0.4362
f1s (weighed): 0.5367, 0.3695, 0.3830, 0.5942,  => 0.4708

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  6.99it/s]
Evaluation, Loss: 146.1463,

accs: 0.7162, 0.6124, 0.5527, 0.8556,  => 0.6842

precs (macro): 0.2387, 0.3062, 0.2975, 0.2852,  => 0.2819
recalls (macro): 0.3333, 0.5000, 0.3030, 0.3333,  => 0.3674
f1s (macro): 0.2782, 0.3798, 0.2590, 0.3074,  => 0.3061

precs (micro): 0.7162, 0.6124, 0.5527, 0.8556,  => 0.6842
recalls (micro): 0.7162, 0.6124, 0.5527, 0.8556,  => 0.6842
f1s (micro): 0.7162, 0.6124, 0.5527, 0.8556,  => 0.6842

precs (weighed): 0.5129, 0.3751, 0.4763, 0.7321,  => 0.5241
recalls (weighed): 0.7162, 0.6124, 0.5527, 0.8556,  => 0.6842
f1s (weighed): 0.5977, 0.4652, 0.4500, 0.7890,  => 0.5755

Saved the best model to path: ./models/task_2/simple_phobert-base_0.pth

Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.94it/s]
Epoch 2/20, Loss: 1146.4608,

accs: 0.5048, 0.3800, 0.4026, 0.5092,  => 0.4492

precs (macro): 0.2555, 0.1570, 0.2580, 0.2959,  => 0.2416
recalls (macro): 0.2451, 0.1533, 0.2835, 0.3375,  => 0.2549
f1s (macro): 0.2354, 0.1551, 0.2588, 0.1766,  => 0.2065

precs (micro): 0.5048, 0.3800, 0.4026, 0.5092,  => 0.4492
recalls (micro): 0.5048, 0.3800, 0.4026, 0.5092,  => 0.4492
f1s (micro): 0.5048, 0.3800, 0.4026, 0.5092,  => 0.4492

precs (weighed): 0.6018, 0.3890, 0.4176, 0.7593,  => 0.5419
recalls (weighed): 0.5048, 0.3800, 0.4026, 0.5092,  => 0.4492
f1s (weighed): 0.5484, 0.3844, 0.4074, 0.5969,  => 0.4843

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.46it/s]
Evaluation, Loss: 138.2109,

accs: 0.7187, 0.6124, 0.6050, 0.8556,  => 0.6979

precs (macro): 0.5727, 0.3062, 0.3028, 0.2852,  => 0.3667
recalls (macro): 0.3364, 0.5000, 0.3490, 0.3333,  => 0.3797
f1s (macro): 0.2846, 0.3798, 0.3149, 0.3074,  => 0.3217

precs (micro): 0.7187, 0.6124, 0.6050, 0.8556,  => 0.6979
recalls (micro): 0.7187, 0.6124, 0.6050, 0.8556,  => 0.6979
f1s (micro): 0.7187, 0.6124, 0.6050, 0.8556,  => 0.6979

precs (weighed): 0.7889, 0.3751, 0.4933, 0.7321,  => 0.5973
recalls (weighed): 0.7187, 0.6124, 0.6050, 0.8556,  => 0.6979
f1s (weighed): 0.6035, 0.4652, 0.5293, 0.7890,  => 0.5968

Saved the best model to path: ./models/task_2/simple_phobert-base_1.pth

Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.89it/s]
Epoch 3/20, Loss: 1151.0743,

accs: 0.4929, 0.3781, 0.4030, 0.5044,  => 0.4446

precs (macro): 0.2531, 0.1560, 0.2615, 0.2119,  => 0.2206
recalls (macro): 0.2678, 0.4024, 0.2815, 0.3359,  => 0.3219
f1s (macro): 0.2332, 0.1545, 0.2591, 0.1749,  => 0.2054

precs (micro): 0.4929, 0.3781, 0.4030, 0.5044,  => 0.4446
recalls (micro): 0.4929, 0.3781, 0.4030, 0.5044,  => 0.4446
f1s (micro): 0.4929, 0.3781, 0.4030, 0.5044,  => 0.4446

precs (weighed): 0.5940, 0.3861, 0.4252, 0.7188,  => 0.5310
recalls (weighed): 0.4929, 0.3781, 0.4030, 0.5044,  => 0.4446
f1s (weighed): 0.5375, 0.3819, 0.4112, 0.5926,  => 0.4808

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.40it/s]
Evaluation, Loss: 139.6704,

accs: 0.7220, 0.6100, 0.6108, 0.8556,  => 0.6996

precs (macro): 0.5735, 0.2057, 0.2976, 0.2852,  => 0.3405
recalls (macro): 0.3404, 0.3320, 0.3570, 0.3333,  => 0.3407
f1s (macro): 0.2930, 0.2540, 0.3217, 0.3074,  => 0.2940

precs (micro): 0.7220, 0.6100, 0.6108, 0.8556,  => 0.6996
recalls (micro): 0.7220, 0.6100, 0.6108, 0.8556,  => 0.6996
f1s (micro): 0.7220, 0.6100, 0.6108, 0.8556,  => 0.6996

precs (weighed): 0.7906, 0.3780, 0.4916, 0.7321,  => 0.5981
recalls (weighed): 0.7220, 0.6100, 0.6108, 0.8556,  => 0.6996
f1s (weighed): 0.6112, 0.4667, 0.5403, 0.7890,  => 0.6018


Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.85it/s]
Epoch 4/20, Loss: 1148.8150,

accs: 0.4873, 0.3622, 0.4131, 0.5028,  => 0.4413

precs (macro): 0.2513, 0.1552, 0.2688, 0.2121,  => 0.2219
recalls (macro): 0.2455, 0.3960, 0.2953, 0.2730,  => 0.3024
f1s (macro): 0.2293, 0.1508, 0.2675, 0.1746,  => 0.2055

precs (micro): 0.4873, 0.3622, 0.4131, 0.5028,  => 0.4413
recalls (micro): 0.4873, 0.3622, 0.4131, 0.5028,  => 0.4413
f1s (micro): 0.4873, 0.3622, 0.4131, 0.5028,  => 0.4413

precs (weighed): 0.5923, 0.3840, 0.4349, 0.7201,  => 0.5328
recalls (weighed): 0.4873, 0.3622, 0.4131, 0.5028,  => 0.4413
f1s (weighed): 0.5338, 0.3726, 0.4206, 0.5920,  => 0.4797

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.40it/s]
Evaluation, Loss: 138.4805,

accs: 0.7311, 0.6091, 0.6183, 0.8556,  => 0.7035

precs (macro): 0.4669, 0.2061, 0.3126, 0.2852,  => 0.3177
recalls (macro): 0.3626, 0.3315, 0.3817, 0.3333,  => 0.3523
f1s (macro): 0.3420, 0.2542, 0.3386, 0.3074,  => 0.3106

precs (micro): 0.7311, 0.6091, 0.6183, 0.8556,  => 0.7035
recalls (micro): 0.7311, 0.6091, 0.6183, 0.8556,  => 0.7035
f1s (micro): 0.7311, 0.6091, 0.6183, 0.8556,  => 0.7035

precs (weighed): 0.7089, 0.3787, 0.5299, 0.7321,  => 0.5874
recalls (weighed): 0.7311, 0.6091, 0.6183, 0.8556,  => 0.7035
f1s (weighed): 0.6523, 0.4671, 0.5629, 0.7890,  => 0.6178


Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.86it/s]
Epoch 5/20, Loss: 1137.7881,

accs: 0.4967, 0.3713, 0.4052, 0.5150,  => 0.4471

precs (macro): 0.2562, 0.1573, 0.3994, 0.2120,  => 0.2562
recalls (macro): 0.2521, 0.1498, 0.2891, 0.2141,  => 0.2263
f1s (macro): 0.2354, 0.1535, 0.2650, 0.1769,  => 0.2077

precs (micro): 0.4967, 0.3713, 0.4052, 0.5150,  => 0.4471
recalls (micro): 0.4967, 0.3713, 0.4052, 0.5150,  => 0.4471
f1s (micro): 0.4967, 0.3713, 0.4052, 0.5150,  => 0.4471

precs (weighed): 0.5997, 0.3898, 0.5004, 0.7199,  => 0.5524
recalls (weighed): 0.4967, 0.3713, 0.4052, 0.5150,  => 0.4471
f1s (weighed): 0.5425, 0.3803, 0.4159, 0.6004,  => 0.4847

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.41it/s]
Evaluation, Loss: 134.7038,

accs: 0.7162, 0.6091, 0.6440, 0.8556,  => 0.7062

precs (macro): 0.2387, 0.2065, 0.5614, 0.2852,  => 0.3229
recalls (macro): 0.3333, 0.3315, 0.3985, 0.3333,  => 0.3492
f1s (macro): 0.2782, 0.2545, 0.3817, 0.3074,  => 0.3054

precs (micro): 0.7162, 0.6091, 0.6440, 0.8556,  => 0.7062
recalls (micro): 0.7162, 0.6091, 0.6440, 0.8556,  => 0.7062
f1s (micro): 0.7162, 0.6091, 0.6440, 0.8556,  => 0.7062

precs (weighed): 0.5129, 0.3794, 0.6572, 0.7321,  => 0.5704
recalls (weighed): 0.7162, 0.6091, 0.6440, 0.8556,  => 0.7062
f1s (weighed): 0.5977, 0.4675, 0.5921, 0.7890,  => 0.6116

Saved the best model to path: ./models/task_2/simple_phobert-base_4.pth

Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.84it/s]
Epoch 6/20, Loss: 1139.6761,

accs: 0.5015, 0.3651, 0.4208, 0.5017,  => 0.4473

precs (macro): 0.2603, 0.1561, 0.3825, 0.2119,  => 0.2527
recalls (macro): 0.2594, 0.2306, 0.3160, 0.2726,  => 0.2696
f1s (macro): 0.2384, 0.1517, 0.3135, 0.1743,  => 0.2195

precs (micro): 0.5015, 0.3651, 0.4208, 0.5017,  => 0.4473
recalls (micro): 0.5015, 0.3651, 0.4208, 0.5017,  => 0.4473
f1s (micro): 0.5015, 0.3651, 0.4208, 0.5017,  => 0.4473

precs (weighed): 0.6069, 0.3868, 0.4978, 0.7191,  => 0.5526
recalls (weighed): 0.5015, 0.3651, 0.4208, 0.5017,  => 0.4473
f1s (weighed): 0.5482, 0.3755, 0.4440, 0.5909,  => 0.4896

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.38it/s]
Evaluation, Loss: 137.1206,

accs: 0.7402, 0.6100, 0.6481, 0.8556,  => 0.7135

precs (macro): 0.4489, 0.2059, 0.5050, 0.2852,  => 0.3613
recalls (macro): 0.4048, 0.3320, 0.4235, 0.3333,  => 0.3734
f1s (macro): 0.4087, 0.2541, 0.4159, 0.3074,  => 0.3465

precs (micro): 0.7402, 0.6100, 0.6481, 0.8556,  => 0.7135
recalls (micro): 0.7402, 0.6100, 0.6481, 0.8556,  => 0.7135
f1s (micro): 0.7402, 0.6100, 0.6481, 0.8556,  => 0.7135

precs (weighed): 0.7081, 0.3783, 0.6401, 0.7321,  => 0.6146
recalls (weighed): 0.7402, 0.6100, 0.6481, 0.8556,  => 0.7135
f1s (weighed): 0.7060, 0.4670, 0.6140, 0.7890,  => 0.6440


Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.88it/s]
Epoch 7/20, Loss: 1132.0039,

accs: 0.4947, 0.3706, 0.4295, 0.5035,  => 0.4496

precs (macro): 0.2622, 0.1592, 0.3857, 0.2118,  => 0.2547
recalls (macro): 0.2721, 0.1496, 0.3367, 0.2107,  => 0.2423
f1s (macro): 0.2391, 0.1542, 0.3421, 0.1745,  => 0.2275

precs (micro): 0.4947, 0.3706, 0.4295, 0.5035,  => 0.4496
recalls (micro): 0.4947, 0.3706, 0.4295, 0.5035,  => 0.4496
f1s (micro): 0.4947, 0.3706, 0.4295, 0.5035,  => 0.4496

precs (weighed): 0.6086, 0.3946, 0.5026, 0.7192,  => 0.5563
recalls (weighed): 0.4947, 0.3706, 0.4295, 0.5035,  => 0.4496
f1s (weighed): 0.5444, 0.3822, 0.4583, 0.5922,  => 0.4943

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.40it/s]
Evaluation, Loss: 136.5536,

accs: 0.7286, 0.6091, 0.6523, 0.8556,  => 0.7114

precs (macro): 0.5018, 0.2082, 0.5190, 0.2852,  => 0.3785
recalls (macro): 0.3522, 0.3315, 0.4634, 0.3333,  => 0.3701
f1s (macro): 0.3190, 0.2558, 0.4651, 0.3074,  => 0.3368

precs (micro): 0.7286, 0.6091, 0.6523, 0.8556,  => 0.7114
recalls (micro): 0.7286, 0.6091, 0.6523, 0.8556,  => 0.7114
f1s (micro): 0.7286, 0.6091, 0.6523, 0.8556,  => 0.7114

precs (weighed): 0.7347, 0.3826, 0.6598, 0.7321,  => 0.6273
recalls (weighed): 0.7286, 0.6091, 0.6523, 0.8556,  => 0.7114
f1s (weighed): 0.6337, 0.4700, 0.6356, 0.7890,  => 0.6321


Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.88it/s]
Epoch 8/20, Loss: 1136.1476,

accs: 0.4880, 0.3721, 0.4252, 0.5097,  => 0.4487

precs (macro): 0.2570, 0.1583, 0.3853, 0.2125,  => 0.2533
recalls (macro): 0.2539, 0.2334, 0.3361, 0.1501,  => 0.2434
f1s (macro): 0.2335, 0.1542, 0.3335, 0.1759,  => 0.2243

precs (micro): 0.4880, 0.3721, 0.4252, 0.5097,  => 0.4487
recalls (micro): 0.4880, 0.3721, 0.4252, 0.5097,  => 0.4487
f1s (micro): 0.4880, 0.3721, 0.4252, 0.5097,  => 0.4487

precs (weighed): 0.5994, 0.3920, 0.5058, 0.7219,  => 0.5548
recalls (weighed): 0.4880, 0.3721, 0.4252, 0.5097,  => 0.4487
f1s (weighed): 0.5369, 0.3817, 0.4547, 0.5975,  => 0.4927

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.39it/s]
Evaluation, Loss: 134.7361,

accs: 0.7369, 0.6108, 0.6581, 0.8556,  => 0.7154

precs (macro): 0.4971, 0.2058, 0.5448, 0.2852,  => 0.3832
recalls (macro): 0.3660, 0.3324, 0.4223, 0.3333,  => 0.3635
f1s (macro): 0.3459, 0.2542, 0.4176, 0.3074,  => 0.3313

precs (micro): 0.7369, 0.6108, 0.6581, 0.8556,  => 0.7154
recalls (micro): 0.7369, 0.6108, 0.6581, 0.8556,  => 0.7154
f1s (micro): 0.7369, 0.6108, 0.6581, 0.8556,  => 0.7154

precs (weighed): 0.7346, 0.3782, 0.6548, 0.7321,  => 0.6249
recalls (weighed): 0.7369, 0.6108, 0.6581, 0.8556,  => 0.7154
f1s (weighed): 0.6572, 0.4671, 0.6166, 0.7890,  => 0.6325


Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.86it/s]
Epoch 9/20, Loss: 1123.4679,

accs: 0.4973, 0.3703, 0.4423, 0.5127,  => 0.4557

precs (macro): 0.2614, 0.1577, 0.4081, 0.2119,  => 0.2598
recalls (macro): 0.2605, 0.2327, 0.3600, 0.2759,  => 0.2823
f1s (macro): 0.2393, 0.1535, 0.3630, 0.1765,  => 0.2331

precs (micro): 0.4973, 0.3703, 0.4423, 0.5127,  => 0.4557
recalls (micro): 0.4973, 0.3703, 0.4423, 0.5127,  => 0.4557
f1s (micro): 0.4973, 0.3703, 0.4423, 0.5127,  => 0.4557

precs (weighed): 0.6094, 0.3906, 0.5281, 0.7191,  => 0.5618
recalls (weighed): 0.4973, 0.3703, 0.4423, 0.5127,  => 0.4557
f1s (weighed): 0.5465, 0.3801, 0.4766, 0.5985,  => 0.5004

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.28it/s]
Evaluation, Loss: 132.9747,

accs: 0.7237, 0.6083, 0.6490, 0.8556,  => 0.7091

precs (macro): 0.5437, 0.2085, 0.5344, 0.2852,  => 0.3929
recalls (macro): 0.3430, 0.3311, 0.4323, 0.3333,  => 0.3599
f1s (macro): 0.2989, 0.2558, 0.4445, 0.3074,  => 0.3266

precs (micro): 0.7237, 0.6083, 0.6490, 0.8556,  => 0.7091
recalls (micro): 0.7237, 0.6083, 0.6490, 0.8556,  => 0.7091
f1s (micro): 0.7237, 0.6083, 0.6490, 0.8556,  => 0.7091

precs (weighed): 0.7668, 0.3830, 0.6397, 0.7321,  => 0.6304
recalls (weighed): 0.7237, 0.6083, 0.6490, 0.8556,  => 0.7091
f1s (weighed): 0.6163, 0.4701, 0.6083, 0.7890,  => 0.6209

Saved the best model to path: ./models/task_2/simple_phobert-base_8.pth

Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.86it/s]
Epoch 10/20, Loss: 1126.7350,

accs: 0.4977, 0.3653, 0.4391, 0.5016,  => 0.4509

precs (macro): 0.2634, 0.1590, 0.4010, 0.2116,  => 0.2588
recalls (macro): 0.2614, 0.1474, 0.3474, 0.2726,  => 0.2572
f1s (macro): 0.2406, 0.1530, 0.3567, 0.1742,  => 0.2311

precs (micro): 0.4977, 0.3653, 0.4391, 0.5016,  => 0.4509
recalls (micro): 0.4977, 0.3653, 0.4391, 0.5016,  => 0.4509
f1s (micro): 0.4977, 0.3653, 0.4391, 0.5016,  => 0.4509

precs (weighed): 0.6117, 0.3940, 0.5166, 0.7184,  => 0.5602
recalls (weighed): 0.4977, 0.3653, 0.4391, 0.5016,  => 0.4509
f1s (weighed): 0.5476, 0.3791, 0.4705, 0.5906,  => 0.4969

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.12it/s]
Evaluation, Loss: 132.8110,

accs: 0.7320, 0.6100, 0.6855, 0.8556,  => 0.7207

precs (macro): 0.5007, 0.2064, 0.5467, 0.2852,  => 0.3848
recalls (macro): 0.3574, 0.3320, 0.4635, 0.3333,  => 0.3716
f1s (macro): 0.3296, 0.2545, 0.4740, 0.3074,  => 0.3414

precs (micro): 0.7320, 0.6100, 0.6855, 0.8556,  => 0.7207
recalls (micro): 0.7320, 0.6100, 0.6855, 0.8556,  => 0.7207
f1s (micro): 0.7320, 0.6100, 0.6855, 0.8556,  => 0.7207

precs (weighed): 0.7353, 0.3792, 0.6659, 0.7321,  => 0.6281
recalls (weighed): 0.7320, 0.6100, 0.6855, 0.8556,  => 0.7207
f1s (weighed): 0.6429, 0.4677, 0.6545, 0.7890,  => 0.6385

Saved the best model to path: ./models/task_2/simple_phobert-base_9.pth

Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  6.98it/s]
Epoch 11/20, Loss: 1118.1240,

accs: 0.5061, 0.3684, 0.4311, 0.5108,  => 0.4541

precs (macro): 0.2646, 0.1600, 0.4005, 0.2138,  => 0.2597
recalls (macro): 0.2549, 0.2319, 0.3515, 0.4003,  => 0.3097
f1s (macro): 0.2419, 0.1542, 0.3528, 0.1770,  => 0.2315

precs (micro): 0.5061, 0.3684, 0.4311, 0.5108,  => 0.4541
recalls (micro): 0.5061, 0.3684, 0.4311, 0.5108,  => 0.4541
f1s (micro): 0.5061, 0.3684, 0.4311, 0.5108,  => 0.4541

precs (weighed): 0.6138, 0.3964, 0.5174, 0.7252,  => 0.5632
recalls (weighed): 0.5061, 0.3684, 0.4311, 0.5108,  => 0.4541
f1s (weighed): 0.5539, 0.3818, 0.4649, 0.5991,  => 0.4999

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.60it/s]
Evaluation, Loss: 132.7136,

accs: 0.7336, 0.6100, 0.6432, 0.8556,  => 0.7106

precs (macro): 0.5308, 0.2061, 0.5616, 0.2852,  => 0.3959
recalls (macro): 0.3570, 0.3320, 0.4202, 0.3333,  => 0.3606
f1s (macro): 0.3272, 0.2543, 0.4320, 0.3074,  => 0.3302

precs (micro): 0.7336, 0.6100, 0.6432, 0.8556,  => 0.7106
recalls (micro): 0.7336, 0.6100, 0.6432, 0.8556,  => 0.7106
f1s (micro): 0.7336, 0.6100, 0.6432, 0.8556,  => 0.7106

precs (weighed): 0.7599, 0.3786, 0.6575, 0.7321,  => 0.6320
recalls (weighed): 0.7336, 0.6100, 0.6432, 0.8556,  => 0.7106
f1s (weighed): 0.6416, 0.4672, 0.5937, 0.7890,  => 0.6229

Saved the best model to path: ./models/task_2/simple_phobert-base_10.pth

Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  7.00it/s]
Epoch 12/20, Loss: 1117.1861,

accs: 0.5035, 0.3651, 0.4484, 0.5120,  => 0.4572

precs (macro): 0.2660, 0.1575, 0.4186, 0.2123,  => 0.2636
recalls (macro): 0.2683, 0.1473, 0.3677, 0.1508,  => 0.2335
f1s (macro): 0.2429, 0.1522, 0.3703, 0.1763,  => 0.2355

precs (micro): 0.5035, 0.3651, 0.4484, 0.5120,  => 0.4572
recalls (micro): 0.5035, 0.3651, 0.4484, 0.5120,  => 0.4572
f1s (micro): 0.5035, 0.3651, 0.4484, 0.5120,  => 0.4572

precs (weighed): 0.6154, 0.3903, 0.5386, 0.7211,  => 0.5663
recalls (weighed): 0.5035, 0.3651, 0.4484, 0.5120,  => 0.4572
f1s (weighed): 0.5527, 0.3773, 0.4841, 0.5988,  => 0.5032

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.59it/s]
Evaluation, Loss: 130.0889,

accs: 0.7286, 0.6075, 0.6772, 0.8556,  => 0.7172

precs (macro): 0.5402, 0.2066, 0.5685, 0.2852,  => 0.4001
recalls (macro): 0.3497, 0.3306, 0.4448, 0.3333,  => 0.3646
f1s (macro): 0.3125, 0.2543, 0.4540, 0.3074,  => 0.3320

precs (micro): 0.7286, 0.6075, 0.6772, 0.8556,  => 0.7172
recalls (micro): 0.7286, 0.6075, 0.6772, 0.8556,  => 0.7172
f1s (micro): 0.7286, 0.6075, 0.6772, 0.8556,  => 0.7172

precs (weighed): 0.7657, 0.3796, 0.6724, 0.7321,  => 0.6374
recalls (weighed): 0.7286, 0.6075, 0.6772, 0.8556,  => 0.7172
f1s (weighed): 0.6286, 0.4672, 0.6395, 0.7890,  => 0.6311

Saved the best model to path: ./models/task_2/simple_phobert-base_11.pth

Epoch 13/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  7.00it/s]
Epoch 13/20, Loss: 1116.9120,

accs: 0.4991, 0.3628, 0.4461, 0.5167,  => 0.4562

precs (macro): 0.2677, 0.1603, 0.4223, 0.2135,  => 0.2659
recalls (macro): 0.2350, 0.2297, 0.3622, 0.1521,  => 0.2448
f1s (macro): 0.2415, 0.1531, 0.3674, 0.1776,  => 0.2349

precs (micro): 0.4991, 0.3628, 0.4461, 0.5167,  => 0.4562
recalls (micro): 0.4991, 0.3628, 0.4461, 0.5167,  => 0.4562
f1s (micro): 0.4991, 0.3628, 0.4461, 0.5167,  => 0.4562

precs (weighed): 0.6184, 0.3971, 0.5386, 0.7250,  => 0.5698
recalls (weighed): 0.4991, 0.3628, 0.4461, 0.5167,  => 0.4562
f1s (weighed): 0.5519, 0.3791, 0.4818, 0.6034,  => 0.5041

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.55it/s]
Evaluation, Loss: 134.0134,

accs: 0.7452, 0.6066, 0.6838, 0.8556,  => 0.7228

precs (macro): 0.5375, 0.2081, 0.5575, 0.2852,  => 0.3971
recalls (macro): 0.3723, 0.3302, 0.4636, 0.3333,  => 0.3748
f1s (macro): 0.3550, 0.2553, 0.4738, 0.3074,  => 0.3479

precs (micro): 0.7452, 0.6066, 0.6838, 0.8556,  => 0.7228
recalls (micro): 0.7452, 0.6066, 0.6838, 0.8556,  => 0.7228
f1s (micro): 0.7452, 0.6066, 0.6838, 0.8556,  => 0.7228

precs (weighed): 0.7696, 0.3823, 0.6722, 0.7321,  => 0.6390
recalls (weighed): 0.7452, 0.6066, 0.6838, 0.8556,  => 0.7228
f1s (weighed): 0.6670, 0.4690, 0.6549, 0.7890,  => 0.6450


Epoch 14/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  7.00it/s]
Epoch 14/20, Loss: 1121.9084,

accs: 0.4893, 0.3670, 0.4395, 0.5075,  => 0.4508

precs (macro): 0.2629, 0.1587, 0.4150, 0.2126,  => 0.2623
recalls (macro): 0.2483, 0.3146, 0.3631, 0.2744,  => 0.3001
f1s (macro): 0.2369, 0.1534, 0.3658, 0.1757,  => 0.2329

precs (micro): 0.4893, 0.3670, 0.4395, 0.5075,  => 0.4508
recalls (micro): 0.4893, 0.3670, 0.4395, 0.5075,  => 0.4508
f1s (micro): 0.4893, 0.3670, 0.4395, 0.5075,  => 0.4508

precs (weighed): 0.6109, 0.3930, 0.5359, 0.7215,  => 0.5653
recalls (weighed): 0.4893, 0.3670, 0.4395, 0.5075,  => 0.4508
f1s (weighed): 0.5424, 0.3794, 0.4777, 0.5957,  => 0.4988

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.56it/s]
Evaluation, Loss: 132.9620,

accs: 0.7494, 0.6058, 0.7046, 0.8556,  => 0.7288

precs (macro): 0.4913, 0.2090, 0.5557, 0.2852,  => 0.3853
recalls (macro): 0.3891, 0.3297, 0.5001, 0.3333,  => 0.3881
f1s (macro): 0.3854, 0.2559, 0.5126, 0.3074,  => 0.3653

precs (micro): 0.7494, 0.6058, 0.7046, 0.8556,  => 0.7288
recalls (micro): 0.7494, 0.6058, 0.7046, 0.8556,  => 0.7288
f1s (micro): 0.7494, 0.6058, 0.7046, 0.8556,  => 0.7288

precs (weighed): 0.7366, 0.3841, 0.6797, 0.7321,  => 0.6331
recalls (weighed): 0.7494, 0.6058, 0.7046, 0.8556,  => 0.7288
f1s (weighed): 0.6917, 0.4701, 0.6770, 0.7890,  => 0.6570


Epoch 15/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  7.02it/s]
Epoch 15/20, Loss: 1115.0188,

accs: 0.5081, 0.3622, 0.4365, 0.5100,  => 0.4542

precs (macro): 0.2703, 0.1605, 0.4189, 0.4641,  => 0.3285
recalls (macro): 0.2679, 0.1462, 0.3514, 0.2129,  => 0.2446
f1s (macro): 0.2473, 0.1530, 0.3624, 0.1771,  => 0.2350

precs (micro): 0.5081, 0.3622, 0.4365, 0.5100,  => 0.4542
recalls (micro): 0.5081, 0.3622, 0.4365, 0.5100,  => 0.4542
f1s (micro): 0.5081, 0.3622, 0.4365, 0.5100,  => 0.4542

precs (weighed): 0.6248, 0.3978, 0.5370, 0.8408,  => 0.6001
recalls (weighed): 0.5081, 0.3622, 0.4365, 0.5100,  => 0.4542
f1s (weighed): 0.5592, 0.3792, 0.4764, 0.5995,  => 0.5036

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.57it/s]
Evaluation, Loss: 135.4719,

accs: 0.7253, 0.6075, 0.6440, 0.8556,  => 0.7081

precs (macro): 0.5301, 0.2064, 0.5510, 0.2852,  => 0.3932
recalls (macro): 0.3457, 0.3306, 0.3940, 0.3333,  => 0.3509
f1s (macro): 0.3046, 0.2542, 0.3868, 0.3074,  => 0.3133

precs (micro): 0.7253, 0.6075, 0.6440, 0.8556,  => 0.7081
recalls (micro): 0.7253, 0.6075, 0.6440, 0.8556,  => 0.7081
f1s (micro): 0.7253, 0.6075, 0.6440, 0.8556,  => 0.7081

precs (weighed): 0.7562, 0.3793, 0.6504, 0.7321,  => 0.6295
recalls (weighed): 0.7253, 0.6075, 0.6440, 0.8556,  => 0.7081
f1s (weighed): 0.6214, 0.4670, 0.5862, 0.7890,  => 0.6159


Epoch 16/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  7.02it/s]
Epoch 16/20, Loss: 1112.5187,

accs: 0.5105, 0.3671, 0.4444, 0.5060,  => 0.4570

precs (macro): 0.2695, 0.1618, 0.4230, 0.2956,  => 0.2875
recalls (macro): 0.2745, 0.2314, 0.3617, 0.2117,  => 0.2698
f1s (macro): 0.2467, 0.1547, 0.3703, 0.1757,  => 0.2368

precs (micro): 0.5105, 0.3671, 0.4444, 0.5060,  => 0.4570
recalls (micro): 0.5105, 0.3671, 0.4444, 0.5060,  => 0.4570
f1s (micro): 0.5105, 0.3671, 0.4444, 0.5060,  => 0.4570

precs (weighed): 0.6216, 0.4007, 0.5400, 0.7587,  => 0.5802
recalls (weighed): 0.5105, 0.3671, 0.4444, 0.5060,  => 0.4570
f1s (weighed): 0.5594, 0.3831, 0.4824, 0.5946,  => 0.5049

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.55it/s]
Evaluation, Loss: 131.4097,

accs: 0.7485, 0.6066, 0.6888, 0.8556,  => 0.7249

precs (macro): 0.5124, 0.2084, 0.5598, 0.2852,  => 0.3915
recalls (macro): 0.3813, 0.3302, 0.4803, 0.3333,  => 0.3813
f1s (macro): 0.3716, 0.2555, 0.4953, 0.3074,  => 0.3575

precs (micro): 0.7485, 0.6066, 0.6888, 0.8556,  => 0.7249
recalls (micro): 0.7485, 0.6066, 0.6888, 0.8556,  => 0.7249
f1s (micro): 0.7485, 0.6066, 0.6888, 0.8556,  => 0.7249

precs (weighed): 0.7516, 0.3830, 0.6752, 0.7321,  => 0.6355
recalls (weighed): 0.7485, 0.6066, 0.6888, 0.8556,  => 0.7249
f1s (weighed): 0.6809, 0.4695, 0.6563, 0.7890,  => 0.6489


Epoch 17/20, Batch 264/264: 100%|██████████| 264/264 [00:37<00:00,  7.04it/s]
Epoch 17/20, Loss: 1115.1831,

accs: 0.4971, 0.3745, 0.4415, 0.5079,  => 0.4553

precs (macro): 0.2650, 0.2860, 0.4247, 0.4625,  => 0.3595
recalls (macro): 0.2861, 0.2381, 0.3641, 0.2747,  => 0.2907
f1s (macro): 0.2426, 0.1713, 0.3742, 0.1762,  => 0.2411

precs (micro): 0.4971, 0.3745, 0.4415, 0.5079,  => 0.4553
recalls (micro): 0.4971, 0.3745, 0.4415, 0.5079,  => 0.4553
f1s (micro): 0.4971, 0.3745, 0.4415, 0.5079,  => 0.4553

precs (weighed): 0.6133, 0.5899, 0.5409, 0.8352,  => 0.6448
recalls (weighed): 0.4971, 0.3745, 0.4415, 0.5079,  => 0.4553
f1s (weighed): 0.5474, 0.4070, 0.4820, 0.5960,  => 0.5081

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.58it/s]
Evaluation, Loss: 127.9100,

accs: 0.7552, 0.6456, 0.6996, 0.8556,  => 0.7390

precs (macro): 0.5080, 0.7198, 0.5645, 0.2852,  => 0.5194
recalls (macro): 0.3931, 0.5479, 0.4898, 0.3333,  => 0.4410
f1s (macro): 0.3906, 0.4857, 0.5043, 0.3074,  => 0.4220

precs (micro): 0.7552, 0.6456, 0.6996, 0.8556,  => 0.7390
recalls (micro): 0.7552, 0.6456, 0.6996, 0.8556,  => 0.7390
f1s (micro): 0.7552, 0.6456, 0.6996, 0.8556,  => 0.7390

precs (weighed): 0.7516, 0.7011, 0.6806, 0.7321,  => 0.7163
recalls (weighed): 0.7552, 0.6456, 0.6996, 0.8556,  => 0.7390
f1s (weighed): 0.6976, 0.5502, 0.6741, 0.7890,  => 0.6777

Saved the best model to path: ./models/task_2/simple_phobert-base_16.pth

Epoch 18/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.93it/s]
Epoch 18/20, Loss: 1088.2910,

accs: 0.4944, 0.4790, 0.4499, 0.5174,  => 0.4852

precs (macro): 0.2675, 0.2745, 0.4340, 0.3237,  => 0.3249
recalls (macro): 0.2685, 0.2272, 0.3762, 0.2157,  => 0.2719
f1s (macro): 0.2418, 0.2486, 0.3831, 0.1795,  => 0.2633

precs (micro): 0.4944, 0.4790, 0.4499, 0.5174,  => 0.4852
recalls (micro): 0.4944, 0.4790, 0.4499, 0.5174,  => 0.4852
f1s (micro): 0.4944, 0.4790, 0.4499, 0.5174,  => 0.4852

precs (weighed): 0.6175, 0.5744, 0.5516, 0.7724,  => 0.6290
recalls (weighed): 0.4944, 0.4790, 0.4499, 0.5174,  => 0.4852
f1s (weighed): 0.5478, 0.5222, 0.4909, 0.6033,  => 0.5410

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.20it/s]
Evaluation, Loss: 128.5002,

accs: 0.7452, 0.6647, 0.7037, 0.8556,  => 0.7423

precs (macro): 0.5280, 0.6686, 0.5669, 0.2852,  => 0.5122
recalls (macro): 0.3735, 0.5906, 0.5035, 0.3333,  => 0.4503
f1s (macro): 0.3576, 0.5736, 0.5169, 0.3074,  => 0.4389

precs (micro): 0.7452, 0.6647, 0.7037, 0.8556,  => 0.7423
recalls (micro): 0.7452, 0.6647, 0.7037, 0.8556,  => 0.7423
f1s (micro): 0.7452, 0.6647, 0.7037, 0.8556,  => 0.7423

precs (weighed): 0.7622, 0.6673, 0.6860, 0.7321,  => 0.7119
recalls (weighed): 0.7452, 0.6647, 0.7037, 0.8556,  => 0.7423
f1s (weighed): 0.6690, 0.6180, 0.6829, 0.7890,  => 0.6897


Epoch 19/20, Batch 264/264: 100%|██████████| 264/264 [00:39<00:00,  6.76it/s]
Epoch 19/20, Loss: 1091.9049,

accs: 0.5060, 0.4754, 0.4362, 0.5142,  => 0.4829

precs (macro): 0.2693, 0.2775, 0.4321, 0.4211,  => 0.3500
recalls (macro): 0.2763, 0.3106, 0.3606, 0.2151,  => 0.2906
f1s (macro): 0.2461, 0.2501, 0.3683, 0.1796,  => 0.2610

precs (micro): 0.5060, 0.4754, 0.4362, 0.5142,  => 0.4829
recalls (micro): 0.5060, 0.4754, 0.4362, 0.5142,  => 0.4829
f1s (micro): 0.5060, 0.4754, 0.4362, 0.5142,  => 0.4829

precs (weighed): 0.6193, 0.5783, 0.5410, 0.7787,  => 0.6293
recalls (weighed): 0.5060, 0.4754, 0.4362, 0.5142,  => 0.4829
f1s (weighed): 0.5556, 0.5217, 0.4762, 0.6011,  => 0.5387

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.13it/s]
Evaluation, Loss: 129.8988,

accs: 0.7411, 0.6589, 0.6954, 0.8556,  => 0.7378

precs (macro): 0.5261, 0.6958, 0.5677, 0.2852,  => 0.5187
recalls (macro): 0.3679, 0.5718, 0.5077, 0.3333,  => 0.4452
f1s (macro): 0.3477, 0.5350, 0.5189, 0.3074,  => 0.4273

precs (micro): 0.7411, 0.6589, 0.6954, 0.8556,  => 0.7378
recalls (micro): 0.7411, 0.6589, 0.6954, 0.8556,  => 0.7378
f1s (micro): 0.7411, 0.6589, 0.6954, 0.8556,  => 0.7378

precs (weighed): 0.7590, 0.6855, 0.6887, 0.7321,  => 0.7163
recalls (weighed): 0.7411, 0.6589, 0.6954, 0.8556,  => 0.7378
f1s (weighed): 0.6600, 0.5890, 0.6796, 0.7890,  => 0.6794


Epoch 20/20, Batch 264/264: 100%|██████████| 264/264 [00:38<00:00,  6.78it/s]
Epoch 20/20, Loss: 1086.7503,

accs: 0.5050, 0.4737, 0.4468, 0.5156,  => 0.4853

precs (macro): 0.2715, 0.2776, 0.4425, 0.3553,  => 0.3367
recalls (macro): 0.2571, 0.2270, 0.3840, 0.2776,  => 0.2864
f1s (macro): 0.2458, 0.2498, 0.3861, 0.1793,  => 0.2652

precs (micro): 0.5050, 0.4737, 0.4468, 0.5156,  => 0.4853
recalls (micro): 0.5050, 0.4737, 0.4468, 0.5156,  => 0.4853
f1s (micro): 0.5050, 0.4737, 0.4468, 0.5156,  => 0.4853

precs (weighed): 0.6245, 0.5782, 0.5589, 0.7862,  => 0.6370
recalls (weighed): 0.5050, 0.4737, 0.4468, 0.5156,  => 0.4853
f1s (weighed): 0.5576, 0.5208, 0.4910, 0.6017,  => 0.5428

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:05<00:00,  7.55it/s]
Evaluation, Loss: 126.5763,

accs: 0.7527, 0.6606, 0.6880, 0.8556,  => 0.7392

precs (macro): 0.5378, 0.6503, 0.5595, 0.2852,  => 0.5082
recalls (macro): 0.3826, 0.5932, 0.4806, 0.3333,  => 0.4474
f1s (macro): 0.3725, 0.5822, 0.4963, 0.3074,  => 0.4396

precs (micro): 0.7527, 0.6606, 0.6880, 0.8556,  => 0.7392
recalls (micro): 0.7527, 0.6606, 0.6880, 0.8556,  => 0.7392
f1s (micro): 0.7527, 0.6606, 0.6880, 0.8556,  => 0.7392

precs (weighed): 0.7729, 0.6539, 0.6695, 0.7321,  => 0.7071
recalls (weighed): 0.7527, 0.6606, 0.6880, 0.8556,  => 0.7392
f1s (weighed): 0.6830, 0.6229, 0.6594, 0.7890,  => 0.6886

Saved the best model to path: ./models/task_2/simple_phobert-base_19.pth

uitnlp/visobert

[12:24:47] task: task-2                                                                                my_import.py:133
           model_type: simple                                                                          my_import.py:133
           model_name: uitnlp/visobert                                                                 my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/simple_visobert                                                my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/visobert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Training ...
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.36it/s]
Epoch 1/20, Loss: 1149.1489,

accs: 0.4964, 0.4742, 0.3933, 0.5118,  => 0.4689

precs (macro): 0.2530, 0.2693, 0.3039, 0.2625,  => 0.2722
recalls (macro): 0.2534, 0.2245, 0.2834, 0.2758,  => 0.2593
f1s (macro): 0.2335, 0.2448, 0.2564, 0.1770,  => 0.2279

precs (micro): 0.4964, 0.4742, 0.3933, 0.5118,  => 0.4689
recalls (micro): 0.4964, 0.4742, 0.3933, 0.5118,  => 0.4689
f1s (micro): 0.4964, 0.4742, 0.3933, 0.5118,  => 0.4689

precs (weighed): 0.5957, 0.5652, 0.4357, 0.7442,  => 0.5852
recalls (weighed): 0.4964, 0.4742, 0.3933, 0.5118,  => 0.4689
f1s (weighed): 0.5405, 0.5156, 0.3995, 0.5988,  => 0.5136

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.72it/s]
Evaluation, Loss: 137.8824,

accs: 0.7162, 0.6614, 0.6066, 0.8556,  => 0.7100

precs (macro): 0.2387, 0.6394, 0.2975, 0.2852,  => 0.3652
recalls (macro): 0.3333, 0.6139, 0.3622, 0.3333,  => 0.4107
f1s (macro): 0.2782, 0.6143, 0.3263, 0.3074,  => 0.3816

precs (micro): 0.7162, 0.6614, 0.6066, 0.8556,  => 0.7100
recalls (micro): 0.7162, 0.6614, 0.6066, 0.8556,  => 0.7100
f1s (micro): 0.7162, 0.6614, 0.6066, 0.8556,  => 0.7100

precs (weighed): 0.5129, 0.6499, 0.4919, 0.7321,  => 0.5967
recalls (weighed): 0.7162, 0.6614, 0.6066, 0.8556,  => 0.7100
f1s (weighed): 0.5977, 0.6446, 0.5427, 0.7890,  => 0.6435

Saved the best model to path: ./models/task_2/simple_visobert_0.pth

Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.39it/s]
Epoch 2/20, Loss: 1124.0084,

accs: 0.4976, 0.4707, 0.4131, 0.5143,  => 0.4739

precs (macro): 0.2536, 0.2700, 0.2683, 0.2130,  => 0.2512
recalls (macro): 0.2589, 0.2231, 0.2948, 0.2763,  => 0.2633
f1s (macro): 0.2324, 0.2442, 0.2663, 0.1772,  => 0.2300

precs (micro): 0.4976, 0.4707, 0.4131, 0.5143,  => 0.4739
recalls (micro): 0.4976, 0.4707, 0.4131, 0.5143,  => 0.4739
f1s (micro): 0.4976, 0.4707, 0.4131, 0.5143,  => 0.4739

precs (weighed): 0.5975, 0.5655, 0.4335, 0.7228,  => 0.5798
recalls (weighed): 0.4976, 0.4707, 0.4131, 0.5143,  => 0.4739
f1s (weighed): 0.5419, 0.5136, 0.4198, 0.6008,  => 0.5190

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.71it/s]
Evaluation, Loss: 135.0946,

accs: 0.7228, 0.6448, 0.6050, 0.8556,  => 0.7071

precs (macro): 0.4635, 0.6315, 0.3013, 0.2852,  => 0.4204
recalls (macro): 0.3464, 0.6357, 0.3712, 0.3333,  => 0.4217
f1s (macro): 0.3089, 0.6325, 0.3303, 0.3074,  => 0.3948

precs (micro): 0.7228, 0.6448, 0.6050, 0.8556,  => 0.7071
recalls (micro): 0.7228, 0.6448, 0.6050, 0.8556,  => 0.7071
f1s (micro): 0.7228, 0.6448, 0.6050, 0.8556,  => 0.7071

precs (weighed): 0.7016, 0.6526, 0.5068, 0.7321,  => 0.6483
recalls (weighed): 0.7228, 0.6448, 0.6050, 0.8556,  => 0.7071
f1s (weighed): 0.6239, 0.6476, 0.5479, 0.7890,  => 0.6521

Saved the best model to path: ./models/task_2/simple_visobert_1.pth

Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.37it/s]
Epoch 3/20, Loss: 1112.6912,

accs: 0.4999, 0.4806, 0.4159, 0.5086,  => 0.4763

precs (macro): 0.2560, 0.2781, 0.3698, 0.2121,  => 0.2790
recalls (macro): 0.2424, 0.2302, 0.3057, 0.2747,  => 0.2632
f1s (macro): 0.2354, 0.2519, 0.2910, 0.1757,  => 0.2385

precs (micro): 0.4999, 0.4806, 0.4159, 0.5086,  => 0.4763
recalls (micro): 0.4999, 0.4806, 0.4159, 0.5086,  => 0.4763
f1s (micro): 0.4999, 0.4806, 0.4159, 0.5086,  => 0.4763

precs (weighed): 0.6013, 0.5789, 0.4931, 0.7197,  => 0.5983
recalls (weighed): 0.4999, 0.4806, 0.4159, 0.5086,  => 0.4763
f1s (weighed): 0.5453, 0.5252, 0.4344, 0.5959,  => 0.5252

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.72it/s]
Evaluation, Loss: 127.2472,

accs: 0.7212, 0.6830, 0.6274, 0.8556,  => 0.7218

precs (macro): 0.4530, 0.6733, 0.4369, 0.2852,  => 0.4621
recalls (macro): 0.3443, 0.6280, 0.3812, 0.3333,  => 0.4217
f1s (macro): 0.3050, 0.6275, 0.3667, 0.3074,  => 0.4017

precs (micro): 0.7212, 0.6830, 0.6274, 0.8556,  => 0.7218
recalls (micro): 0.7212, 0.6830, 0.6274, 0.8556,  => 0.7218
f1s (micro): 0.7212, 0.6830, 0.6274, 0.8556,  => 0.7218

precs (weighed): 0.6924, 0.6773, 0.5818, 0.7321,  => 0.6709
recalls (weighed): 0.7212, 0.6830, 0.6274, 0.8556,  => 0.7218
f1s (weighed): 0.6204, 0.6598, 0.5717, 0.7890,  => 0.6602

Saved the best model to path: ./models/task_2/simple_visobert_2.pth

Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.42it/s]
Epoch 4/20, Loss: 1100.0010,

accs: 0.4903, 0.4874, 0.4426, 0.5182,  => 0.4846

precs (macro): 0.2546, 0.2808, 0.4016, 0.2130,  => 0.2875
recalls (macro): 0.2520, 0.3993, 0.3537, 0.3400,  => 0.3362
f1s (macro): 0.2330, 0.2550, 0.3500, 0.1781,  => 0.2540

precs (micro): 0.4903, 0.4874, 0.4426, 0.5182,  => 0.4846
recalls (micro): 0.4903, 0.4874, 0.4426, 0.5182,  => 0.4846
f1s (micro): 0.4903, 0.4874, 0.4426, 0.5182,  => 0.4846

precs (weighed): 0.5980, 0.5848, 0.5179, 0.7226,  => 0.6058
recalls (weighed): 0.4903, 0.4874, 0.4426, 0.5182,  => 0.4846
f1s (weighed): 0.5378, 0.5315, 0.4696, 0.6033,  => 0.5356

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.71it/s]
Evaluation, Loss: 131.8410,

accs: 0.7228, 0.6357, 0.6498, 0.8556,  => 0.7160

precs (macro): 0.5183, 0.7047, 0.5121, 0.2852,  => 0.5051
recalls (macro): 0.3426, 0.5343, 0.4286, 0.3333,  => 0.4097
f1s (macro): 0.2986, 0.4595, 0.4273, 0.3074,  => 0.3732

precs (micro): 0.7228, 0.6357, 0.6498, 0.8556,  => 0.7160
recalls (micro): 0.7228, 0.6357, 0.6498, 0.8556,  => 0.7160
f1s (micro): 0.7228, 0.6357, 0.6498, 0.8556,  => 0.7160

precs (weighed): 0.7458, 0.6878, 0.6378, 0.7321,  => 0.7009
recalls (weighed): 0.7228, 0.6357, 0.6498, 0.8556,  => 0.7160
f1s (weighed): 0.6159, 0.5289, 0.6175, 0.7890,  => 0.6378


Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.42it/s]
Epoch 5/20, Loss: 1097.3445,

accs: 0.5010, 0.4783, 0.4426, 0.5049,  => 0.4817

precs (macro): 0.2658, 0.2791, 0.4150, 0.2117,  => 0.2929
recalls (macro): 0.2561, 0.2284, 0.3612, 0.2111,  => 0.2642
f1s (macro): 0.2415, 0.2512, 0.3641, 0.1748,  => 0.2579

precs (micro): 0.5010, 0.4783, 0.4426, 0.5049,  => 0.4817
recalls (micro): 0.5010, 0.4783, 0.4426, 0.5049,  => 0.4817
f1s (micro): 0.5010, 0.4783, 0.4426, 0.5049,  => 0.4817

precs (weighed): 0.6133, 0.5815, 0.5244, 0.7188,  => 0.6095
recalls (weighed): 0.5010, 0.4783, 0.4426, 0.5049,  => 0.4817
f1s (weighed): 0.5506, 0.5248, 0.4739, 0.5931,  => 0.5356

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.71it/s]
Evaluation, Loss: 131.1223,

accs: 0.7178, 0.6921, 0.6506, 0.8556,  => 0.7290

precs (macro): 0.5725, 0.6797, 0.5199, 0.2852,  => 0.5143
recalls (macro): 0.3353, 0.6452, 0.4309, 0.3333,  => 0.4362
f1s (macro): 0.2825, 0.6481, 0.4345, 0.3074,  => 0.4181

precs (micro): 0.7178, 0.6921, 0.6506, 0.8556,  => 0.7290
recalls (micro): 0.7178, 0.6921, 0.6506, 0.8556,  => 0.7290
f1s (micro): 0.7178, 0.6921, 0.6506, 0.8556,  => 0.7290

precs (weighed): 0.7885, 0.6855, 0.6367, 0.7321,  => 0.7107
recalls (weighed): 0.7178, 0.6921, 0.6506, 0.8556,  => 0.7290
f1s (weighed): 0.6016, 0.6761, 0.6183, 0.7890,  => 0.6713


Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.41it/s]
Epoch 6/20, Loss: 1088.6865,

accs: 0.5144, 0.4779, 0.4464, 0.5157,  => 0.4886

precs (macro): 0.2664, 0.2795, 0.4164, 0.2124,  => 0.2937
recalls (macro): 0.2820, 0.3128, 0.3617, 0.2768,  => 0.3083
f1s (macro): 0.2457, 0.2523, 0.3638, 0.1773,  => 0.2598

precs (micro): 0.5144, 0.4779, 0.4464, 0.5157,  => 0.4886
recalls (micro): 0.5144, 0.4779, 0.4464, 0.5157,  => 0.4886
f1s (micro): 0.5144, 0.4779, 0.4464, 0.5157,  => 0.4886

precs (weighed): 0.6165, 0.5830, 0.5295, 0.7210,  => 0.6125
recalls (weighed): 0.5144, 0.4779, 0.4464, 0.5157,  => 0.4886
f1s (weighed): 0.5595, 0.5252, 0.4777, 0.6011,  => 0.5409

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.72it/s]
Evaluation, Loss: 125.8873,

accs: 0.7295, 0.6880, 0.6598, 0.8556,  => 0.7332

precs (macro): 0.4984, 0.6693, 0.5276, 0.2852,  => 0.4951
recalls (macro): 0.3538, 0.6580, 0.4432, 0.3333,  => 0.4471
f1s (macro): 0.3225, 0.6611, 0.4537, 0.3074,  => 0.4362

precs (micro): 0.7295, 0.6880, 0.6598, 0.8556,  => 0.7332
recalls (micro): 0.7295, 0.6880, 0.6598, 0.8556,  => 0.7332
f1s (micro): 0.7295, 0.6880, 0.6598, 0.8556,  => 0.7332

precs (weighed): 0.7323, 0.6817, 0.6411, 0.7321,  => 0.6968
recalls (weighed): 0.7295, 0.6880, 0.6598, 0.8556,  => 0.7332
f1s (weighed): 0.6366, 0.6826, 0.6277, 0.7890,  => 0.6840

Saved the best model to path: ./models/task_2/simple_visobert_5.pth

Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.37it/s]
Epoch 7/20, Loss: 1091.4022,

accs: 0.4947, 0.4781, 0.4537, 0.5149,  => 0.4854

precs (macro): 0.2646, 0.2821, 0.4284, 0.2117,  => 0.2967
recalls (macro): 0.2575, 0.2298, 0.3688, 0.3390,  => 0.2988
f1s (macro): 0.2402, 0.2533, 0.3723, 0.1770,  => 0.2607

precs (micro): 0.4947, 0.4781, 0.4537, 0.5149,  => 0.4854
recalls (micro): 0.4947, 0.4781, 0.4537, 0.5149,  => 0.4854
f1s (micro): 0.4947, 0.4781, 0.4537, 0.5149,  => 0.4854

precs (weighed): 0.6136, 0.5879, 0.5446, 0.7182,  => 0.6161
recalls (weighed): 0.4947, 0.4781, 0.4537, 0.5149,  => 0.4854
f1s (weighed): 0.5467, 0.5274, 0.4884, 0.5995,  => 0.5405

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.66it/s]
Evaluation, Loss: 129.0672,

accs: 0.7361, 0.6763, 0.6349, 0.8556,  => 0.7257

precs (macro): 0.4890, 0.6824, 0.5201, 0.2852,  => 0.4942
recalls (macro): 0.3662, 0.6068, 0.4547, 0.3333,  => 0.4403
f1s (macro): 0.3469, 0.5961, 0.4549, 0.3074,  => 0.4263

precs (micro): 0.7361, 0.6763, 0.6349, 0.8556,  => 0.7257
recalls (micro): 0.7361, 0.6763, 0.6349, 0.8556,  => 0.7257
f1s (micro): 0.7361, 0.6763, 0.6349, 0.8556,  => 0.7257

precs (weighed): 0.7281, 0.6804, 0.6554, 0.7321,  => 0.6990
recalls (weighed): 0.7361, 0.6763, 0.6349, 0.8556,  => 0.7257
f1s (weighed): 0.6577, 0.6366, 0.6181, 0.7890,  => 0.6754


Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.37it/s]
Epoch 8/20, Loss: 1090.7359,

accs: 0.4967, 0.4691, 0.4447, 0.5100,  => 0.4801

precs (macro): 0.2656, 0.2776, 0.4243, 0.4637,  => 0.3578
recalls (macro): 0.2371, 0.2249, 0.3603, 0.2129,  => 0.2588
f1s (macro): 0.2388, 0.2485, 0.3691, 0.1770,  => 0.2584

precs (micro): 0.4967, 0.4691, 0.4447, 0.5100,  => 0.4801
recalls (micro): 0.4967, 0.4691, 0.4447, 0.5100,  => 0.4801
f1s (micro): 0.4967, 0.4691, 0.4447, 0.5100,  => 0.4801

precs (weighed): 0.6157, 0.5788, 0.5369, 0.8394,  => 0.6427
recalls (weighed): 0.4967, 0.4691, 0.4447, 0.5100,  => 0.4801
f1s (weighed): 0.5493, 0.5182, 0.4808, 0.5990,  => 0.5368

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.66it/s]
Evaluation, Loss: 122.6390,

accs: 0.7361, 0.6905, 0.6689, 0.8556,  => 0.7378

precs (macro): 0.4952, 0.6722, 0.5245, 0.2852,  => 0.4942
recalls (macro): 0.3650, 0.6643, 0.4600, 0.3333,  => 0.4557
f1s (macro): 0.3442, 0.6670, 0.4664, 0.3074,  => 0.4462

precs (micro): 0.7361, 0.6905, 0.6689, 0.8556,  => 0.7378
recalls (micro): 0.7361, 0.6905, 0.6689, 0.8556,  => 0.7378
f1s (micro): 0.7361, 0.6905, 0.6689, 0.8556,  => 0.7378

precs (weighed): 0.7328, 0.6856, 0.6523, 0.7321,  => 0.7007
recalls (weighed): 0.7361, 0.6905, 0.6689, 0.8556,  => 0.7378
f1s (weighed): 0.6556, 0.6869, 0.6443, 0.7890,  => 0.6940

Saved the best model to path: ./models/task_2/simple_visobert_7.pth

Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.36it/s]
Epoch 9/20, Loss: 1087.1572,

accs: 0.4961, 0.4862, 0.4487, 0.5076,  => 0.4847

precs (macro): 0.2641, 0.2860, 0.4332, 0.2123,  => 0.2989
recalls (macro): 0.2700, 0.2334, 0.3750, 0.2744,  => 0.2882
f1s (macro): 0.2400, 0.2570, 0.3796, 0.1756,  => 0.2630

precs (micro): 0.4961, 0.4862, 0.4487, 0.5076,  => 0.4847
recalls (micro): 0.4961, 0.4862, 0.4487, 0.5076,  => 0.4847
f1s (micro): 0.4961, 0.4862, 0.4487, 0.5076,  => 0.4847

precs (weighed): 0.6122, 0.5949, 0.5479, 0.7205,  => 0.6188
recalls (weighed): 0.4961, 0.4862, 0.4487, 0.5076,  => 0.4847
f1s (weighed): 0.5468, 0.5351, 0.4878, 0.5955,  => 0.5413

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.70it/s]
Evaluation, Loss: 130.5643,

accs: 0.7344, 0.6689, 0.6680, 0.8556,  => 0.7317

precs (macro): 0.5326, 0.6730, 0.5045, 0.2852,  => 0.4988
recalls (macro): 0.3580, 0.5968, 0.4732, 0.3333,  => 0.4403
f1s (macro): 0.3290, 0.5825, 0.4767, 0.3074,  => 0.4239

precs (micro): 0.7344, 0.6689, 0.6680, 0.8556,  => 0.7317
recalls (micro): 0.7344, 0.6689, 0.6680, 0.8556,  => 0.7317
f1s (micro): 0.7344, 0.6689, 0.6680, 0.8556,  => 0.7317

precs (weighed): 0.7616, 0.6716, 0.6458, 0.7321,  => 0.7028
recalls (weighed): 0.7344, 0.6689, 0.6680, 0.8556,  => 0.7317
f1s (weighed): 0.6433, 0.6252, 0.6486, 0.7890,  => 0.6765


Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.37it/s]
Epoch 10/20, Loss: 1078.6281,

accs: 0.4998, 0.4903, 0.4531, 0.5101,  => 0.4884

precs (macro): 0.2694, 0.2870, 0.4342, 0.2130,  => 0.3009
recalls (macro): 0.2641, 0.2356, 0.3701, 0.3376,  => 0.3019
f1s (macro): 0.2430, 0.2588, 0.3785, 0.1765,  => 0.2642

precs (micro): 0.4998, 0.4903, 0.4531, 0.5101,  => 0.4884
recalls (micro): 0.4998, 0.4903, 0.4531, 0.5101,  => 0.4884
f1s (micro): 0.4998, 0.4903, 0.4531, 0.5101,  => 0.4884

precs (weighed): 0.6185, 0.5958, 0.5446, 0.7226,  => 0.6204
recalls (weighed): 0.4998, 0.4903, 0.4531, 0.5101,  => 0.4884
f1s (weighed): 0.5518, 0.5379, 0.4889, 0.5978,  => 0.5441

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.71it/s]
Evaluation, Loss: 130.1390,

accs: 0.7402, 0.6772, 0.6598, 0.8556,  => 0.7332

precs (macro): 0.5359, 0.6663, 0.5255, 0.2141,  => 0.4854
recalls (macro): 0.3656, 0.6728, 0.4472, 0.2500,  => 0.4339
f1s (macro): 0.3432, 0.6675, 0.4553, 0.2306,  => 0.4241

precs (micro): 0.7402, 0.6772, 0.6598, 0.8556,  => 0.7332
recalls (micro): 0.7402, 0.6772, 0.6598, 0.8556,  => 0.7332
f1s (micro): 0.7402, 0.6772, 0.6598, 0.8556,  => 0.7332

precs (weighed): 0.7665, 0.6872, 0.6428, 0.7327,  => 0.7073
recalls (weighed): 0.7402, 0.6772, 0.6598, 0.8556,  => 0.7332
f1s (weighed): 0.6562, 0.6802, 0.6315, 0.7894,  => 0.6893


Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.37it/s]
Epoch 11/20, Loss: 1079.8093,

accs: 0.5002, 0.4817, 0.4531, 0.5084,  => 0.4858

precs (macro): 0.2659, 0.2884, 0.4486, 0.3634,  => 0.3416
recalls (macro): 0.2500, 0.2317, 0.3837, 0.2128,  => 0.2696
f1s (macro): 0.2408, 0.2570, 0.3852, 0.1775,  => 0.2651

precs (micro): 0.5002, 0.4817, 0.4531, 0.5084,  => 0.4858
recalls (micro): 0.5002, 0.4817, 0.4531, 0.5084,  => 0.4858
f1s (micro): 0.5002, 0.4817, 0.4531, 0.5084,  => 0.4858

precs (weighed): 0.6139, 0.5990, 0.5609, 0.7930,  => 0.6417
recalls (weighed): 0.5002, 0.4817, 0.4531, 0.5084,  => 0.4858
f1s (weighed): 0.5505, 0.5340, 0.4941, 0.5979,  => 0.5441

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.70it/s]
Evaluation, Loss: 125.1767,

accs: 0.7378, 0.6938, 0.6656, 0.8556,  => 0.7382

precs (macro): 0.4685, 0.6983, 0.5293, 0.2852,  => 0.4953
recalls (macro): 0.3763, 0.6324, 0.4539, 0.3333,  => 0.4490
f1s (macro): 0.3663, 0.6301, 0.4651, 0.3074,  => 0.4422

precs (micro): 0.7378, 0.6938, 0.6656, 0.8556,  => 0.7382
recalls (micro): 0.7378, 0.6938, 0.6656, 0.8556,  => 0.7382
f1s (micro): 0.7378, 0.6938, 0.6656, 0.8556,  => 0.7382

precs (weighed): 0.7142, 0.6966, 0.6450, 0.7321,  => 0.6970
recalls (weighed): 0.7378, 0.6938, 0.6656, 0.8556,  => 0.7382
f1s (weighed): 0.6732, 0.6646, 0.6366, 0.7890,  => 0.6908


Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.36it/s]
Epoch 12/20, Loss: 1079.3933,

accs: 0.5002, 0.4873, 0.4436, 0.5072,  => 0.4846

precs (macro): 0.2696, 0.2911, 0.4319, 0.2962,  => 0.3222
recalls (macro): 0.2784, 0.3171, 0.3662, 0.2747,  => 0.3091
f1s (macro): 0.2457, 0.2595, 0.3716, 0.1767,  => 0.2634

precs (micro): 0.5002, 0.4873, 0.4436, 0.5072,  => 0.4846
recalls (micro): 0.5002, 0.4873, 0.4436, 0.5072,  => 0.4846
f1s (micro): 0.5002, 0.4873, 0.4436, 0.5072,  => 0.4846

precs (weighed): 0.6217, 0.6028, 0.5443, 0.7604,  => 0.6323
recalls (weighed): 0.5002, 0.4873, 0.4436, 0.5072,  => 0.4846
f1s (weighed): 0.5528, 0.5388, 0.4823, 0.5961,  => 0.5425

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.70it/s]
Evaluation, Loss: 124.9518,

accs: 0.7461, 0.6813, 0.6763, 0.8556,  => 0.7398

precs (macro): 0.5057, 0.6681, 0.4970, 0.2852,  => 0.4890
recalls (macro): 0.3789, 0.6298, 0.4961, 0.3333,  => 0.4595
f1s (macro): 0.3680, 0.6305, 0.4927, 0.3074,  => 0.4497

precs (micro): 0.7461, 0.6813, 0.6763, 0.8556,  => 0.7398
recalls (micro): 0.7461, 0.6813, 0.6763, 0.8556,  => 0.7398
f1s (micro): 0.7461, 0.6813, 0.6763, 0.8556,  => 0.7398

precs (weighed): 0.7455, 0.6739, 0.6526, 0.7321,  => 0.7010
recalls (weighed): 0.7461, 0.6813, 0.6763, 0.8556,  => 0.7398
f1s (weighed): 0.6773, 0.6613, 0.6610, 0.7890,  => 0.6971


Epoch 13/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.36it/s]
Epoch 13/20, Loss: 1075.7811,

accs: 0.4985, 0.4835, 0.4608, 0.5036,  => 0.4866

precs (macro): 0.2736, 0.2901, 0.4486, 0.2131,  => 0.3063
recalls (macro): 0.2760, 0.3985, 0.3786, 0.2732,  => 0.3316
f1s (macro): 0.2456, 0.2582, 0.3874, 0.1751,  => 0.2666

precs (micro): 0.4985, 0.4835, 0.4608, 0.5036,  => 0.4866
recalls (micro): 0.4985, 0.4835, 0.4608, 0.5036,  => 0.4866
f1s (micro): 0.4985, 0.4835, 0.4608, 0.5036,  => 0.4866

precs (weighed): 0.6257, 0.6015, 0.5625, 0.7233,  => 0.6283
recalls (weighed): 0.4985, 0.4835, 0.4608, 0.5036,  => 0.4866
f1s (weighed): 0.5536, 0.5359, 0.5004, 0.5936,  => 0.5459

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.69it/s]
Evaluation, Loss: 125.0772,

accs: 0.7477, 0.6938, 0.6730, 0.8556,  => 0.7425

precs (macro): 0.5035, 0.6763, 0.5054, 0.2144,  => 0.4749
recalls (macro): 0.3822, 0.6604, 0.4635, 0.2500,  => 0.4390
f1s (macro): 0.3735, 0.6641, 0.4703, 0.2309,  => 0.4347

precs (micro): 0.7477, 0.6938, 0.6730, 0.8556,  => 0.7425
recalls (micro): 0.7477, 0.6938, 0.6730, 0.8556,  => 0.7425
f1s (micro): 0.7477, 0.6938, 0.6730, 0.8556,  => 0.7425

precs (weighed): 0.7446, 0.6869, 0.6400, 0.7339,  => 0.7013
recalls (weighed): 0.7477, 0.6938, 0.6730, 0.8556,  => 0.7425
f1s (weighed): 0.6821, 0.6866, 0.6452, 0.7901,  => 0.7010

Early stopping triggered
uitnlp/CafeBERT

[12:44:16] task: task-2                                                                                my_import.py:133
           model_type: simple                                                                          my_import.py:133
           model_name: uitnlp/CafeBERT                                                                 my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/simple_CafeBERT                                                my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/CafeBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Training ...
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [03:54<00:00,  1.13it/s]
Epoch 1/20, Loss: 1184.0196,

accs: 0.4649, 0.4568, 0.3121, 0.5071,  => 0.4352

precs (macro): 0.2538, 0.2533, 0.2418, 0.2125,  => 0.2403
recalls (macro): 0.2447, 0.2111, 0.2543, 0.2742,  => 0.2461
f1s (macro): 0.2095, 0.2295, 0.1658, 0.1756,  => 0.1951

precs (micro): 0.4649, 0.4568, 0.3121, 0.5071,  => 0.4352
recalls (micro): 0.4649, 0.4568, 0.3121, 0.5071,  => 0.4352
f1s (micro): 0.4649, 0.4568, 0.3121, 0.5071,  => 0.4352

precs (weighed): 0.5983, 0.5345, 0.3583, 0.7214,  => 0.5531
recalls (weighed): 0.4649, 0.4568, 0.3121, 0.5071,  => 0.4352
f1s (weighed): 0.5168, 0.4913, 0.2751, 0.5954,  => 0.4697

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 147.6388,

accs: 0.7162, 0.6100, 0.4855, 0.8556,  => 0.6668

precs (macro): 0.2387, 0.5268, 0.1214, 0.2852,  => 0.2930
recalls (macro): 0.3333, 0.5023, 0.2500, 0.3333,  => 0.3547
f1s (macro): 0.2782, 0.3998, 0.1634, 0.3074,  => 0.2872

precs (micro): 0.7162, 0.6100, 0.4855, 0.8556,  => 0.6668
recalls (micro): 0.7162, 0.6100, 0.4855, 0.8556,  => 0.6668
f1s (micro): 0.7162, 0.6100, 0.4855, 0.8556,  => 0.6668

precs (weighed): 0.5129, 0.5463, 0.2357, 0.7321,  => 0.5067
recalls (weighed): 0.7162, 0.6100, 0.4855, 0.8556,  => 0.6668
f1s (weighed): 0.5977, 0.4797, 0.3173, 0.7890,  => 0.5459

Saved the best model to path: ./models/task_2/simple_CafeBERT_0.pth

Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [03:52<00:00,  1.14it/s]
Epoch 2/20, Loss: 1166.7606,

accs: 0.4754, 0.4769, 0.3248, 0.5098,  => 0.4467

precs (macro): 0.2528, 0.2638, 0.2689, 0.2122,  => 0.2494
recalls (macro): 0.2494, 0.3041, 0.2489, 0.2126,  => 0.2537
f1s (macro): 0.2213, 0.2397, 0.1788, 0.1759,  => 0.2039

precs (micro): 0.4754, 0.4769, 0.3248, 0.5098,  => 0.4467
recalls (micro): 0.4754, 0.4769, 0.3248, 0.5098,  => 0.4467
f1s (micro): 0.4754, 0.4769, 0.3248, 0.5098,  => 0.4467

precs (weighed): 0.5949, 0.5535, 0.3797, 0.7204,  => 0.5621
recalls (weighed): 0.4754, 0.4769, 0.3248, 0.5098,  => 0.4467
f1s (weighed): 0.5263, 0.5105, 0.2970, 0.5970,  => 0.4827

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 144.1989,

accs: 0.7162, 0.6116, 0.4863, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.5063, 0.2466, 0.2852,  => 0.3192
recalls (macro): 0.3333, 0.5001, 0.2506, 0.3333,  => 0.3544
f1s (macro): 0.2782, 0.3835, 0.1648, 0.3074,  => 0.2835

precs (micro): 0.7162, 0.6116, 0.4863, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6116, 0.4863, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6116, 0.4863, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.5301, 0.4008, 0.7321,  => 0.5440
recalls (weighed): 0.7162, 0.6116, 0.4863, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4678, 0.3193, 0.7890,  => 0.5435

Saved the best model to path: ./models/task_2/simple_CafeBERT_1.pth

Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [03:52<00:00,  1.14it/s]
Epoch 3/20, Loss: 1151.4703,

accs: 0.4970, 0.4577, 0.3920, 0.5133,  => 0.4650

precs (macro): 0.2538, 0.2558, 0.2333, 0.2119,  => 0.2387
recalls (macro): 0.2592, 0.2968, 0.2714, 0.2136,  => 0.2603
f1s (macro): 0.2343, 0.2326, 0.2425, 0.1765,  => 0.2215

precs (micro): 0.4970, 0.4577, 0.3920, 0.5133,  => 0.4650
recalls (micro): 0.4970, 0.4577, 0.3920, 0.5133,  => 0.4650
f1s (micro): 0.4970, 0.4577, 0.3920, 0.5133,  => 0.4650

precs (weighed): 0.5961, 0.5378, 0.3754, 0.7194,  => 0.5572
recalls (weighed): 0.4970, 0.4577, 0.3920, 0.5133,  => 0.4650
f1s (weighed): 0.5410, 0.4937, 0.3814, 0.5991,  => 0.5038

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 144.8320,

accs: 0.7162, 0.6141, 0.4888, 0.8556,  => 0.6687

precs (macro): 0.2387, 0.5669, 0.1784, 0.2852,  => 0.3173
recalls (macro): 0.3333, 0.5238, 0.2533, 0.3333,  => 0.3609
f1s (macro): 0.2782, 0.4715, 0.1749, 0.3074,  => 0.3080

precs (micro): 0.7162, 0.6141, 0.4888, 0.8556,  => 0.6687
recalls (micro): 0.7162, 0.6141, 0.4888, 0.8556,  => 0.6687
f1s (micro): 0.7162, 0.6141, 0.4888, 0.8556,  => 0.6687

precs (weighed): 0.5129, 0.5799, 0.3127, 0.7321,  => 0.5344
recalls (weighed): 0.7162, 0.6141, 0.4888, 0.8556,  => 0.6687
f1s (weighed): 0.5977, 0.5332, 0.3340, 0.7890,  => 0.5635


Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [03:51<00:00,  1.14it/s]
Epoch 4/20, Loss: 1149.9655,

accs: 0.4993, 0.4703, 0.3818, 0.4999,  => 0.4628

precs (macro): 0.2562, 0.2648, 0.2291, 0.2117,  => 0.2405
recalls (macro): 0.2460, 0.3055, 0.2588, 0.2721,  => 0.2706
f1s (macro): 0.2347, 0.2418, 0.2367, 0.1738,  => 0.2218

precs (micro): 0.4993, 0.4703, 0.3818, 0.4999,  => 0.4628
recalls (micro): 0.4993, 0.4703, 0.3818, 0.4999,  => 0.4628
f1s (micro): 0.4993, 0.4703, 0.3818, 0.4999,  => 0.4628

precs (weighed): 0.5999, 0.5549, 0.3734, 0.7184,  => 0.5616
recalls (weighed): 0.4993, 0.4703, 0.3818, 0.4999,  => 0.4628
f1s (weighed): 0.5443, 0.5089, 0.3761, 0.5894,  => 0.5047

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 142.0937,

accs: 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683

precs (macro): 0.2387, 0.5731, 0.1214, 0.2852,  => 0.3046
recalls (macro): 0.3333, 0.5204, 0.2500, 0.3333,  => 0.3593
f1s (macro): 0.2782, 0.4567, 0.1634, 0.3074,  => 0.3014

precs (micro): 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683
recalls (micro): 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683
f1s (micro): 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683

precs (weighed): 0.5129, 0.5843, 0.2357, 0.7321,  => 0.5162
recalls (weighed): 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683
f1s (weighed): 0.5977, 0.5228, 0.3173, 0.7890,  => 0.5567

Saved the best model to path: ./models/task_2/simple_CafeBERT_3.pth

Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [03:51<00:00,  1.14it/s]
Epoch 5/20, Loss: 1144.2418,

accs: 0.4935, 0.4688, 0.3805, 0.5133,  => 0.4640

precs (macro): 0.2537, 0.2659, 0.2305, 0.2119,  => 0.2405
recalls (macro): 0.2504, 0.3042, 0.2618, 0.2136,  => 0.2575
f1s (macro): 0.2318, 0.2414, 0.2379, 0.1765,  => 0.2219

precs (micro): 0.4935, 0.4688, 0.3805, 0.5133,  => 0.4640
recalls (micro): 0.4935, 0.4688, 0.3805, 0.5133,  => 0.4640
f1s (micro): 0.4935, 0.4688, 0.3805, 0.5133,  => 0.4640

precs (weighed): 0.5958, 0.5577, 0.3740, 0.7194,  => 0.5617
recalls (weighed): 0.4935, 0.4688, 0.3805, 0.5133,  => 0.4640
f1s (weighed): 0.5390, 0.5091, 0.3755, 0.5991,  => 0.5057

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 140.9281,

accs: 0.7162, 0.6191, 0.5469, 0.8556,  => 0.6844

precs (macro): 0.2387, 0.5900, 0.2567, 0.2852,  => 0.3427
recalls (macro): 0.3333, 0.5208, 0.3081, 0.3333,  => 0.3739
f1s (macro): 0.2782, 0.4501, 0.2713, 0.3074,  => 0.3267

precs (micro): 0.7162, 0.6191, 0.5469, 0.8556,  => 0.6844
recalls (micro): 0.7162, 0.6191, 0.5469, 0.8556,  => 0.6844
f1s (micro): 0.7162, 0.6191, 0.5469, 0.8556,  => 0.6844

precs (weighed): 0.5129, 0.5974, 0.4286, 0.7321,  => 0.5677
recalls (weighed): 0.7162, 0.6191, 0.5469, 0.8556,  => 0.6844
f1s (weighed): 0.5977, 0.5187, 0.4675, 0.7890,  => 0.5932

Saved the best model to path: ./models/task_2/simple_CafeBERT_4.pth

Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [03:51<00:00,  1.14it/s]
Epoch 6/20, Loss: 1139.7807,

accs: 0.5007, 0.4719, 0.3853, 0.5059,  => 0.4659

precs (macro): 0.2536, 0.2641, 0.2342, 0.2114,  => 0.2408
recalls (macro): 0.2617, 0.3046, 0.2614, 0.2739,  => 0.2754
f1s (macro): 0.2325, 0.2409, 0.2405, 0.1750,  => 0.2222

precs (micro): 0.5007, 0.4719, 0.3853, 0.5059,  => 0.4659
recalls (micro): 0.5007, 0.4719, 0.3853, 0.5059,  => 0.4659
f1s (micro): 0.5007, 0.4719, 0.3853, 0.5059,  => 0.4659

precs (weighed): 0.5944, 0.5559, 0.3836, 0.7176,  => 0.5629
recalls (weighed): 0.5007, 0.4719, 0.3853, 0.5059,  => 0.4659
f1s (weighed): 0.5422, 0.5101, 0.3820, 0.5932,  => 0.5069

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.23it/s]
Evaluation, Loss: 137.7722,

accs: 0.7162, 0.6199, 0.5427, 0.8556,  => 0.6836

precs (macro): 0.2387, 0.5927, 0.2532, 0.2852,  => 0.3425
recalls (macro): 0.3333, 0.5222, 0.3074, 0.3333,  => 0.3741
f1s (macro): 0.2782, 0.4535, 0.2717, 0.3074,  => 0.3277

precs (micro): 0.7162, 0.6199, 0.5427, 0.8556,  => 0.6836
recalls (micro): 0.7162, 0.6199, 0.5427, 0.8556,  => 0.6836
f1s (micro): 0.7162, 0.6199, 0.5427, 0.8556,  => 0.6836

precs (weighed): 0.5129, 0.5996, 0.4249, 0.7321,  => 0.5674
recalls (weighed): 0.7162, 0.6199, 0.5427, 0.8556,  => 0.6836
f1s (weighed): 0.5977, 0.5213, 0.4678, 0.7890,  => 0.5940

Saved the best model to path: ./models/task_2/simple_CafeBERT_5.pth

Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.14it/s]
Epoch 7/20, Loss: 1136.2708,

accs: 0.4943, 0.4730, 0.3926, 0.5117,  => 0.4679

precs (macro): 0.2564, 0.2645, 0.2398, 0.2125,  => 0.2433
recalls (macro): 0.2660, 0.2230, 0.2714, 0.2756,  => 0.2590
f1s (macro): 0.2348, 0.2418, 0.2467, 0.1765,  => 0.2250

precs (micro): 0.4943, 0.4730, 0.3926, 0.5117,  => 0.4679
recalls (micro): 0.4943, 0.4730, 0.3926, 0.5117,  => 0.4679
f1s (micro): 0.4943, 0.4730, 0.3926, 0.5117,  => 0.4679

precs (weighed): 0.5972, 0.5556, 0.3900, 0.7214,  => 0.5661
recalls (weighed): 0.4943, 0.4730, 0.3926, 0.5117,  => 0.4679
f1s (weighed): 0.5396, 0.5108, 0.3891, 0.5985,  => 0.5095

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 136.2413,

accs: 0.7162, 0.6207, 0.5535, 0.8556,  => 0.6865

precs (macro): 0.2387, 0.5866, 0.2602, 0.2852,  => 0.3427
recalls (macro): 0.3333, 0.5308, 0.3142, 0.3333,  => 0.3779
f1s (macro): 0.2782, 0.4806, 0.2783, 0.3074,  => 0.3361

precs (micro): 0.7162, 0.6207, 0.5535, 0.8556,  => 0.6865
recalls (micro): 0.7162, 0.6207, 0.5535, 0.8556,  => 0.6865
f1s (micro): 0.7162, 0.6207, 0.5535, 0.8556,  => 0.6865

precs (weighed): 0.5129, 0.5960, 0.4352, 0.7321,  => 0.5690
recalls (weighed): 0.7162, 0.6207, 0.5535, 0.8556,  => 0.6865
f1s (weighed): 0.5977, 0.5412, 0.4778, 0.7890,  => 0.6014

Saved the best model to path: ./models/task_2/simple_CafeBERT_6.pth

Epoch 8/20, Batch 264/264: 100%|██████████| 264/264 [03:50<00:00,  1.14it/s]
Epoch 8/20, Loss: 1128.1334,

accs: 0.4899, 0.4745, 0.4071, 0.5103,  => 0.4704

precs (macro): 0.2531, 0.2632, 0.2487, 0.2122,  => 0.2443
recalls (macro): 0.2578, 0.3889, 0.2842, 0.2127,  => 0.2859
f1s (macro): 0.2296, 0.2414, 0.2572, 0.1760,  => 0.2260

precs (micro): 0.4899, 0.4745, 0.4071, 0.5103,  => 0.4704
recalls (micro): 0.4899, 0.4745, 0.4071, 0.5103,  => 0.4704
f1s (micro): 0.4899, 0.4745, 0.4071, 0.5103,  => 0.4704

precs (weighed): 0.5931, 0.5526, 0.4022, 0.7204,  => 0.5671
recalls (weighed): 0.4899, 0.4745, 0.4071, 0.5103,  => 0.4704
f1s (weighed): 0.5353, 0.5100, 0.4020, 0.5973,  => 0.5111

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 137.8852,

accs: 0.7162, 0.6166, 0.5826, 0.8556,  => 0.6927

precs (macro): 0.2387, 0.6178, 0.2821, 0.2852,  => 0.3559
recalls (macro): 0.3333, 0.5085, 0.3461, 0.3333,  => 0.3803
f1s (macro): 0.2782, 0.4065, 0.3108, 0.3074,  => 0.3257

precs (micro): 0.7162, 0.6166, 0.5826, 0.8556,  => 0.6927
recalls (micro): 0.7162, 0.6166, 0.5826, 0.8556,  => 0.6927
f1s (micro): 0.7162, 0.6166, 0.5826, 0.8556,  => 0.6927

precs (weighed): 0.5129, 0.6175, 0.4760, 0.7321,  => 0.5846
recalls (weighed): 0.7162, 0.6166, 0.5826, 0.8556,  => 0.6927
f1s (weighed): 0.5977, 0.4859, 0.5239, 0.7890,  => 0.5991


Epoch 9/20, Batch 264/264: 100%|██████████| 264/264 [03:51<00:00,  1.14it/s]
Epoch 9/20, Loss: 1132.7414,

accs: 0.4900, 0.4732, 0.3982, 0.5123,  => 0.4684

precs (macro): 0.2532, 0.2665, 0.2445, 0.2136,  => 0.2444
recalls (macro): 0.2464, 0.3061, 0.2726, 0.1508,  => 0.2440
f1s (macro): 0.2288, 0.2427, 0.2501, 0.1768,  => 0.2246

precs (micro): 0.4900, 0.4732, 0.3982, 0.5123,  => 0.4684
recalls (micro): 0.4900, 0.4732, 0.3982, 0.5123,  => 0.4684
f1s (micro): 0.4900, 0.4732, 0.3982, 0.5123,  => 0.4684

precs (weighed): 0.5934, 0.5580, 0.4003, 0.7254,  => 0.5693
recalls (weighed): 0.4900, 0.4732, 0.3982, 0.5123,  => 0.4684
f1s (weighed): 0.5357, 0.5116, 0.3961, 0.6005,  => 0.5110

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.23it/s]
Evaluation, Loss: 135.4086,

accs: 0.7162, 0.6257, 0.5361, 0.8556,  => 0.6834

precs (macro): 0.2387, 0.6453, 0.2566, 0.2852,  => 0.3565
recalls (macro): 0.3333, 0.5234, 0.2921, 0.3333,  => 0.3705
f1s (macro): 0.2782, 0.4431, 0.2448, 0.3074,  => 0.3184

precs (micro): 0.7162, 0.6257, 0.5361, 0.8556,  => 0.6834
recalls (micro): 0.7162, 0.6257, 0.5361, 0.8556,  => 0.6834
f1s (micro): 0.7162, 0.6257, 0.5361, 0.8556,  => 0.6834

precs (weighed): 0.5129, 0.6406, 0.4231, 0.7321,  => 0.5772
recalls (weighed): 0.7162, 0.6257, 0.5361, 0.8556,  => 0.6834
f1s (weighed): 0.5977, 0.5149, 0.4314, 0.7890,  => 0.5832

Saved the best model to path: ./models/task_2/simple_CafeBERT_8.pth

Epoch 10/20, Batch 264/264: 100%|██████████| 264/264 [03:52<00:00,  1.14it/s]
Epoch 10/20, Loss: 1129.3570,

accs: 0.4986, 0.4703, 0.4004, 0.5125,  => 0.4705

precs (macro): 0.2586, 0.2602, 0.2460, 0.2121,  => 0.2442
recalls (macro): 0.2576, 0.2189, 0.2780, 0.4008,  => 0.2888
f1s (macro): 0.2349, 0.2372, 0.2527, 0.1767,  => 0.2254

precs (micro): 0.4986, 0.4703, 0.4004, 0.5125,  => 0.4705
recalls (micro): 0.4986, 0.4703, 0.4004, 0.5125,  => 0.4705
f1s (micro): 0.4986, 0.4703, 0.4004, 0.5125,  => 0.4705

precs (weighed): 0.6005, 0.5469, 0.4004, 0.7194,  => 0.5668
recalls (weighed): 0.4986, 0.4703, 0.4004, 0.5125,  => 0.4705
f1s (weighed): 0.5437, 0.5047, 0.3972, 0.5983,  => 0.5110

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 136.0453,

accs: 0.7162, 0.6174, 0.5718, 0.8556,  => 0.6902

precs (macro): 0.2387, 0.6164, 0.2731, 0.2852,  => 0.3534
recalls (macro): 0.3333, 0.5104, 0.3246, 0.3333,  => 0.3754
f1s (macro): 0.2782, 0.4122, 0.2881, 0.3074,  => 0.3215

precs (micro): 0.7162, 0.6174, 0.5718, 0.8556,  => 0.6902
recalls (micro): 0.7162, 0.6174, 0.5718, 0.8556,  => 0.6902
f1s (micro): 0.7162, 0.6174, 0.5718, 0.8556,  => 0.6902

precs (weighed): 0.5129, 0.6167, 0.4531, 0.7321,  => 0.5787
recalls (weighed): 0.7162, 0.6174, 0.5718, 0.8556,  => 0.6902
f1s (weighed): 0.5977, 0.4903, 0.4928, 0.7890,  => 0.5925


Epoch 11/20, Batch 264/264: 100%|██████████| 264/264 [03:52<00:00,  1.14it/s]
Epoch 11/20, Loss: 1129.4581,

accs: 0.4807, 0.4760, 0.4088, 0.5112,  => 0.4692

precs (macro): 0.2517, 0.2619, 0.2511, 0.2124,  => 0.2443
recalls (macro): 0.2538, 0.2209, 0.2860, 0.2130,  => 0.2434
f1s (macro): 0.2243, 0.2389, 0.2587, 0.1763,  => 0.2246

precs (micro): 0.4807, 0.4760, 0.4088, 0.5112,  => 0.4692
recalls (micro): 0.4807, 0.4760, 0.4088, 0.5112,  => 0.4692
f1s (micro): 0.4807, 0.4760, 0.4088, 0.5112,  => 0.4692

precs (weighed): 0.5890, 0.5503, 0.4072, 0.7212,  => 0.5669
recalls (weighed): 0.4807, 0.4760, 0.4088, 0.5112,  => 0.4692
f1s (weighed): 0.5276, 0.5090, 0.4049, 0.5982,  => 0.5099

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 139.1097,

accs: 0.7162, 0.6357, 0.5685, 0.8556,  => 0.6940

precs (macro): 0.2387, 0.6403, 0.2700, 0.2852,  => 0.3586
recalls (macro): 0.3333, 0.5433, 0.3207, 0.3333,  => 0.3827
f1s (macro): 0.2782, 0.4913, 0.2829, 0.3074,  => 0.3400

precs (micro): 0.7162, 0.6357, 0.5685, 0.8556,  => 0.6940
recalls (micro): 0.7162, 0.6357, 0.5685, 0.8556,  => 0.6940
f1s (micro): 0.7162, 0.6357, 0.5685, 0.8556,  => 0.6940

precs (weighed): 0.5129, 0.6391, 0.4484, 0.7321,  => 0.5831
recalls (weighed): 0.7162, 0.6357, 0.5685, 0.8556,  => 0.6940
f1s (weighed): 0.5977, 0.5522, 0.4862, 0.7890,  => 0.6063


Epoch 12/20, Batch 264/264: 100%|██████████| 264/264 [03:52<00:00,  1.14it/s]
Epoch 12/20, Loss: 1123.7804,

accs: 0.4964, 0.4819, 0.3915, 0.5136,  => 0.4708

precs (macro): 0.2571, 0.2683, 0.2433, 0.2125,  => 0.2453
recalls (macro): 0.2643, 0.3084, 0.2708, 0.2761,  => 0.2799
f1s (macro): 0.2337, 0.2445, 0.2476, 0.1769,  => 0.2257

precs (micro): 0.4964, 0.4819, 0.3915, 0.5136,  => 0.4708
recalls (micro): 0.4964, 0.4819, 0.3915, 0.5136,  => 0.4708
f1s (micro): 0.4964, 0.4819, 0.3915, 0.5136,  => 0.4708

precs (weighed): 0.5997, 0.5621, 0.3978, 0.7212,  => 0.5702
recalls (weighed): 0.4964, 0.4819, 0.3915, 0.5136,  => 0.4708
f1s (weighed): 0.5418, 0.5179, 0.3913, 0.5998,  => 0.5127

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 135.9676,

accs: 0.7162, 0.6174, 0.5618, 0.8556,  => 0.6878

precs (macro): 0.2387, 0.6830, 0.2911, 0.2852,  => 0.3745
recalls (macro): 0.3333, 0.5076, 0.3484, 0.3333,  => 0.3807
f1s (macro): 0.2782, 0.3994, 0.3090, 0.3074,  => 0.3235

precs (micro): 0.7162, 0.6174, 0.5618, 0.8556,  => 0.6878
recalls (micro): 0.7162, 0.6174, 0.5618, 0.8556,  => 0.6878
f1s (micro): 0.7162, 0.6174, 0.5618, 0.8556,  => 0.6878

precs (weighed): 0.5129, 0.6680, 0.4984, 0.7321,  => 0.6028
recalls (weighed): 0.7162, 0.6174, 0.5618, 0.8556,  => 0.6878
f1s (weighed): 0.5977, 0.4808, 0.5157, 0.7890,  => 0.5958


Epoch 13/20, Batch 264/264: 100%|██████████| 264/264 [03:51<00:00,  1.14it/s]
Epoch 13/20, Loss: 1124.7878,

accs: 0.4938, 0.4682, 0.3994, 0.5157,  => 0.4693

precs (macro): 0.2606, 0.2646, 0.2473, 0.2123,  => 0.2462
recalls (macro): 0.2601, 0.3042, 0.2748, 0.2143,  => 0.2633
f1s (macro): 0.2336, 0.2410, 0.2519, 0.1772,  => 0.2259

precs (micro): 0.4938, 0.4682, 0.3994, 0.5157,  => 0.4693
recalls (micro): 0.4938, 0.4682, 0.3994, 0.5157,  => 0.4693
f1s (micro): 0.4938, 0.4682, 0.3994, 0.5157,  => 0.4693

precs (weighed): 0.6045, 0.5549, 0.4049, 0.7209,  => 0.5713
recalls (weighed): 0.4938, 0.4682, 0.3994, 0.5157,  => 0.4693
f1s (weighed): 0.5421, 0.5076, 0.3990, 0.6012,  => 0.5125

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 136.9802,

accs: 0.7162, 0.6274, 0.5593, 0.8556,  => 0.6896

precs (macro): 0.2387, 0.6693, 0.2674, 0.2852,  => 0.3652
recalls (macro): 0.3333, 0.5240, 0.3123, 0.3333,  => 0.3757
f1s (macro): 0.2782, 0.4408, 0.2728, 0.3074,  => 0.3248

precs (micro): 0.7162, 0.6274, 0.5593, 0.8556,  => 0.6896
recalls (micro): 0.7162, 0.6274, 0.5593, 0.8556,  => 0.6896
f1s (micro): 0.7162, 0.6274, 0.5593, 0.8556,  => 0.6896

precs (weighed): 0.5129, 0.6591, 0.4423, 0.7321,  => 0.5866
recalls (weighed): 0.7162, 0.6274, 0.5593, 0.8556,  => 0.6896
f1s (weighed): 0.5977, 0.5135, 0.4713, 0.7890,  => 0.5929


Epoch 14/20, Batch 264/264: 100%|██████████| 264/264 [03:52<00:00,  1.14it/s]
Epoch 14/20, Loss: 1122.4288,

accs: 0.4970, 0.4754, 0.4065, 0.5003,  => 0.4698

precs (macro): 0.2604, 0.2675, 0.2504, 0.2120,  => 0.2476
recalls (macro): 0.2535, 0.2243, 0.2837, 0.3347,  => 0.2741
f1s (macro): 0.2343, 0.2439, 0.2572, 0.1741,  => 0.2274

precs (micro): 0.4970, 0.4754, 0.4065, 0.5003,  => 0.4698
recalls (micro): 0.4970, 0.4754, 0.4065, 0.5003,  => 0.4698
f1s (micro): 0.4970, 0.4754, 0.4065, 0.5003,  => 0.4698

precs (weighed): 0.6044, 0.5604, 0.4071, 0.7195,  => 0.5729
recalls (weighed): 0.4970, 0.4754, 0.4065, 0.5003,  => 0.4698
f1s (weighed): 0.5442, 0.5141, 0.4038, 0.5899,  => 0.5130

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 136.7798,

accs: 0.7162, 0.6315, 0.5618, 0.8556,  => 0.6913

precs (macro): 0.2387, 0.6723, 0.2664, 0.2852,  => 0.3656
recalls (macro): 0.3333, 0.5305, 0.3142, 0.3333,  => 0.3778
f1s (macro): 0.2782, 0.4556, 0.2748, 0.3074,  => 0.3290

precs (micro): 0.7162, 0.6315, 0.5618, 0.8556,  => 0.6913
recalls (micro): 0.7162, 0.6315, 0.5618, 0.8556,  => 0.6913
f1s (micro): 0.7162, 0.6315, 0.5618, 0.8556,  => 0.6913

precs (weighed): 0.5129, 0.6622, 0.4420, 0.7321,  => 0.5873
recalls (weighed): 0.7162, 0.6315, 0.5618, 0.8556,  => 0.6913
f1s (weighed): 0.5977, 0.5252, 0.4747, 0.7890,  => 0.5967

Early stopping triggered
xlm-roberta-base

[13:46:14] task: task-2                                                                                my_import.py:133
           model_type: simple                                                                          my_import.py:133
           model_name: xlm-roberta-base                                                                my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/simple_xlm-roberta-base                                        my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

Training ...
Epoch 1/20, Batch 264/264: 100%|██████████| 264/264 [01:22<00:00,  3.22it/s]
Epoch 1/20, Loss: 1174.8198,

accs: 0.5039, 0.4573, 0.3623, 0.5101,  => 0.4584

precs (macro): 0.2524, 0.2522, 0.2519, 0.2580,  => 0.2536
recalls (macro): 0.2650, 0.2941, 0.2490, 0.2139,  => 0.2555
f1s (macro): 0.2345, 0.2290, 0.2176, 0.1791,  => 0.2150

precs (micro): 0.5039, 0.4573, 0.3623, 0.5101,  => 0.4584
recalls (micro): 0.5039, 0.4573, 0.3623, 0.5101,  => 0.4584
f1s (micro): 0.5039, 0.4573, 0.3623, 0.5101,  => 0.4584

precs (weighed): 0.5951, 0.5321, 0.3725, 0.7419,  => 0.5604
recalls (weighed): 0.5039, 0.4573, 0.3623, 0.5101,  => 0.4584
f1s (weighed): 0.5446, 0.4902, 0.3498, 0.5997,  => 0.4961

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.63it/s]
Evaluation, Loss: 148.9573,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423

Saved the best model to path: ./models/task_2/simple_xlm-roberta-base_0.pth

Epoch 2/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.34it/s]
Epoch 2/20, Loss: 1175.7467,

accs: 0.5014, 0.4652, 0.3776, 0.5055,  => 0.4624

precs (macro): 0.2542, 0.2543, 0.2185, 0.2125,  => 0.2349
recalls (macro): 0.2488, 0.2131, 0.2459, 0.2738,  => 0.2454
f1s (macro): 0.2342, 0.2303, 0.2227, 0.1753,  => 0.2156

precs (micro): 0.5014, 0.4652, 0.3776, 0.5055,  => 0.4624
recalls (micro): 0.5014, 0.4652, 0.3776, 0.5055,  => 0.4624
f1s (micro): 0.5014, 0.4652, 0.3776, 0.5055,  => 0.4624

precs (weighed): 0.5995, 0.5361, 0.3561, 0.7214,  => 0.5533
recalls (weighed): 0.5014, 0.4652, 0.3776, 0.5055,  => 0.4624
f1s (weighed): 0.5453, 0.4954, 0.3616, 0.5943,  => 0.4991

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.64it/s]
Evaluation, Loss: 141.4460,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423

Saved the best model to path: ./models/task_2/simple_xlm-roberta-base_1.pth

Epoch 3/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.35it/s]
Epoch 3/20, Loss: 1171.8503,

accs: 0.5018, 0.4560, 0.3839, 0.5088,  => 0.4626

precs (macro): 0.2479, 0.2443, 0.2204, 0.2115,  => 0.2310
recalls (macro): 0.2449, 0.2066, 0.2520, 0.2123,  => 0.2289
f1s (macro): 0.2310, 0.2218, 0.2255, 0.1755,  => 0.2134

precs (micro): 0.5018, 0.4560, 0.3839, 0.5088,  => 0.4626
recalls (micro): 0.5018, 0.4560, 0.3839, 0.5088,  => 0.4626
f1s (micro): 0.5018, 0.4560, 0.3839, 0.5088,  => 0.4626

precs (weighed): 0.5884, 0.5179, 0.3572, 0.7180,  => 0.5454
recalls (weighed): 0.5018, 0.4560, 0.3839, 0.5088,  => 0.4626
f1s (weighed): 0.5410, 0.4814, 0.3640, 0.5955,  => 0.4955

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.64it/s]
Evaluation, Loss: 149.7664,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423


Epoch 4/20, Batch 264/264: 100%|██████████| 264/264 [01:19<00:00,  3.34it/s]
Epoch 4/20, Loss: 1168.9709,

accs: 0.4982, 0.4698, 0.3849, 0.5111,  => 0.4660

precs (macro): 0.2510, 0.2518, 0.2185, 0.2117,  => 0.2332
recalls (macro): 0.2544, 0.2130, 0.2482, 0.3379,  => 0.2634
f1s (macro): 0.2334, 0.2284, 0.2237, 0.1762,  => 0.2154

precs (micro): 0.4982, 0.4698, 0.3849, 0.5111,  => 0.4660
recalls (micro): 0.4982, 0.4698, 0.3849, 0.5111,  => 0.4660
f1s (micro): 0.4982, 0.4698, 0.3849, 0.5111,  => 0.4660

precs (weighed): 0.5929, 0.5319, 0.3550, 0.7184,  => 0.5496
recalls (weighed): 0.4982, 0.4698, 0.3849, 0.5111,  => 0.4660
f1s (weighed): 0.5404, 0.4947, 0.3634, 0.5970,  => 0.4989

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.61it/s]
Evaluation, Loss: 146.8508,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423


Epoch 5/20, Batch 264/264: 100%|██████████| 264/264 [01:18<00:00,  3.35it/s]
Epoch 5/20, Loss: 1167.6460,

accs: 0.5081, 0.4560, 0.3728, 0.5144,  => 0.4628

precs (macro): 0.2539, 0.2471, 0.3333, 0.2123,  => 0.2617
recalls (macro): 0.2638, 0.2071, 0.2373, 0.2764,  => 0.2461
f1s (macro): 0.2376, 0.2232, 0.2145, 0.1770,  => 0.2131

precs (micro): 0.5081, 0.4560, 0.3728, 0.5144,  => 0.4628
recalls (micro): 0.5081, 0.4560, 0.3728, 0.5144,  => 0.4628
f1s (micro): 0.5081, 0.4560, 0.3728, 0.5144,  => 0.4628

precs (weighed): 0.5968, 0.5223, 0.4053, 0.7207,  => 0.5613
recalls (weighed): 0.5081, 0.4560, 0.3728, 0.5144,  => 0.4628
f1s (weighed): 0.5478, 0.4832, 0.3506, 0.6002,  => 0.4954

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.65it/s]
Evaluation, Loss: 149.3197,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423


Epoch 6/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.40it/s]
Epoch 6/20, Loss: 1168.3340,

accs: 0.5033, 0.4562, 0.3800, 0.5094,  => 0.4622

precs (macro): 0.2526, 0.2453, 0.2176, 0.2123,  => 0.2319
recalls (macro): 0.2535, 0.2071, 0.2546, 0.1500,  => 0.2163
f1s (macro): 0.2350, 0.2227, 0.2250, 0.1758,  => 0.2146

precs (micro): 0.5033, 0.4562, 0.3800, 0.5094,  => 0.4622
recalls (micro): 0.5033, 0.4562, 0.3800, 0.5094,  => 0.4622
f1s (micro): 0.5033, 0.4562, 0.3800, 0.5094,  => 0.4622

precs (weighed): 0.5958, 0.5203, 0.3490, 0.7210,  => 0.5465
recalls (weighed): 0.5033, 0.4562, 0.3800, 0.5094,  => 0.4622
f1s (weighed): 0.5447, 0.4830, 0.3580, 0.5970,  => 0.4957

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.70it/s]
Evaluation, Loss: 145.0270,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423


Epoch 7/20, Batch 264/264: 100%|██████████| 264/264 [01:17<00:00,  3.39it/s]
Epoch 7/20, Loss: 1164.1805,

accs: 0.5020, 0.4620, 0.3838, 0.5100,  => 0.4644

precs (macro): 0.2525, 0.2494, 0.2180, 0.2120,  => 0.2330
recalls (macro): 0.2527, 0.2093, 0.2514, 0.3376,  => 0.2627
f1s (macro): 0.2345, 0.2252, 0.2241, 0.1761,  => 0.2150

precs (micro): 0.5020, 0.4620, 0.3838, 0.5100,  => 0.4644
recalls (micro): 0.5020, 0.4620, 0.3838, 0.5100,  => 0.4644
f1s (micro): 0.5020, 0.4620, 0.3838, 0.5100,  => 0.4644

precs (weighed): 0.5952, 0.5272, 0.3523, 0.7193,  => 0.5485
recalls (weighed): 0.5020, 0.4620, 0.3838, 0.5100,  => 0.4644
f1s (weighed): 0.5437, 0.4883, 0.3611, 0.5966,  => 0.4974

Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:10<00:00,  3.68it/s]
Evaluation, Loss: 143.2543,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423

Early stopping triggered

bert-base-multilingual-cased

[14:47:50] task: task-2                                                                                                                                       my_import.py:133
           model_type: simple                                                                                                                                 my_import.py:133
           model_name: bert-base-multilingual-cased                                                                                                           my_import.py:133
           padding_len: 512                                                                                                                                   my_import.py:131
           batch_size: 32                                                                                                                                     my_import.py:133
           learning_rate: 0.001                                                                                                                               my_import.py:133
           epochs: 20                                                                                                                                         my_import.py:133
           fine_tune: True                                                                                                                                    my_import.py:133
           device: cuda                                                                                                                                       my_import.py:133
           saving_path: ./models/task_2/simple_bert-base-multilingual-cased                                                                                   my_import.py:133
           train_shape: (8437, 27)                                                                                                                            my_import.py:133
           dev_shape: (1205, 27)                                                                                                                              my_import.py:133
           test_shape: (2412, 27)                                                                                                                             my_import.py:133
Training ...

Epoch 1/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:10<00:00,  3.74it/s]
Epoch 1/20, Loss: 1189.7544,

accs: 0.4412, 0.4496, 0.3076, 0.5116,  => 0.4275

precs (macro): 0.2149, 0.2528, 0.1580, 0.2417,  => 0.2168
recalls (macro): 0.2551, 0.2072, 0.2413, 0.2758,  => 0.2448
f1s (macro): 0.1704, 0.2268, 0.1542, 0.1774,  => 0.1822

precs (micro): 0.4412, 0.4496, 0.3076, 0.5116,  => 0.4275
recalls (micro): 0.4412, 0.4496, 0.3076, 0.5116,  => 0.4275
f1s (micro): 0.4412, 0.4496, 0.3076, 0.5116,  => 0.4275

precs (weighed): 0.5554, 0.5336, 0.2529, 0.7386,  => 0.5201
recalls (weighed): 0.4412, 0.4496, 0.3076, 0.5116,  => 0.4275
f1s (weighed): 0.4760, 0.4863, 0.2652, 0.6002,  => 0.4569

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:09<00:00,  3.93it/s]
Evaluation, Loss: 151.0271,

accs: 0.7162, 0.6241, 0.4855, 0.8556,  => 0.6703

precs (macro): 0.2387, 0.6701, 0.1214, 0.2852,  => 0.3289
recalls (macro): 0.3333, 0.5185, 0.2500, 0.3333,  => 0.3588
f1s (macro): 0.2782, 0.4276, 0.1634, 0.3074,  => 0.2941

precs (micro): 0.7162, 0.6241, 0.4855, 0.8556,  => 0.6703
recalls (micro): 0.7162, 0.6241, 0.4855, 0.8556,  => 0.6703
f1s (micro): 0.7162, 0.6241, 0.4855, 0.8556,  => 0.6703

precs (weighed): 0.5129, 0.6592, 0.2357, 0.7321,  => 0.5350
recalls (weighed): 0.7162, 0.6241, 0.4855, 0.8556,  => 0.6703
f1s (weighed): 0.5977, 0.5030, 0.3173, 0.7890,  => 0.5518

Saved the best model to path: ./models/task_2/simple_bert-base-multilingual-cased_0.pth

Epoch 2/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:10<00:00,  3.74it/s]
Epoch 2/20, Loss: 1174.1111,

accs: 0.4380, 0.4606, 0.3097, 0.5226,  => 0.4327

precs (macro): 0.1832, 0.2576, 0.1345, 0.2138,  => 0.1973
recalls (macro): 0.2567, 0.2980, 0.2487, 0.2788,  => 0.2705
f1s (macro): 0.1694, 0.2340, 0.1558, 0.1791,  => 0.1846

precs (micro): 0.4380, 0.4606, 0.3097, 0.5226,  => 0.4327
recalls (micro): 0.4380, 0.4606, 0.3097, 0.5226,  => 0.4327
f1s (micro): 0.4380, 0.4606, 0.3097, 0.5226,  => 0.4327

precs (weighed): 0.5202, 0.5419, 0.2416, 0.7255,  => 0.5073
recalls (weighed): 0.4380, 0.4606, 0.3097, 0.5226,  => 0.4327
f1s (weighed): 0.4731, 0.4971, 0.2664, 0.6074,  => 0.4610

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:09<00:00,  3.84it/s]
Evaluation, Loss: 143.4588,

accs: 0.7162, 0.5934, 0.4855, 0.8556,  => 0.6627

precs (macro): 0.2387, 0.5684, 0.1214, 0.2852,  => 0.3034
recalls (macro): 0.3333, 0.5670, 0.2500, 0.3333,  => 0.3709
f1s (macro): 0.2782, 0.5674, 0.1634, 0.3074,  => 0.3291

precs (micro): 0.7162, 0.5934, 0.4855, 0.8556,  => 0.6627
recalls (micro): 0.7162, 0.5934, 0.4855, 0.8556,  => 0.6627
f1s (micro): 0.7162, 0.5934, 0.4855, 0.8556,  => 0.6627

precs (weighed): 0.5129, 0.5896, 0.2357, 0.7321,  => 0.5176
recalls (weighed): 0.7162, 0.5934, 0.4855, 0.8556,  => 0.6627
f1s (weighed): 0.5977, 0.5913, 0.3173, 0.7890,  => 0.5738

Saved the best model to path: ./models/task_2/simple_bert-base-multilingual-cased_1.pth

Epoch 3/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:11<00:00,  3.72it/s]
Epoch 3/20, Loss: 1184.7114,

accs: 0.4334, 0.4558, 0.3099, 0.5018,  => 0.4253

precs (macro): 0.1819, 0.2526, 0.1352, 0.2109,  => 0.1951
recalls (macro): 0.2466, 0.2117, 0.2553, 0.2102,  => 0.2310
f1s (macro): 0.1677, 0.2298, 0.1569, 0.1739,  => 0.1820

precs (micro): 0.4334, 0.4558, 0.3099, 0.5018,  => 0.4253
recalls (micro): 0.4334, 0.4558, 0.3099, 0.5018,  => 0.4253
f1s (micro): 0.4334, 0.4558, 0.3099, 0.5018,  => 0.4253

precs (weighed): 0.5173, 0.5326, 0.2415, 0.7160,  => 0.5018
recalls (weighed): 0.4334, 0.4558, 0.3099, 0.5018,  => 0.4253
f1s (weighed): 0.4694, 0.4903, 0.2660, 0.5900,  => 0.4539

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:10<00:00,  3.73it/s]
Evaluation, Loss: 153.1206,

accs: 0.7162, 0.6232, 0.4855, 0.8556,  => 0.6701

precs (macro): 0.2387, 0.6902, 0.1214, 0.2852,  => 0.3339
recalls (macro): 0.3333, 0.5163, 0.2500, 0.3333,  => 0.3582
f1s (macro): 0.2782, 0.4203, 0.1634, 0.3074,  => 0.2923

precs (micro): 0.7162, 0.6232, 0.4855, 0.8556,  => 0.6701
recalls (micro): 0.7162, 0.6232, 0.4855, 0.8556,  => 0.6701
f1s (micro): 0.7162, 0.6232, 0.4855, 0.8556,  => 0.6701

precs (weighed): 0.5129, 0.6745, 0.2357, 0.7321,  => 0.5388
recalls (weighed): 0.7162, 0.6232, 0.4855, 0.8556,  => 0.6701
f1s (weighed): 0.5977, 0.4974, 0.3173, 0.7890,  => 0.5504


Epoch 4/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:12<00:00,  3.66it/s]
Epoch 4/20, Loss: 1174.6467,

accs: 0.4365, 0.4595, 0.3043, 0.5138,  => 0.4285

precs (macro): 0.1825, 0.2550, 0.1307, 0.2134,  => 0.1954
recalls (macro): 0.2392, 0.3802, 0.2410, 0.2762,  => 0.2842
f1s (macro): 0.1683, 0.2324, 0.1520, 0.1772,  => 0.1825

precs (micro): 0.4365, 0.4595, 0.3043, 0.5138,  => 0.4285
recalls (micro): 0.4365, 0.4595, 0.3043, 0.5138,  => 0.4285
f1s (micro): 0.4365, 0.4595, 0.3043, 0.5138,  => 0.4285

precs (weighed): 0.5197, 0.5358, 0.2349, 0.7243,  => 0.5037
recalls (weighed): 0.4365, 0.4595, 0.3043, 0.5138,  => 0.4285
f1s (weighed): 0.4725, 0.4936, 0.2605, 0.6010,  => 0.4569

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:09<00:00,  3.86it/s]
Evaluation, Loss: 150.0841,

accs: 0.7162, 0.6340, 0.4855, 0.8556,  => 0.6728

precs (macro): 0.2387, 0.6609, 0.1214, 0.2852,  => 0.3265
recalls (macro): 0.3333, 0.5361, 0.2500, 0.3333,  => 0.3632
f1s (macro): 0.2782, 0.4703, 0.1634, 0.3074,  => 0.3048

precs (micro): 0.7162, 0.6340, 0.4855, 0.8556,  => 0.6728
recalls (micro): 0.7162, 0.6340, 0.4855, 0.8556,  => 0.6728
f1s (micro): 0.7162, 0.6340, 0.4855, 0.8556,  => 0.6728

precs (weighed): 0.5129, 0.6541, 0.2357, 0.7321,  => 0.5337
recalls (weighed): 0.7162, 0.6340, 0.4855, 0.8556,  => 0.6728
f1s (weighed): 0.5977, 0.5365, 0.3173, 0.7890,  => 0.5601


Epoch 5/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:11<00:00,  3.70it/s]
Epoch 5/20, Loss: 1172.5721,

accs: 0.4388, 0.4665, 0.3117, 0.5121,  => 0.4323

precs (macro): 0.1822, 0.2610, 0.1328, 0.2128,  => 0.1972
recalls (macro): 0.2315, 0.3006, 0.2443, 0.2133,  => 0.2474
f1s (macro): 0.1684, 0.2369, 0.1549, 0.1766,  => 0.1842

precs (micro): 0.4388, 0.4665, 0.3117, 0.5121,  => 0.4323
recalls (micro): 0.4388, 0.4665, 0.3117, 0.5121,  => 0.4323
f1s (micro): 0.4388, 0.4665, 0.3117, 0.5121,  => 0.4323

precs (weighed): 0.5192, 0.5485, 0.2390, 0.7224,  => 0.5073
recalls (weighed): 0.4388, 0.4665, 0.3117, 0.5121,  => 0.4323
f1s (weighed): 0.4738, 0.5032, 0.2660, 0.5993,  => 0.4606

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:09<00:00,  3.95it/s]
Evaluation, Loss: 145.9453,

accs: 0.7162, 0.6257, 0.4855, 0.8556,  => 0.6707

precs (macro): 0.2387, 0.5990, 0.1214, 0.2852,  => 0.3111
recalls (macro): 0.3333, 0.5380, 0.2500, 0.3333,  => 0.3637
f1s (macro): 0.2782, 0.4933, 0.1634, 0.3074,  => 0.3106

precs (micro): 0.7162, 0.6257, 0.4855, 0.8556,  => 0.6707
recalls (micro): 0.7162, 0.6257, 0.4855, 0.8556,  => 0.6707
f1s (micro): 0.7162, 0.6257, 0.4855, 0.8556,  => 0.6707

precs (weighed): 0.5129, 0.6066, 0.2357, 0.7321,  => 0.5218
recalls (weighed): 0.7162, 0.6257, 0.4855, 0.8556,  => 0.6707
f1s (weighed): 0.5977, 0.5516, 0.3173, 0.7890,  => 0.5639


Epoch 6/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:10<00:00,  3.77it/s]
Epoch 6/20, Loss: 1170.3943,

accs: 0.4301, 0.4767, 0.3156, 0.5084,  => 0.4327

precs (macro): 0.1820, 0.2654, 0.2600, 0.2122,  => 0.2299
recalls (macro): 0.2512, 0.3904, 0.2557, 0.2121,  => 0.2773
f1s (macro): 0.1671, 0.2431, 0.1585, 0.1756,  => 0.1861

precs (micro): 0.4301, 0.4767, 0.3156, 0.5084,  => 0.4327
recalls (micro): 0.4301, 0.4767, 0.3156, 0.5084,  => 0.4327
f1s (micro): 0.4301, 0.4767, 0.3156, 0.5084,  => 0.4327

precs (weighed): 0.5172, 0.5561, 0.3052, 0.7204,  => 0.5247
recalls (weighed): 0.4301, 0.4767, 0.3156, 0.5084,  => 0.4327
f1s (weighed): 0.4673, 0.5127, 0.2684, 0.5960,  => 0.4611

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:09<00:00,  3.94it/s]
Evaluation, Loss: 145.3376,

accs: 0.7162, 0.6307, 0.4855, 0.8556,  => 0.6720

precs (macro): 0.2387, 0.5992, 0.1214, 0.2852,  => 0.3111
recalls (macro): 0.3333, 0.5637, 0.2500, 0.3333,  => 0.3701
f1s (macro): 0.2782, 0.5493, 0.1634, 0.3074,  => 0.3246

precs (micro): 0.7162, 0.6307, 0.4855, 0.8556,  => 0.6720
recalls (micro): 0.7162, 0.6307, 0.4855, 0.8556,  => 0.6720
f1s (micro): 0.7162, 0.6307, 0.4855, 0.8556,  => 0.6720

precs (weighed): 0.5129, 0.6105, 0.2357, 0.7321,  => 0.5228
recalls (weighed): 0.7162, 0.6307, 0.4855, 0.8556,  => 0.6720
f1s (weighed): 0.5977, 0.5924, 0.3173, 0.7890,  => 0.5741


Epoch 7/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:10<00:00,  3.76it/s]
Epoch 7/20, Loss: 1172.3602,

accs: 0.4333, 0.4697, 0.3051, 0.5111,  => 0.4298

precs (macro): 0.1822, 0.2578, 0.1348, 0.2129,  => 0.1969
recalls (macro): 0.2466, 0.2183, 0.2517, 0.3379,  => 0.2636
f1s (macro): 0.1677, 0.2359, 0.1552, 0.1766,  => 0.1839

precs (micro): 0.4333, 0.4697, 0.3051, 0.5111,  => 0.4298
recalls (micro): 0.4333, 0.4697, 0.3051, 0.5111,  => 0.4298
f1s (micro): 0.4333, 0.4697, 0.3051, 0.5111,  => 0.4298

precs (weighed): 0.5182, 0.5436, 0.2414, 0.7224,  => 0.5064
recalls (weighed): 0.4333, 0.4697, 0.3051, 0.5111,  => 0.4298
f1s (weighed): 0.4697, 0.5030, 0.2640, 0.5984,  => 0.4588

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:09<00:00,  3.95it/s]
Evaluation, Loss: 143.6228,

accs: 0.7162, 0.6332, 0.4855, 0.8556,  => 0.6726

precs (macro): 0.2387, 0.6916, 0.1214, 0.2852,  => 0.3342
recalls (macro): 0.3333, 0.5315, 0.2500, 0.3333,  => 0.3620
f1s (macro): 0.2782, 0.4550, 0.1634, 0.3074,  => 0.3010

precs (micro): 0.7162, 0.6332, 0.4855, 0.8556,  => 0.6726
recalls (micro): 0.7162, 0.6332, 0.4855, 0.8556,  => 0.6726
f1s (micro): 0.7162, 0.6332, 0.4855, 0.8556,  => 0.6726

precs (weighed): 0.5129, 0.6773, 0.2357, 0.7321,  => 0.5395
recalls (weighed): 0.7162, 0.6332, 0.4855, 0.8556,  => 0.6726
f1s (weighed): 0.5977, 0.5251, 0.3173, 0.7890,  => 0.5573

Early stopping triggered
(quocenv) ➜  training-model git:(master) ✗ python run_train_cls_task.py --task "task-2" --model_type "simple" --model_name "distilbert-base-multilingual-cased" --source_len 512 --batch_size 32 --learning_rate 0.001 --epochs 20

[15:01:11] task: task-2                                                                                my_import.py:133
           model_type: simple                                                                          my_import.py:133
           model_name: distilbert-base-multilingual-cased                                              my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/simple_distilbert-base-multilingual-cased                      my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

Training ...
Epoch 1/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:45<00:00,  5.86it/s]
Epoch 1/20, Loss: 1170.3409,

accs: 0.4980, 0.4695, 0.3670, 0.5052,  => 0.4599

precs (macro): 0.2550, 0.2635, 0.2422, 0.2361,  => 0.2492
recalls (macro): 0.2580, 0.3044, 0.2559, 0.2116,  => 0.2575
f1s (macro): 0.2355, 0.2405, 0.2224, 0.1760,  => 0.2186

precs (micro): 0.4980, 0.4695, 0.3670, 0.5052,  => 0.4599
recalls (micro): 0.4980, 0.4695, 0.3670, 0.5052,  => 0.4599
f1s (micro): 0.4980, 0.4695, 0.3670, 0.5052,  => 0.4599

precs (weighed): 0.5994, 0.5526, 0.3645, 0.7317,  => 0.5620
recalls (weighed): 0.4980, 0.4695, 0.3670, 0.5052,  => 0.4599
f1s (weighed): 0.5429, 0.5073, 0.3528, 0.5942,  => 0.4993

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.86it/s]
Evaluation, Loss: 141.7363,

accs: 0.7162, 0.6415, 0.5037, 0.8556,  => 0.6793

precs (macro): 0.2387, 0.6305, 0.2618, 0.2852,  => 0.3541
recalls (macro): 0.3333, 0.5603, 0.2655, 0.3333,  => 0.3731
f1s (macro): 0.2782, 0.5299, 0.1995, 0.3074,  => 0.3288

precs (micro): 0.7162, 0.6415, 0.5037, 0.8556,  => 0.6793
recalls (micro): 0.7162, 0.6415, 0.5037, 0.8556,  => 0.6793
f1s (micro): 0.7162, 0.6415, 0.5037, 0.8556,  => 0.6793

precs (weighed): 0.5129, 0.6338, 0.4233, 0.7321,  => 0.5755
recalls (weighed): 0.7162, 0.6415, 0.5037, 0.8556,  => 0.6793
f1s (weighed): 0.5977, 0.5814, 0.3666, 0.7890,  => 0.5837

Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_0.pth

Epoch 2/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:43<00:00,  6.12it/s]
Epoch 2/20, Loss: 1137.5408,

accs: 0.5010, 0.4729, 0.3980, 0.5105,  => 0.4706

precs (macro): 0.2513, 0.2675, 0.2396, 0.2132,  => 0.2429
recalls (macro): 0.2704, 0.2241, 0.2738, 0.3377,  => 0.2765
f1s (macro): 0.2345, 0.2438, 0.2477, 0.1766,  => 0.2256

precs (micro): 0.5010, 0.4729, 0.3980, 0.5105,  => 0.4706
recalls (micro): 0.5010, 0.4729, 0.3980, 0.5105,  => 0.4706
f1s (micro): 0.5010, 0.4729, 0.3980, 0.5105,  => 0.4706

precs (weighed): 0.5922, 0.5616, 0.3847, 0.7235,  => 0.5655
recalls (weighed): 0.5010, 0.4729, 0.3980, 0.5105,  => 0.4706
f1s (weighed): 0.5415, 0.5134, 0.3894, 0.5984,  => 0.5107

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.87it/s]
Evaluation, Loss: 135.6420,

accs: 0.7162, 0.6257, 0.5743, 0.8556,  => 0.6929

precs (macro): 0.2387, 0.5933, 0.3031, 0.2852,  => 0.3551
recalls (macro): 0.3333, 0.5757, 0.3625, 0.3333,  => 0.4012
f1s (macro): 0.2782, 0.5728, 0.3171, 0.3074,  => 0.3689

precs (micro): 0.7162, 0.6257, 0.5743, 0.8556,  => 0.6929
recalls (micro): 0.7162, 0.6257, 0.5743, 0.8556,  => 0.6929
f1s (micro): 0.7162, 0.6257, 0.5743, 0.8556,  => 0.6929

precs (weighed): 0.5129, 0.6085, 0.5178, 0.7321,  => 0.5928
recalls (weighed): 0.7162, 0.6257, 0.5743, 0.8556,  => 0.6929
f1s (weighed): 0.5977, 0.6066, 0.5242, 0.7890,  => 0.6294

Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_1.pth

Epoch 3/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:42<00:00,  6.18it/s]
Epoch 3/20, Loss: 1124.4472,

accs: 0.4945, 0.4784, 0.4086, 0.5214,  => 0.4757

precs (macro): 0.2485, 0.2689, 0.2550, 0.2122,  => 0.2462
recalls (macro): 0.2293, 0.2259, 0.2892, 0.2784,  => 0.2557
f1s (macro): 0.2289, 0.2454, 0.2606, 0.1784,  => 0.2283

precs (micro): 0.4945, 0.4784, 0.4086, 0.5214,  => 0.4757
recalls (micro): 0.4945, 0.4784, 0.4086, 0.5214,  => 0.4757
f1s (micro): 0.4945, 0.4784, 0.4086, 0.5214,  => 0.4757

precs (weighed): 0.5913, 0.5642, 0.4113, 0.7202,  => 0.5717
recalls (weighed): 0.4945, 0.4784, 0.4086, 0.5214,  => 0.4757
f1s (weighed): 0.5380, 0.5175, 0.4074, 0.6047,  => 0.5169

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.85it/s]
Evaluation, Loss: 133.2536,

accs: 0.7162, 0.6498, 0.6299, 0.8556,  => 0.7129

precs (macro): 0.2387, 0.6809, 0.3089, 0.2852,  => 0.3784
recalls (macro): 0.3333, 0.5596, 0.3800, 0.3333,  => 0.4016
f1s (macro): 0.2782, 0.5152, 0.3406, 0.3074,  => 0.3603

precs (micro): 0.7162, 0.6498, 0.6299, 0.8556,  => 0.7129
recalls (micro): 0.7162, 0.6498, 0.6299, 0.8556,  => 0.7129
f1s (micro): 0.7162, 0.6498, 0.6299, 0.8556,  => 0.7129

precs (weighed): 0.5129, 0.6725, 0.5163, 0.7321,  => 0.6084
recalls (weighed): 0.7162, 0.6498, 0.6299, 0.8556,  => 0.7129
f1s (weighed): 0.5977, 0.5726, 0.5672, 0.7890,  => 0.6316

Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_2.pth

Epoch 4/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:42<00:00,  6.22it/s]
Epoch 4/20, Loss: 1124.1096,

accs: 0.4913, 0.4737, 0.4126, 0.5063,  => 0.4710

precs (macro): 0.2480, 0.2682, 0.2578, 0.2113,  => 0.2463
recalls (macro): 0.2379, 0.2254, 0.2811, 0.2740,  => 0.2546
f1s (macro): 0.2294, 0.2449, 0.2603, 0.1750,  => 0.2274

precs (micro): 0.4913, 0.4737, 0.4126, 0.5063,  => 0.4710
recalls (micro): 0.4913, 0.4737, 0.4126, 0.5063,  => 0.4710
f1s (micro): 0.4913, 0.4737, 0.4126, 0.5063,  => 0.4710

precs (weighed): 0.5894, 0.5614, 0.4209, 0.7173,  => 0.5722
recalls (weighed): 0.4913, 0.4737, 0.4126, 0.5063,  => 0.4710
f1s (weighed): 0.5350, 0.5138, 0.4146, 0.5935,  => 0.5142

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.53it/s]
Evaluation, Loss: 130.5261,

accs: 0.7228, 0.6498, 0.6282, 0.8556,  => 0.7141

precs (macro): 0.5404, 0.6851, 0.3079, 0.2852,  => 0.4547
recalls (macro): 0.3420, 0.5588, 0.3788, 0.3333,  => 0.4032
f1s (macro): 0.2968, 0.5128, 0.3394, 0.3074,  => 0.3641

precs (micro): 0.7228, 0.6498, 0.6282, 0.8556,  => 0.7141
recalls (micro): 0.7228, 0.6498, 0.6282, 0.8556,  => 0.7141
f1s (micro): 0.7228, 0.6498, 0.6282, 0.8556,  => 0.7141

precs (weighed): 0.7638, 0.6756, 0.5162, 0.7321,  => 0.6719
recalls (weighed): 0.7228, 0.6498, 0.6282, 0.8556,  => 0.7141
f1s (weighed): 0.6145, 0.5709, 0.5663, 0.7890,  => 0.6352

Saved the best model to path: ./models/task_2/simple_distilbert-base-multilingual-cased_3.pth

Epoch 5/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:44<00:00,  5.94it/s]
Epoch 5/20, Loss: 1126.4046,

accs: 0.4797, 0.4697, 0.4106, 0.5148,  => 0.4687

precs (macro): 0.2450, 0.2659, 0.2575, 0.2128,  => 0.2453
recalls (macro): 0.2387, 0.3057, 0.2834, 0.1516,  => 0.2448
f1s (macro): 0.2256, 0.2424, 0.2606, 0.1770,  => 0.2264

precs (micro): 0.4797, 0.4697, 0.4106, 0.5148,  => 0.4687
recalls (micro): 0.4797, 0.4697, 0.4106, 0.5148,  => 0.4687
f1s (micro): 0.4797, 0.4697, 0.4106, 0.5148,  => 0.4687

precs (weighed): 0.5827, 0.5583, 0.4180, 0.7228,  => 0.5704
recalls (weighed): 0.4797, 0.4697, 0.4106, 0.5148,  => 0.4687
f1s (weighed): 0.5251, 0.5101, 0.4120, 0.6013,  => 0.5121

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.58it/s]
Evaluation, Loss: 132.4928,

accs: 0.7220, 0.6523, 0.6224, 0.8556,  => 0.7131

precs (macro): 0.5365, 0.6483, 0.3057, 0.2852,  => 0.4439
recalls (macro): 0.3410, 0.5750, 0.3701, 0.3333,  => 0.4049
f1s (macro): 0.2948, 0.5521, 0.3339, 0.3074,  => 0.3720

precs (micro): 0.7220, 0.6523, 0.6224, 0.8556,  => 0.7131
recalls (micro): 0.7220, 0.6523, 0.6224, 0.8556,  => 0.7131
f1s (micro): 0.7220, 0.6523, 0.6224, 0.8556,  => 0.7131

precs (weighed): 0.7603, 0.6496, 0.5042, 0.7321,  => 0.6615
recalls (weighed): 0.7220, 0.6523, 0.6224, 0.8556,  => 0.7131
f1s (weighed): 0.6126, 0.5997, 0.5557, 0.7890,  => 0.6392


Epoch 6/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:43<00:00,  6.09it/s]
Epoch 6/20, Loss: 1113.6365,

accs: 0.5094, 0.4820, 0.4035, 0.5174,  => 0.4781

precs (macro): 0.2583, 0.2679, 0.2536, 0.2136,  => 0.2484
recalls (macro): 0.2667, 0.2257, 0.2845, 0.4022,  => 0.2948
f1s (macro): 0.2409, 0.2446, 0.2572, 0.1782,  => 0.2302

precs (micro): 0.5094, 0.4820, 0.4035, 0.5174,  => 0.4781
recalls (micro): 0.5094, 0.4820, 0.4035, 0.5174,  => 0.4781
f1s (micro): 0.5094, 0.4820, 0.4035, 0.5174,  => 0.4781

precs (weighed): 0.6049, 0.5628, 0.4070, 0.7245,  => 0.5748
recalls (weighed): 0.5094, 0.4820, 0.4035, 0.5174,  => 0.4781
f1s (weighed): 0.5519, 0.5187, 0.4028, 0.6033,  => 0.5192

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.49it/s]
Evaluation, Loss: 135.7840,

accs: 0.7220, 0.6465, 0.6166, 0.8556,  => 0.7102

precs (macro): 0.5365, 0.6371, 0.3135, 0.2852,  => 0.4431
recalls (macro): 0.3410, 0.5683, 0.3586, 0.3333,  => 0.4003
f1s (macro): 0.2948, 0.5432, 0.3258, 0.3074,  => 0.3678

precs (micro): 0.7220, 0.6465, 0.6166, 0.8556,  => 0.7102
recalls (micro): 0.7220, 0.6465, 0.6166, 0.8556,  => 0.7102
f1s (micro): 0.7220, 0.6465, 0.6166, 0.8556,  => 0.7102

precs (weighed): 0.7603, 0.6400, 0.5076, 0.7321,  => 0.6600
recalls (weighed): 0.7220, 0.6465, 0.6166, 0.8556,  => 0.7102
f1s (weighed): 0.6126, 0.5920, 0.5434, 0.7890,  => 0.6343


Epoch 7/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:43<00:00,  6.06it/s]
Epoch 7/20, Loss: 1114.6906,

accs: 0.4984, 0.4774, 0.4078, 0.5065,  => 0.4725

precs (macro): 0.2519, 0.2684, 0.2589, 0.2123,  => 0.2479
recalls (macro): 0.2328, 0.3931, 0.2882, 0.2740,  => 0.2970
f1s (macro): 0.2324, 0.2462, 0.2607, 0.1754,  => 0.2287

precs (micro): 0.4984, 0.4774, 0.4078, 0.5065,  => 0.4725
recalls (micro): 0.4984, 0.4774, 0.4078, 0.5065,  => 0.4725
f1s (micro): 0.4984, 0.4774, 0.4078, 0.5065,  => 0.4725

precs (weighed): 0.5973, 0.5620, 0.4161, 0.7207,  => 0.5740
recalls (weighed): 0.4984, 0.4774, 0.4078, 0.5065,  => 0.4725
f1s (weighed): 0.5428, 0.5161, 0.4092, 0.5947,  => 0.5157

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.67it/s]
Evaluation, Loss: 137.6665,

accs: 0.7162, 0.6506, 0.6191, 0.8556,  => 0.7104

precs (macro): 0.2387, 0.6303, 0.3069, 0.2852,  => 0.3653
recalls (macro): 0.3333, 0.5870, 0.3797, 0.3333,  => 0.4083
f1s (macro): 0.2782, 0.5780, 0.3382, 0.3074,  => 0.3754

precs (micro): 0.7162, 0.6506, 0.6191, 0.8556,  => 0.7104
recalls (micro): 0.7162, 0.6506, 0.6191, 0.8556,  => 0.7104
f1s (micro): 0.7162, 0.6506, 0.6191, 0.8556,  => 0.7104

precs (weighed): 0.5129, 0.6378, 0.5120, 0.7321,  => 0.5987
recalls (weighed): 0.7162, 0.6506, 0.6191, 0.8556,  => 0.7104
f1s (weighed): 0.5977, 0.6173, 0.5585, 0.7890,  => 0.6407


Epoch 8/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:43<00:00,  6.13it/s]
Epoch 8/20, Loss: 1108.2097,

accs: 0.5020, 0.4742, 0.4235, 0.5088,  => 0.4771

precs (macro): 0.2485, 0.2689, 0.2644, 0.2123,  => 0.2485
recalls (macro): 0.2536, 0.2251, 0.2942, 0.1498,  => 0.2307
f1s (macro): 0.2321, 0.2450, 0.2684, 0.1757,  => 0.2303

precs (micro): 0.5020, 0.4742, 0.4235, 0.5088,  => 0.4771
recalls (micro): 0.5020, 0.4742, 0.4235, 0.5088,  => 0.4771
f1s (micro): 0.5020, 0.4742, 0.4235, 0.5088,  => 0.4771

precs (weighed): 0.5891, 0.5639, 0.4254, 0.7211,  => 0.5749
recalls (weighed): 0.5020, 0.4742, 0.4235, 0.5088,  => 0.4771
f1s (weighed): 0.5411, 0.5152, 0.4222, 0.5967,  => 0.5188

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.65it/s]
Evaluation, Loss: 131.0733,

accs: 0.7220, 0.6432, 0.6324, 0.8556,  => 0.7133

precs (macro): 0.5365, 0.6827, 0.3306, 0.2852,  => 0.4588
recalls (macro): 0.3410, 0.5483, 0.3698, 0.3333,  => 0.3981
f1s (macro): 0.2948, 0.4922, 0.3386, 0.3074,  => 0.3583

precs (micro): 0.7220, 0.6432, 0.6324, 0.8556,  => 0.7133
recalls (micro): 0.7220, 0.6432, 0.6324, 0.8556,  => 0.7133
f1s (micro): 0.7220, 0.6432, 0.6324, 0.8556,  => 0.7133

precs (weighed): 0.7603, 0.6724, 0.5298, 0.7321,  => 0.6737
recalls (weighed): 0.7220, 0.6432, 0.6324, 0.8556,  => 0.7133
f1s (weighed): 0.6126, 0.5545, 0.5604, 0.7890,  => 0.6291


Epoch 9/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:43<00:00,  6.13it/s]
Epoch 9/20, Loss: 1109.0934,

accs: 0.5005, 0.4836, 0.4129, 0.5053,  => 0.4756

precs (macro): 0.2528, 0.2715, 0.2575, 0.2125,  => 0.2486
recalls (macro): 0.2389, 0.2276, 0.2904, 0.2112,  => 0.2420
f1s (macro): 0.2335, 0.2474, 0.2627, 0.1751,  => 0.2297

precs (micro): 0.5005, 0.4836, 0.4129, 0.5053,  => 0.4756
recalls (micro): 0.5005, 0.4836, 0.4129, 0.5053,  => 0.4756
f1s (micro): 0.5005, 0.4836, 0.4129, 0.5053,  => 0.4756

precs (weighed): 0.5967, 0.5694, 0.4146, 0.7215,  => 0.5755
recalls (weighed): 0.5005, 0.4836, 0.4129, 0.5053,  => 0.4756
f1s (weighed): 0.5438, 0.5226, 0.4115, 0.5942,  => 0.5180

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.52it/s]
Evaluation, Loss: 131.7257,

accs: 0.7162, 0.6465, 0.6025, 0.8556,  => 0.7052

precs (macro): 0.2387, 0.6202, 0.3053, 0.2852,  => 0.3624
recalls (macro): 0.3333, 0.5966, 0.3758, 0.3333,  => 0.4098
f1s (macro): 0.2782, 0.5952, 0.3310, 0.3074,  => 0.3779

precs (micro): 0.7162, 0.6465, 0.6025, 0.8556,  => 0.7052
recalls (micro): 0.7162, 0.6465, 0.6025, 0.8556,  => 0.7052
f1s (micro): 0.7162, 0.6465, 0.6025, 0.8556,  => 0.7052

precs (weighed): 0.5129, 0.6323, 0.5147, 0.7321,  => 0.5980
recalls (weighed): 0.7162, 0.6465, 0.6025, 0.8556,  => 0.7052
f1s (weighed): 0.5977, 0.6276, 0.5460, 0.7890,  => 0.6401

Early stopping triggered


################# EVALUATING #################
vinai/phobert-base

[15:11:36] task: task-2                                                                                my_import.py:133
           model_type: simple                                                                          my_import.py:133
           model_name: vinai/phobert-base                                                              my_import.py:133
           padding_len: 256                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/simple_phobert-base                                            my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

model_weight_path: ./models/task_2/simple_phobert-base_19.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.29it/s]
Evaluation, Loss: 126.5763,

accs: 0.7527, 0.6606, 0.6880, 0.8556,  => 0.7392

precs (macro): 0.5378, 0.6503, 0.5595, 0.2852,  => 0.5082
recalls (macro): 0.3826, 0.5932, 0.4806, 0.3333,  => 0.4474
f1s (macro): 0.3725, 0.5822, 0.4963, 0.3074,  => 0.4396

precs (micro): 0.7527, 0.6606, 0.6880, 0.8556,  => 0.7392
recalls (micro): 0.7527, 0.6606, 0.6880, 0.8556,  => 0.7392
f1s (micro): 0.7527, 0.6606, 0.6880, 0.8556,  => 0.7392

precs (weighed): 0.7729, 0.6539, 0.6695, 0.7321,  => 0.7071
recalls (weighed): 0.7527, 0.6606, 0.6880, 0.8556,  => 0.7392
f1s (weighed): 0.6830, 0.6229, 0.6594, 0.7890,  => 0.6886

Confusion Matrix of title aspect
[[  0  11   0]
 [  0 855   8]
 [  0 279  52]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.75      0.99      0.85       863
           2       0.87      0.16      0.27       331

    accuracy                           0.75      1205
   macro avg       0.54      0.38      0.37      1205
weighted avg       0.77      0.75      0.68      1205

Confusion Matrix of desc aspect
[[659  79]
 [330 137]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.67      0.89      0.76       738
           2       0.63      0.29      0.40       467

    accuracy                           0.66      1205
   macro avg       0.65      0.59      0.58      1205
weighted avg       0.65      0.66      0.62      1205

Confusion Matrix of company aspect
[[  0  31  23   2]
 [  0 535  48   2]
 [  0 177 217   3]
 [  0  36  54  77]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.69      0.91      0.78       585
           2       0.63      0.55      0.59       397
           3       0.92      0.46      0.61       167

    accuracy                           0.69      1205
   macro avg       0.56      0.48      0.50      1205
weighted avg       0.67      0.69      0.66      1205

Confusion Matrix of other aspect
[[1031    0    0]
 [ 137    0    0]
 [  37    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.86      1.00      0.92      1031
           2       0.00      0.00      0.00       137
           3       0.00      0.00      0.00        37

    accuracy                           0.86      1205
   macro avg       0.29      0.33      0.31      1205
weighted avg       0.73      0.86      0.79      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:10<00:00,  7.32it/s]
Evaluation, Loss: 251.0488,

accs: 0.7425, 0.6841, 0.6907, 0.8520,  => 0.7423

precs (macro): 0.3845, 0.4534, 0.5542, 0.2840,  => 0.4190
recalls (macro): 0.2827, 0.4036, 0.4791, 0.3333,  => 0.3747
f1s (macro): 0.2723, 0.3984, 0.4976, 0.3067,  => 0.3688

precs (micro): 0.7425, 0.6841, 0.6907, 0.8520,  => 0.7423
recalls (micro): 0.7425, 0.6841, 0.6907, 0.8520,  => 0.7423
f1s (micro): 0.7425, 0.6841, 0.6907, 0.8520,  => 0.7423

precs (weighed): 0.7456, 0.6811, 0.6698, 0.7259,  => 0.7056
recalls (weighed): 0.7425, 0.6841, 0.6907, 0.8520,  => 0.7423
f1s (weighed): 0.6691, 0.6447, 0.6637, 0.7839,  => 0.6904

Confusion Matrix of title aspect
[[   0   28    1    0]
 [   0 1696   22    0]
 [   0  566   95    0]
 [   0    3    1    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.74      0.99      0.85      1718
           2       0.80      0.14      0.24       661
           3       0.00      0.00      0.00         4

    accuracy                           0.74      2412
   macro avg       0.38      0.28      0.27      2412
weighted avg       0.75      0.74      0.67      2412

Confusion Matrix of desc aspect
[[   0    2    0]
 [   0 1385  128]
 [   0  632  265]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.69      0.92      0.78      1513
           2       0.67      0.30      0.41       897

    accuracy                           0.68      2412
   macro avg       0.45      0.40      0.40      2412
weighted avg       0.68      0.68      0.64      2412

Confusion Matrix of company aspect
[[   0   58   37    4]
 [   0 1119  111    5]
 [   0  371  397   10]
 [   0   74   76  150]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.69      0.91      0.78      1235
           2       0.64      0.51      0.57       778
           3       0.89      0.50      0.64       300

    accuracy                           0.69      2412
   macro avg       0.55      0.48      0.50      2412
weighted avg       0.67      0.69      0.66      2412

Confusion Matrix of other aspect
[[2055    0    0]
 [ 262    0    0]
 [  95    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.85      1.00      0.92      2055
           2       0.00      0.00      0.00       262
           3       0.00      0.00      0.00        95

    accuracy                           0.85      2412
   macro avg       0.28      0.33      0.31      2412
weighted avg       0.73      0.85      0.78      2412

uitnlp/visobert

[15:12:13] task: task-2                                                                                my_import.py:133
           model_type: simple                                                                          my_import.py:133
           model_name: uitnlp/visobert                                                                 my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/simple_visobert                                                my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/visobert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

model_weight_path: ./models/task_2/simple_visobert_7.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:12<00:00,  3.04it/s]
Evaluation, Loss: 122.6390,

accs: 0.7361, 0.6905, 0.6689, 0.8556,  => 0.7378

precs (macro): 0.4952, 0.6722, 0.5245, 0.2852,  => 0.4942
recalls (macro): 0.3650, 0.6643, 0.4600, 0.3333,  => 0.4557
f1s (macro): 0.3442, 0.6670, 0.4664, 0.3074,  => 0.4462

precs (micro): 0.7361, 0.6905, 0.6689, 0.8556,  => 0.7378
recalls (micro): 0.7361, 0.6905, 0.6689, 0.8556,  => 0.7378
f1s (micro): 0.7361, 0.6905, 0.6689, 0.8556,  => 0.7378

precs (weighed): 0.7328, 0.6856, 0.6523, 0.7321,  => 0.7007
recalls (weighed): 0.7361, 0.6905, 0.6689, 0.8556,  => 0.7378
f1s (weighed): 0.6556, 0.6869, 0.6443, 0.7890,  => 0.6940

Confusion Matrix of title aspect
[[  0  11   0]
 [  0 851  12]
 [  0 295  36]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.74      0.99      0.84       863
           2       0.75      0.11      0.19       331

    accuracy                           0.74      1205
   macro avg       0.50      0.36      0.34      1205
weighted avg       0.73      0.74      0.66      1205

Confusion Matrix of desc aspect
[[576 162]
 [211 256]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.73      0.78      0.76       738
           2       0.61      0.55      0.58       467

    accuracy                           0.69      1205
   macro avg       0.67      0.66      0.67      1205
weighted avg       0.69      0.69      0.69      1205

Confusion Matrix of company aspect
[[  0  20  34   2]
 [  0 475 105   5]
 [  0 115 275   7]
 [  0  41  70  56]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.73      0.81      0.77       585
           2       0.57      0.69      0.62       397
           3       0.80      0.34      0.47       167

    accuracy                           0.67      1205
   macro avg       0.52      0.46      0.47      1205
weighted avg       0.65      0.67      0.64      1205

Confusion Matrix of other aspect
[[1031    0    0]
 [ 137    0    0]
 [  37    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.86      1.00      0.92      1031
           2       0.00      0.00      0.00       137
           3       0.00      0.00      0.00        37

    accuracy                           0.86      1205
   macro avg       0.29      0.33      0.31      1205
weighted avg       0.73      0.86      0.79      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:20<00:00,  3.74it/s]
Evaluation, Loss: 245.5742,

accs: 0.7318, 0.7094, 0.6700, 0.8520,  => 0.7408

precs (macro): 0.3696, 0.4585, 0.5371, 0.2840,  => 0.4123
recalls (macro): 0.2734, 0.4538, 0.4553, 0.3333,  => 0.3790
f1s (macro): 0.2569, 0.4556, 0.4675, 0.3067,  => 0.3717

precs (micro): 0.7318, 0.7094, 0.6700, 0.8520,  => 0.7408
recalls (micro): 0.7318, 0.7094, 0.6700, 0.8520,  => 0.7408
f1s (micro): 0.7318, 0.7094, 0.6700, 0.8520,  => 0.7408

precs (weighed): 0.7256, 0.7044, 0.6612, 0.7259,  => 0.7043
recalls (weighed): 0.7318, 0.7094, 0.6700, 0.8520,  => 0.7408
f1s (weighed): 0.6495, 0.7061, 0.6497, 0.7839,  => 0.6973

Confusion Matrix of title aspect
[[   0   29    0    0]
 [   0 1694   24    0]
 [   0  590   71    0]
 [   0    4    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.73      0.99      0.84      1718
           2       0.75      0.11      0.19       661
           3       0.00      0.00      0.00         4

    accuracy                           0.73      2412
   macro avg       0.37      0.27      0.26      2412
weighted avg       0.73      0.73      0.65      2412

Confusion Matrix of desc aspect
[[   0    1    1]
 [   0 1203  310]
 [   0  389  508]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.76      0.80      0.77      1513
           2       0.62      0.57      0.59       897

    accuracy                           0.71      2412
   macro avg       0.46      0.45      0.46      2412
weighted avg       0.70      0.71      0.71      2412

Confusion Matrix of company aspect
[[  0  33  64   2]
 [  0 990 242   3]
 [  0 245 521  12]
 [  0  83 112 105]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.73      0.80      0.77      1235
           2       0.55      0.67      0.61       778
           3       0.86      0.35      0.50       300

    accuracy                           0.67      2412
   macro avg       0.54      0.46      0.47      2412
weighted avg       0.66      0.67      0.65      2412

Confusion Matrix of other aspect
[[2055    0    0]
 [ 262    0    0]
 [  95    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.85      1.00      0.92      2055
           2       0.00      0.00      0.00       262
           3       0.00      0.00      0.00        95

    accuracy                           0.85      2412
   macro avg       0.28      0.33      0.31      2412
weighted avg       0.73      0.85      0.78      2412

uitnlp/CafeBERT

[15:13:07] task: task-2                                                                                my_import.py:133
           model_type: simple                                                                          my_import.py:133
           model_name: uitnlp/CafeBERT                                                                 my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/simple_CafeBERT                                                my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/CafeBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

model_weight_path: ./models/task_2/simple_CafeBERT_8.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:31<00:00,  1.21it/s]
Evaluation, Loss: 135.4086,

accs: 0.7162, 0.6257, 0.5361, 0.8556,  => 0.6834

precs (macro): 0.2387, 0.6453, 0.2566, 0.2852,  => 0.3565
recalls (macro): 0.3333, 0.5234, 0.2921, 0.3333,  => 0.3705
f1s (macro): 0.2782, 0.4431, 0.2448, 0.3074,  => 0.3184

precs (micro): 0.7162, 0.6257, 0.5361, 0.8556,  => 0.6834
recalls (micro): 0.7162, 0.6257, 0.5361, 0.8556,  => 0.6834
f1s (micro): 0.7162, 0.6257, 0.5361, 0.8556,  => 0.6834

precs (weighed): 0.5129, 0.6406, 0.4231, 0.7321,  => 0.5772
recalls (weighed): 0.7162, 0.6257, 0.5361, 0.8556,  => 0.6834
f1s (weighed): 0.5977, 0.5149, 0.4314, 0.7890,  => 0.5832

Confusion Matrix of title aspect
[[  0  11   0]
 [  0 863   0]
 [  0 331   0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.72      1.00      0.83       863
           2       0.00      0.00      0.00       331

    accuracy                           0.72      1205
   macro avg       0.24      0.33      0.28      1205
weighted avg       0.51      0.72      0.60      1205

Confusion Matrix of desc aspect
[[722  16]
 [435  32]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.62      0.98      0.76       738
           2       0.67      0.07      0.12       467

    accuracy                           0.63      1205
   macro avg       0.65      0.52      0.44      1205
weighted avg       0.64      0.63      0.51      1205

Confusion Matrix of company aspect
[[  0  41  15   0]
 [  0 567  18   0]
 [  0 318  79   0]
 [  0 115  52   0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.54      0.97      0.70       585
           2       0.48      0.20      0.28       397
           3       0.00      0.00      0.00       167

    accuracy                           0.54      1205
   macro avg       0.26      0.29      0.24      1205
weighted avg       0.42      0.54      0.43      1205

Confusion Matrix of other aspect
[[1031    0    0]
 [ 137    0    0]
 [  37    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.86      1.00      0.92      1031
           2       0.00      0.00      0.00       137
           3       0.00      0.00      0.00        37

    accuracy                           0.86      1205
   macro avg       0.29      0.33      0.31      1205
weighted avg       0.73      0.86      0.79      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [01:01<00:00,  1.23it/s]
Evaluation, Loss: 269.1761,

accs: 0.7123, 0.6443, 0.5614, 0.8520,  => 0.6925

precs (macro): 0.1781, 0.4434, 0.2732, 0.2840,  => 0.2947
recalls (macro): 0.2500, 0.3537, 0.2930, 0.3333,  => 0.3075
f1s (macro): 0.2080, 0.3081, 0.2522, 0.3067,  => 0.2688

precs (micro): 0.7123, 0.6443, 0.5614, 0.8520,  => 0.6925
recalls (micro): 0.7123, 0.6443, 0.5614, 0.8520,  => 0.6925
f1s (micro): 0.7123, 0.6443, 0.5614, 0.8520,  => 0.6925

precs (weighed): 0.5073, 0.6587, 0.4598, 0.7259,  => 0.5879
recalls (weighed): 0.7123, 0.6443, 0.5614, 0.8520,  => 0.6925
f1s (weighed): 0.5926, 0.5417, 0.4608, 0.7839,  => 0.5947

Confusion Matrix of title aspect
[[   0   29    0    0]
 [   0 1718    0    0]
 [   0  661    0    0]
 [   0    4    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.71      1.00      0.83      1718
           2       0.00      0.00      0.00       661
           3       0.00      0.00      0.00         4

    accuracy                           0.71      2412
   macro avg       0.18      0.25      0.21      2412
weighted avg       0.51      0.71      0.59      2412

Confusion Matrix of desc aspect
[[   0    2    0]
 [   0 1479   34]
 [   0  822   75]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.64      0.98      0.78      1513
           2       0.69      0.08      0.15       897

    accuracy                           0.64      2412
   macro avg       0.44      0.35      0.31      2412
weighted avg       0.66      0.64      0.54      2412

Confusion Matrix of company aspect
[[   0   85   14    0]
 [   0 1195   40    0]
 [   0  619  159    0]
 [   0  211   89    0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.57      0.97      0.71      1235
           2       0.53      0.20      0.29       778
           3       0.00      0.00      0.00       300

    accuracy                           0.56      2412
   macro avg       0.27      0.29      0.25      2412
weighted avg       0.46      0.56      0.46      2412

Confusion Matrix of other aspect
[[2055    0    0]
 [ 262    0    0]
 [  95    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.85      1.00      0.92      2055
           2       0.00      0.00      0.00       262
           3       0.00      0.00      0.00        95

    accuracy                           0.85      2412
   macro avg       0.28      0.33      0.31      2412
weighted avg       0.73      0.85      0.78      2412

xlm-roberta-base

[15:15:01] task: task-2                                                                                my_import.py:133
           model_type: simple                                                                          my_import.py:133
           model_name: xlm-roberta-base                                                                my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/simple_xlm-roberta-base                                        my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

model_weight_path: ./models/task_2/simple_xlm-roberta-base_1.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:12<00:00,  3.00it/s]
Evaluation, Loss: 141.4460,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423

Confusion Matrix of title aspect
[[  0  11   0]
 [  0 863   0]
 [  0 331   0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.72      1.00      0.83       863
           2       0.00      0.00      0.00       331

    accuracy                           0.72      1205
   macro avg       0.24      0.33      0.28      1205
weighted avg       0.51      0.72      0.60      1205

Confusion Matrix of desc aspect
[[738   0]
 [467   0]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.61      1.00      0.76       738
           2       0.00      0.00      0.00       467

    accuracy                           0.61      1205
   macro avg       0.31      0.50      0.38      1205
weighted avg       0.38      0.61      0.47      1205

Confusion Matrix of company aspect
[[  0  56   0   0]
 [  0 585   0   0]
 [  0 397   0   0]
 [  0 167   0   0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.49      1.00      0.65       585
           2       0.00      0.00      0.00       397
           3       0.00      0.00      0.00       167

    accuracy                           0.49      1205
   macro avg       0.12      0.25      0.16      1205
weighted avg       0.24      0.49      0.32      1205

Confusion Matrix of other aspect
[[1031    0    0]
 [ 137    0    0]
 [  37    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.86      1.00      0.92      1031
           2       0.00      0.00      0.00       137
           3       0.00      0.00      0.00        37

    accuracy                           0.86      1205
   macro avg       0.29      0.33      0.31      1205
weighted avg       0.73      0.86      0.79      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:20<00:00,  3.78it/s]
Evaluation, Loss: 282.1497,

accs: 0.7123, 0.6273, 0.5120, 0.8520,  => 0.6759

precs (macro): 0.1781, 0.2091, 0.1280, 0.2840,  => 0.1998
recalls (macro): 0.2500, 0.3333, 0.2500, 0.3333,  => 0.2917
f1s (macro): 0.2080, 0.2570, 0.1693, 0.3067,  => 0.2352

precs (micro): 0.7123, 0.6273, 0.5120, 0.8520,  => 0.6759
recalls (micro): 0.7123, 0.6273, 0.5120, 0.8520,  => 0.6759
f1s (micro): 0.7123, 0.6273, 0.5120, 0.8520,  => 0.6759

precs (weighed): 0.5073, 0.3935, 0.2622, 0.7259,  => 0.4722
recalls (weighed): 0.7123, 0.6273, 0.5120, 0.8520,  => 0.6759
f1s (weighed): 0.5926, 0.4836, 0.3468, 0.7839,  => 0.5517

Confusion Matrix of title aspect
[[   0   29    0    0]
 [   0 1718    0    0]
 [   0  661    0    0]
 [   0    4    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.71      1.00      0.83      1718
           2       0.00      0.00      0.00       661
           3       0.00      0.00      0.00         4

    accuracy                           0.71      2412
   macro avg       0.18      0.25      0.21      2412
weighted avg       0.51      0.71      0.59      2412

Confusion Matrix of desc aspect
[[   0    2    0]
 [   0 1513    0]
 [   0  897    0]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.63      1.00      0.77      1513
           2       0.00      0.00      0.00       897

    accuracy                           0.63      2412
   macro avg       0.21      0.33      0.26      2412
weighted avg       0.39      0.63      0.48      2412

Confusion Matrix of company aspect
[[   0   99    0    0]
 [   0 1235    0    0]
 [   0  778    0    0]
 [   0  300    0    0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.51      1.00      0.68      1235
           2       0.00      0.00      0.00       778
           3       0.00      0.00      0.00       300

    accuracy                           0.51      2412
   macro avg       0.13      0.25      0.17      2412
weighted avg       0.26      0.51      0.35      2412

Confusion Matrix of other aspect
[[2055    0    0]
 [ 262    0    0]
 [  95    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.85      1.00      0.92      2055
           2       0.00      0.00      0.00       262
           3       0.00      0.00      0.00        95

    accuracy                           0.85      2412
   macro avg       0.28      0.33      0.31      2412
weighted avg       0.73      0.85      0.78      2412

bert-base-multilingual-cased

[15:15:50] task: task-2                                                                                my_import.py:133
           model_type: simple                                                                          my_import.py:133
           model_name: bert-base-multilingual-cased                                                    my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/simple_bert-base-multilingual-cased                            my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

model_weight_path: ./models/task_2/simple_bert-base-multilingual-cased_1.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:11<00:00,  3.27it/s]
Evaluation, Loss: 143.4588,

accs: 0.7162, 0.5934, 0.4855, 0.8556,  => 0.6627

precs (macro): 0.2387, 0.5684, 0.1214, 0.2852,  => 0.3034
recalls (macro): 0.3333, 0.5670, 0.2500, 0.3333,  => 0.3709
f1s (macro): 0.2782, 0.5674, 0.1634, 0.3074,  => 0.3291

precs (micro): 0.7162, 0.5934, 0.4855, 0.8556,  => 0.6627
recalls (micro): 0.7162, 0.5934, 0.4855, 0.8556,  => 0.6627
f1s (micro): 0.7162, 0.5934, 0.4855, 0.8556,  => 0.6627

precs (weighed): 0.5129, 0.5896, 0.2357, 0.7321,  => 0.5176
recalls (weighed): 0.7162, 0.5934, 0.4855, 0.8556,  => 0.6627
f1s (weighed): 0.5977, 0.5913, 0.3173, 0.7890,  => 0.5738

Confusion Matrix of title aspect
[[  0  11   0]
 [  0 863   0]
 [  0 331   0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.72      1.00      0.83       863
           2       0.00      0.00      0.00       331

    accuracy                           0.72      1205
   macro avg       0.24      0.33      0.28      1205
weighted avg       0.51      0.72      0.60      1205

Confusion Matrix of desc aspect
[[505 233]
 [257 210]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.66      0.68      0.67       738
           2       0.47      0.45      0.46       467

    accuracy                           0.59      1205
   macro avg       0.57      0.57      0.57      1205
weighted avg       0.59      0.59      0.59      1205

Confusion Matrix of company aspect
[[  0  56   0   0]
 [  0 585   0   0]
 [  0 397   0   0]
 [  0 167   0   0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.49      1.00      0.65       585
           2       0.00      0.00      0.00       397
           3       0.00      0.00      0.00       167

    accuracy                           0.49      1205
   macro avg       0.12      0.25      0.16      1205
weighted avg       0.24      0.49      0.32      1205

Confusion Matrix of other aspect
[[1031    0    0]
 [ 137    0    0]
 [  37    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.86      1.00      0.92      1031
           2       0.00      0.00      0.00       137
           3       0.00      0.00      0.00        37

    accuracy                           0.86      1205
   macro avg       0.29      0.33      0.31      1205
weighted avg       0.73      0.86      0.79      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:19<00:00,  3.87it/s]
Evaluation, Loss: 284.9234,

accs: 0.7123, 0.6124, 0.5112, 0.8520,  => 0.6720

precs (macro): 0.1781, 0.3897, 0.1279, 0.2840,  => 0.2449
recalls (macro): 0.2500, 0.3897, 0.2496, 0.3333,  => 0.3057
f1s (macro): 0.2080, 0.3897, 0.1691, 0.3067,  => 0.2684

precs (micro): 0.7123, 0.6124, 0.5112, 0.8520,  => 0.6720
recalls (micro): 0.7123, 0.6124, 0.5112, 0.8520,  => 0.6720
f1s (micro): 0.7123, 0.6124, 0.5112, 0.8520,  => 0.6720

precs (weighed): 0.5073, 0.6108, 0.2620, 0.7259,  => 0.5265
recalls (weighed): 0.7123, 0.6124, 0.5112, 0.8520,  => 0.6720
f1s (weighed): 0.5926, 0.6116, 0.3464, 0.7839,  => 0.5836

Confusion Matrix of title aspect
[[   0   29    0    0]
 [   0 1718    0    0]
 [   0  661    0    0]
 [   0    4    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.71      1.00      0.83      1718
           2       0.00      0.00      0.00       661
           3       0.00      0.00      0.00         4

    accuracy                           0.71      2412
   macro avg       0.18      0.25      0.21      2412
weighted avg       0.51      0.71      0.59      2412

Confusion Matrix of desc aspect
[[   0    2    0]
 [   0 1052  461]
 [   0  472  425]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.69      0.70      0.69      1513
           2       0.48      0.47      0.48       897

    accuracy                           0.61      2412
   macro avg       0.39      0.39      0.39      2412
weighted avg       0.61      0.61      0.61      2412

Confusion Matrix of company aspect
[[   0   99    0    0]
 [   2 1233    0    0]
 [   0  778    0    0]
 [   0  300    0    0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.51      1.00      0.68      1235
           2       0.00      0.00      0.00       778
           3       0.00      0.00      0.00       300

    accuracy                           0.51      2412
   macro avg       0.13      0.25      0.17      2412
weighted avg       0.26      0.51      0.35      2412

Confusion Matrix of other aspect
[[2055    0    0]
 [ 262    0    0]
 [  95    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.85      1.00      0.92      2055
           2       0.00      0.00      0.00       262
           3       0.00      0.00      0.00        95

    accuracy                           0.85      2412
   macro avg       0.28      0.33      0.31      2412
weighted avg       0.73      0.85      0.78      2412

distilbert-base-multilingual-cased

[15:16:33] task: task-2                                                                                my_import.py:133
           model_type: simple                                                                          my_import.py:133
           model_name: distilbert-base-multilingual-cased                                              my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/simple_distilbert-base-multilingual-cased                      my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

model_weight_path: ./models/task_2/simple_distilbert-base-multilingual-cased_3.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:08<00:00,  4.37it/s]
Evaluation, Loss: 130.5261,

accs: 0.7228, 0.6498, 0.6282, 0.8556,  => 0.7141

precs (macro): 0.5404, 0.6851, 0.3079, 0.2852,  => 0.4547
recalls (macro): 0.3420, 0.5588, 0.3788, 0.3333,  => 0.4032
f1s (macro): 0.2968, 0.5128, 0.3394, 0.3074,  => 0.3641

precs (micro): 0.7228, 0.6498, 0.6282, 0.8556,  => 0.7141
recalls (micro): 0.7228, 0.6498, 0.6282, 0.8556,  => 0.7141
f1s (micro): 0.7228, 0.6498, 0.6282, 0.8556,  => 0.7141

precs (weighed): 0.7638, 0.6756, 0.5162, 0.7321,  => 0.6719
recalls (weighed): 0.7228, 0.6498, 0.6282, 0.8556,  => 0.7141
f1s (weighed): 0.6145, 0.5709, 0.5663, 0.7890,  => 0.6352

Confusion Matrix of title aspect
[[  0  11   0]
 [  0 862   1]
 [  0 322   9]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.72      1.00      0.84       863
           2       0.90      0.03      0.05       331

    accuracy                           0.72      1205
   macro avg       0.54      0.34      0.30      1205
weighted avg       0.76      0.72      0.61      1205

Confusion Matrix of desc aspect
[[711  27]
 [395  72]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.64      0.96      0.77       738
           2       0.73      0.15      0.25       467

    accuracy                           0.65      1205
   macro avg       0.69      0.56      0.51      1205
weighted avg       0.68      0.65      0.57      1205

Confusion Matrix of company aspect
[[  0  15  41   0]
 [  0 484 101   0]
 [  0 124 273   0]
 [  0  61 106   0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.71      0.83      0.76       585
           2       0.52      0.69      0.59       397
           3       0.00      0.00      0.00       167

    accuracy                           0.63      1205
   macro avg       0.31      0.38      0.34      1205
weighted avg       0.52      0.63      0.57      1205

Confusion Matrix of other aspect
[[1031    0    0]
 [ 137    0    0]
 [  37    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.86      1.00      0.92      1031
           2       0.00      0.00      0.00       137
           3       0.00      0.00      0.00        37

    accuracy                           0.86      1205
   macro avg       0.29      0.33      0.31      1205
weighted avg       0.73      0.86      0.79      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:12<00:00,  6.10it/s]
Evaluation, Loss: 259.7785,

accs: 0.7172, 0.6522, 0.6343, 0.8520,  => 0.7139

precs (macro): 0.3579, 0.4341, 0.3086, 0.2840,  => 0.3461
recalls (macro): 0.2564, 0.3662, 0.3705, 0.3333,  => 0.3316
f1s (macro): 0.2229, 0.3373, 0.3367, 0.3067,  => 0.3009

precs (micro): 0.7172, 0.6522, 0.6343, 0.8520,  => 0.7139
recalls (micro): 0.7172, 0.6522, 0.6343, 0.8520,  => 0.7139
f1s (micro): 0.7172, 0.6522, 0.6343, 0.8520,  => 0.7139

precs (weighed): 0.7066, 0.6509, 0.5308, 0.7259,  => 0.6536
recalls (weighed): 0.7172, 0.6522, 0.6343, 0.8520,  => 0.7139
f1s (weighed): 0.6098, 0.5741, 0.5779, 0.7839,  => 0.6364

Confusion Matrix of title aspect
[[   0   29    0    0]
 [   0 1710    8    0]
 [   0  641   20    0]
 [   0    4    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.72      1.00      0.83      1718
           2       0.71      0.03      0.06       661
           3       0.00      0.00      0.00         4

    accuracy                           0.72      2412
   macro avg       0.36      0.26      0.22      2412
weighted avg       0.71      0.72      0.61      2412

Confusion Matrix of desc aspect
[[   0    2    0]
 [   0 1443   70]
 [   0  767  130]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.65      0.95      0.77      1513
           2       0.65      0.14      0.24       897

    accuracy                           0.65      2412
   macro avg       0.43      0.37      0.34      2412
weighted avg       0.65      0.65      0.57      2412

Confusion Matrix of company aspect
[[   0   33   66    0]
 [   0 1019  216    0]
 [   0  267  511    0]
 [   0  136  164    0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.70      0.83      0.76      1235
           2       0.53      0.66      0.59       778
           3       0.00      0.00      0.00       300

    accuracy                           0.63      2412
   macro avg       0.31      0.37      0.34      2412
weighted avg       0.53      0.63      0.58      2412

Confusion Matrix of other aspect
[[2055    0    0]
 [ 262    0    0]
 [  95    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.85      1.00      0.92      2055
           2       0.00      0.00      0.00       262
           3       0.00      0.00      0.00        95

    accuracy                           0.85      2412
   macro avg       0.28      0.33      0.31      2412
weighted avg       0.73      0.85      0.78      2412

All commands completed!