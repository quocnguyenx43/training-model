Training CNN
cnn + vinai/phobert-base

[10:18:45] task: task-2                                                                                my_import.py:133
           model_type: cnn                                                                             my_import.py:133
           model_name: vinai/phobert-base                                                              my_import.py:133
           padding_len: 256                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           num_channels: 64                                                                            my_import.py:133
           kernel_size: 64                                                                             my_import.py:133
           padding: 64                                                                                 my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/cnn_phobert-base                                               my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

Training ...

Epoch 1/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 1/20, Loss: 1164.2026, 

accs: 0.4772, 0.4643, 0.3767, 0.5062,  => 0.4561

precs (macro): 0.2500, 0.2597, 0.2200, 0.2536,  => 0.2458
recalls (macro): 0.2750, 0.3006, 0.2481, 0.2771,  => 0.2752
f1s (macro): 0.2170, 0.2366, 0.2262, 0.1811,  => 0.2152

precs (micro): 0.4772, 0.4643, 0.3767, 0.5062,  => 0.4561
recalls (micro): 0.4772, 0.4643, 0.3767, 0.5062,  => 0.4561
f1s (micro): 0.4772, 0.4643, 0.3767, 0.5062,  => 0.4561

precs (weighed): 0.5916, 0.5463, 0.3588, 0.7255,  => 0.5555
recalls (weighed): 0.4772, 0.4643, 0.3767, 0.5062,  => 0.4561
f1s (weighed): 0.5238, 0.5014, 0.3649, 0.5947,  => 0.4962


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 140.1352, 

accs: 0.7162, 0.6257, 0.5660, 0.8556,  => 0.6909

precs (macro): 0.2387, 0.6236, 0.2743, 0.2852,  => 0.3554
recalls (macro): 0.3333, 0.5270, 0.3204, 0.3333,  => 0.3785
f1s (macro): 0.2782, 0.4568, 0.2842, 0.3074,  => 0.3316

precs (micro): 0.7162, 0.6257, 0.5660, 0.8556,  => 0.6909
recalls (micro): 0.7162, 0.6257, 0.5660, 0.8556,  => 0.6909
f1s (micro): 0.7162, 0.6257, 0.5660, 0.8556,  => 0.6909

precs (weighed): 0.5129, 0.6241, 0.4521, 0.7321,  => 0.5803
recalls (weighed): 0.7162, 0.6257, 0.5660, 0.8556,  => 0.6909
f1s (weighed): 0.5977, 0.5249, 0.4857, 0.7890,  => 0.5993

Saved the best model to path: ./models/task_2/cnn_phobert-base_0.pth


Epoch 2/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 2/20, Loss: 1133.2387, 

accs: 0.5098, 0.4656, 0.4052, 0.5050,  => 0.4714

precs (macro): 0.2533, 0.2651, 0.2516, 0.2123,  => 0.2456
recalls (macro): 0.2467, 0.3055, 0.2882, 0.1487,  => 0.2473
f1s (macro): 0.2357, 0.2420, 0.2583, 0.1749,  => 0.2277

precs (micro): 0.5098, 0.4656, 0.4052, 0.5050,  => 0.4714
recalls (micro): 0.5098, 0.4656, 0.4052, 0.5050,  => 0.4714
f1s (micro): 0.5098, 0.4656, 0.4052, 0.5050,  => 0.4714

precs (weighed): 0.5969, 0.5557, 0.4054, 0.7210,  => 0.5698
recalls (weighed): 0.5098, 0.4656, 0.4052, 0.5050,  => 0.4714
f1s (weighed): 0.5493, 0.5066, 0.4028, 0.5940,  => 0.5132


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 133.6406, 

accs: 0.7162, 0.6432, 0.6041, 0.8556,  => 0.7048

precs (macro): 0.2387, 0.7018, 0.3123, 0.2852,  => 0.3845
recalls (macro): 0.3333, 0.5459, 0.3773, 0.3333,  => 0.3975
f1s (macro): 0.2782, 0.4842, 0.3326, 0.3074,  => 0.3506

precs (micro): 0.7162, 0.6432, 0.6041, 0.8556,  => 0.7048
recalls (micro): 0.7162, 0.6432, 0.6041, 0.8556,  => 0.7048
f1s (micro): 0.7162, 0.6432, 0.6041, 0.8556,  => 0.7048

precs (weighed): 0.5129, 0.6869, 0.5321, 0.7321,  => 0.6160
recalls (weighed): 0.7162, 0.6432, 0.6041, 0.8556,  => 0.7048
f1s (weighed): 0.5977, 0.5486, 0.5516, 0.7890,  => 0.6217

Saved the best model to path: ./models/task_2/cnn_phobert-base_1.pth


Epoch 3/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 3/20, Loss: 1117.1273, 

accs: 0.4977, 0.4867, 0.4102, 0.5130,  => 0.4769

precs (macro): 0.2490, 0.2781, 0.2631, 0.2117,  => 0.2505
recalls (macro): 0.2375, 0.3162, 0.2902, 0.4009,  => 0.3112
f1s (macro): 0.2298, 0.2537, 0.2636, 0.1767,  => 0.2309

precs (micro): 0.4977, 0.4867, 0.4102, 0.5130,  => 0.4769
recalls (micro): 0.4977, 0.4867, 0.4102, 0.5130,  => 0.4769
f1s (micro): 0.4977, 0.4867, 0.4102, 0.5130,  => 0.4769

precs (weighed): 0.5909, 0.5806, 0.4273, 0.7179,  => 0.5792
recalls (weighed): 0.4977, 0.4867, 0.4102, 0.5130,  => 0.4769
f1s (weighed): 0.5397, 0.5294, 0.4157, 0.5981,  => 0.5207


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 134.5807, 

accs: 0.7162, 0.6448, 0.6224, 0.8556,  => 0.7098

precs (macro): 0.2387, 0.6790, 0.3143, 0.2852,  => 0.3793
recalls (macro): 0.3333, 0.5516, 0.3839, 0.3333,  => 0.4005
f1s (macro): 0.2782, 0.4997, 0.3408, 0.3074,  => 0.3565

precs (micro): 0.7162, 0.6448, 0.6224, 0.8556,  => 0.7098
recalls (micro): 0.7162, 0.6448, 0.6224, 0.8556,  => 0.7098
f1s (micro): 0.7162, 0.6448, 0.6224, 0.8556,  => 0.7098

precs (weighed): 0.5129, 0.6700, 0.5330, 0.7321,  => 0.6120
recalls (weighed): 0.7162, 0.6448, 0.6224, 0.8556,  => 0.7098
f1s (weighed): 0.5977, 0.5603, 0.5667, 0.7890,  => 0.6284



Epoch 4/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 4/20, Loss: 1113.8325, 

accs: 0.5016, 0.4813, 0.4115, 0.5159,  => 0.4776

precs (macro): 0.2549, 0.2750, 0.2647, 0.2117,  => 0.2516
recalls (macro): 0.2598, 0.2295, 0.2932, 0.2768,  => 0.2648
f1s (macro): 0.2367, 0.2502, 0.2652, 0.1771,  => 0.2323

precs (micro): 0.5016, 0.4813, 0.4115, 0.5159,  => 0.4776
recalls (micro): 0.5016, 0.4813, 0.4115, 0.5159,  => 0.4776
f1s (micro): 0.5016, 0.4813, 0.4115, 0.5159,  => 0.4776

precs (weighed): 0.5991, 0.5747, 0.4289, 0.7184,  => 0.5803
recalls (weighed): 0.5016, 0.4813, 0.4115, 0.5159,  => 0.4776
f1s (weighed): 0.5449, 0.5239, 0.4170, 0.6004,  => 0.5216


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 128.6749, 

accs: 0.7162, 0.6506, 0.6315, 0.8556,  => 0.7135

precs (macro): 0.2387, 0.6364, 0.3113, 0.2852,  => 0.3679
recalls (macro): 0.3333, 0.5795, 0.3833, 0.3333,  => 0.4074
f1s (macro): 0.2782, 0.5636, 0.3427, 0.3074,  => 0.3730

precs (micro): 0.7162, 0.6506, 0.6315, 0.8556,  => 0.7135
recalls (micro): 0.7162, 0.6506, 0.6315, 0.8556,  => 0.7135
f1s (micro): 0.7162, 0.6506, 0.6315, 0.8556,  => 0.7135

precs (weighed): 0.5129, 0.6412, 0.5230, 0.7321,  => 0.6023
recalls (weighed): 0.7162, 0.6506, 0.6315, 0.8556,  => 0.7135
f1s (weighed): 0.5977, 0.6075, 0.5708, 0.7890,  => 0.6412

Saved the best model to path: ./models/task_2/cnn_phobert-base_3.pth


Epoch 5/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 5/20, Loss: 1109.1895, 

accs: 0.4953, 0.4796, 0.4096, 0.5177,  => 0.4756

precs (macro): 0.2506, 0.2751, 0.2681, 0.2130,  => 0.2517
recalls (macro): 0.2482, 0.2299, 0.2985, 0.2774,  => 0.2635
f1s (macro): 0.2308, 0.2505, 0.2667, 0.1779,  => 0.2315

precs (micro): 0.4953, 0.4796, 0.4096, 0.5177,  => 0.4756
recalls (micro): 0.4953, 0.4796, 0.4096, 0.5177,  => 0.4756
f1s (micro): 0.4953, 0.4796, 0.4096, 0.5177,  => 0.4756

precs (weighed): 0.5922, 0.5750, 0.4363, 0.7229,  => 0.5816
recalls (weighed): 0.4953, 0.4796, 0.4096, 0.5177,  => 0.4756
f1s (weighed): 0.5386, 0.5230, 0.4185, 0.6032,  => 0.5208


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 133.7708, 

accs: 0.7145, 0.6523, 0.5817, 0.8556,  => 0.7010

precs (macro): 0.3499, 0.6349, 0.2961, 0.2852,  => 0.3915
recalls (macro): 0.3338, 0.5860, 0.3263, 0.3333,  => 0.3949
f1s (macro): 0.2817, 0.5749, 0.2884, 0.3074,  => 0.3631

precs (micro): 0.7145, 0.6523, 0.5817, 0.8556,  => 0.7010
recalls (micro): 0.7145, 0.6523, 0.5817, 0.8556,  => 0.7010
f1s (micro): 0.7145, 0.6523, 0.5817, 0.8556,  => 0.7010

precs (weighed): 0.6047, 0.6411, 0.4800, 0.7321,  => 0.6145
recalls (weighed): 0.7145, 0.6523, 0.5817, 0.8556,  => 0.7010
f1s (weighed): 0.6000, 0.6157, 0.4929, 0.7890,  => 0.6244



Epoch 6/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 6/20, Loss: 1103.3596, 

accs: 0.4997, 0.4841, 0.4062, 0.5152,  => 0.4763

precs (macro): 0.2514, 0.2783, 0.2738, 0.2133,  => 0.2542
recalls (macro): 0.2399, 0.2335, 0.3019, 0.2766,  => 0.2630
f1s (macro): 0.2321, 0.2539, 0.2687, 0.1775,  => 0.2331

precs (micro): 0.4997, 0.4841, 0.4062, 0.5152,  => 0.4763
recalls (micro): 0.4997, 0.4841, 0.4062, 0.5152,  => 0.4763
f1s (micro): 0.4997, 0.4841, 0.4062, 0.5152,  => 0.4763

precs (weighed): 0.5956, 0.5807, 0.4426, 0.7238,  => 0.5857
recalls (weighed): 0.4997, 0.4841, 0.4062, 0.5152,  => 0.4763
f1s (weighed): 0.5428, 0.5279, 0.4192, 0.6018,  => 0.5229


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 130.3297, 

accs: 0.7162, 0.6581, 0.6033, 0.8556,  => 0.7083

precs (macro): 0.2387, 0.6556, 0.3019, 0.2852,  => 0.3703
recalls (macro): 0.3333, 0.5837, 0.3435, 0.3333,  => 0.3984
f1s (macro): 0.2782, 0.5652, 0.3074, 0.3074,  => 0.3646

precs (micro): 0.7162, 0.6581, 0.6033, 0.8556,  => 0.7083
recalls (micro): 0.7162, 0.6581, 0.6033, 0.8556,  => 0.7083
f1s (micro): 0.7162, 0.6581, 0.6033, 0.8556,  => 0.7083

precs (weighed): 0.5129, 0.6564, 0.4919, 0.7321,  => 0.5983
recalls (weighed): 0.7162, 0.6581, 0.6033, 0.8556,  => 0.7083
f1s (weighed): 0.5977, 0.6104, 0.5210, 0.7890,  => 0.6295



Epoch 7/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 7/20, Loss: 1103.9277, 

accs: 0.4941, 0.4836, 0.4165, 0.5120,  => 0.4766

precs (macro): 0.2483, 0.2763, 0.2760, 0.2119,  => 0.2531
recalls (macro): 0.2383, 0.3148, 0.3005, 0.2757,  => 0.2823
f1s (macro): 0.2284, 0.2522, 0.2717, 0.1764,  => 0.2322

precs (micro): 0.4941, 0.4836, 0.4165, 0.5120,  => 0.4766
recalls (micro): 0.4941, 0.4836, 0.4165, 0.5120,  => 0.4766
f1s (micro): 0.4941, 0.4836, 0.4165, 0.5120,  => 0.4766

precs (weighed): 0.5913, 0.5772, 0.4476, 0.7193,  => 0.5839
recalls (weighed): 0.4941, 0.4836, 0.4165, 0.5120,  => 0.4766
f1s (weighed): 0.5377, 0.5262, 0.4279, 0.5981,  => 0.5225


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 134.9115, 

accs: 0.7170, 0.6523, 0.6274, 0.8556,  => 0.7131

precs (macro): 0.4392, 0.6665, 0.3122, 0.2852,  => 0.4258
recalls (macro): 0.3356, 0.5675, 0.3830, 0.3333,  => 0.4049
f1s (macro): 0.2842, 0.5335, 0.3417, 0.3074,  => 0.3667

precs (micro): 0.7170, 0.6523, 0.6274, 0.8556,  => 0.7131
recalls (micro): 0.7170, 0.6523, 0.6274, 0.8556,  => 0.7131
f1s (micro): 0.7170, 0.6523, 0.6274, 0.8556,  => 0.7131

precs (weighed): 0.6787, 0.6624, 0.5272, 0.7321,  => 0.6501
recalls (weighed): 0.7170, 0.6523, 0.6274, 0.8556,  => 0.7131
f1s (weighed): 0.6027, 0.5865, 0.5695, 0.7890,  => 0.6369



Epoch 8/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 8/20, Loss: 1102.1446, 

accs: 0.4911, 0.4850, 0.4150, 0.5157,  => 0.4767

precs (macro): 0.2551, 0.2770, 0.2758, 0.2119,  => 0.2549
recalls (macro): 0.2504, 0.2327, 0.2961, 0.2143,  => 0.2484
f1s (macro): 0.2338, 0.2529, 0.2701, 0.1770,  => 0.2335

precs (micro): 0.4911, 0.4850, 0.4150, 0.5157,  => 0.4767
recalls (micro): 0.4911, 0.4850, 0.4150, 0.5157,  => 0.4767
f1s (micro): 0.4911, 0.4850, 0.4150, 0.5157,  => 0.4767

precs (weighed): 0.5985, 0.5787, 0.4494, 0.7193,  => 0.5865
recalls (weighed): 0.4911, 0.4850, 0.4150, 0.5157,  => 0.4767
f1s (weighed): 0.5385, 0.5277, 0.4280, 0.6006,  => 0.5237


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 127.5520, 

accs: 0.7311, 0.6589, 0.6282, 0.8556,  => 0.7185

precs (macro): 0.5249, 0.6977, 0.3078, 0.2852,  => 0.4539
recalls (macro): 0.3539, 0.5714, 0.3698, 0.3333,  => 0.4071
f1s (macro): 0.3215, 0.5339, 0.3341, 0.3074,  => 0.3742

precs (micro): 0.7311, 0.6589, 0.6282, 0.8556,  => 0.7185
recalls (micro): 0.7311, 0.6589, 0.6282, 0.8556,  => 0.7185
f1s (micro): 0.7311, 0.6589, 0.6282, 0.8556,  => 0.7185

precs (weighed): 0.7542, 0.6870, 0.5086, 0.7321,  => 0.6705
recalls (weighed): 0.7311, 0.6589, 0.6282, 0.8556,  => 0.7185
f1s (weighed): 0.6364, 0.5882, 0.5592, 0.7890,  => 0.6432

Saved the best model to path: ./models/task_2/cnn_phobert-base_7.pth


Epoch 9/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 9/20, Loss: 1106.7273, 

accs: 0.4963, 0.4805, 0.4202, 0.5037,  => 0.4752

precs (macro): 0.2581, 0.2770, 0.2842, 0.2116,  => 0.2577
recalls (macro): 0.2679, 0.2306, 0.3055, 0.3357,  => 0.2849
f1s (macro): 0.2381, 0.2517, 0.2766, 0.1747,  => 0.2353

precs (micro): 0.4963, 0.4805, 0.4202, 0.5037,  => 0.4752
recalls (micro): 0.4963, 0.4805, 0.4202, 0.5037,  => 0.4752
f1s (micro): 0.4963, 0.4805, 0.4202, 0.5037,  => 0.4752

precs (weighed): 0.6027, 0.5782, 0.4604, 0.7179,  => 0.5898
recalls (weighed): 0.4963, 0.4805, 0.4202, 0.5037,  => 0.4752
f1s (weighed): 0.5429, 0.5248, 0.4354, 0.5918,  => 0.5237


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 126.2879, 

accs: 0.7228, 0.6714, 0.6066, 0.8556,  => 0.7141

precs (macro): 0.4744, 0.6628, 0.3124, 0.2852,  => 0.4337
recalls (macro): 0.3451, 0.6090, 0.3460, 0.3333,  => 0.4084
f1s (macro): 0.3056, 0.6033, 0.3121, 0.3074,  => 0.3821

precs (micro): 0.7228, 0.6714, 0.6066, 0.8556,  => 0.7141
recalls (micro): 0.7228, 0.6714, 0.6066, 0.8556,  => 0.7141
f1s (micro): 0.7228, 0.6714, 0.6066, 0.8556,  => 0.7141

precs (weighed): 0.7102, 0.6660, 0.5050, 0.7321,  => 0.6533
recalls (weighed): 0.7228, 0.6714, 0.6066, 0.8556,  => 0.7141
f1s (weighed): 0.6213, 0.6403, 0.5266, 0.7890,  => 0.6443

Saved the best model to path: ./models/task_2/cnn_phobert-base_8.pth


Epoch 10/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 10/20, Loss: 1106.4475, 

accs: 0.4890, 0.4835, 0.4090, 0.5063,  => 0.4720

precs (macro): 0.2540, 0.2775, 0.2794, 0.2117,  => 0.2556
recalls (macro): 0.2565, 0.3152, 0.3032, 0.1491,  => 0.2560
f1s (macro): 0.2325, 0.2529, 0.2717, 0.1749,  => 0.2330

precs (micro): 0.4890, 0.4835, 0.4090, 0.5063,  => 0.4720
recalls (micro): 0.4890, 0.4835, 0.4090, 0.5063,  => 0.4720
f1s (micro): 0.4890, 0.4835, 0.4090, 0.5063,  => 0.4720

precs (weighed): 0.5958, 0.5793, 0.4532, 0.7189,  => 0.5868
recalls (weighed): 0.4890, 0.4835, 0.4090, 0.5063,  => 0.4720
f1s (weighed): 0.5360, 0.5270, 0.4252, 0.5942,  => 0.5206


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 125.9645, 

accs: 0.7336, 0.6598, 0.6274, 0.8556,  => 0.7191

precs (macro): 0.5165, 0.6836, 0.3119, 0.2852,  => 0.4493
recalls (macro): 0.3582, 0.5760, 0.3656, 0.3333,  => 0.4083
f1s (macro): 0.3302, 0.5451, 0.3308, 0.3074,  => 0.3784

precs (micro): 0.7336, 0.6598, 0.6274, 0.8556,  => 0.7191
recalls (micro): 0.7336, 0.6598, 0.6274, 0.8556,  => 0.7191
f1s (micro): 0.7336, 0.6598, 0.6274, 0.8556,  => 0.7191

precs (weighed): 0.7484, 0.6767, 0.5098, 0.7321,  => 0.6668
recalls (weighed): 0.7336, 0.6598, 0.6274, 0.8556,  => 0.7191
f1s (weighed): 0.6440, 0.5964, 0.5536, 0.7890,  => 0.6458

Saved the best model to path: ./models/task_2/cnn_phobert-base_9.pth


Epoch 11/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 11/20, Loss: 1103.2274, 

accs: 0.4950, 0.4761, 0.4061, 0.5105,  => 0.4719

precs (macro): 0.2566, 0.2749, 0.2782, 0.2123,  => 0.2555
recalls (macro): 0.2395, 0.2281, 0.3035, 0.2752,  => 0.2616
f1s (macro): 0.2353, 0.2493, 0.2699, 0.1762,  => 0.2327

precs (micro): 0.4950, 0.4761, 0.4061, 0.5105,  => 0.4719
recalls (micro): 0.4950, 0.4761, 0.4061, 0.5105,  => 0.4719
f1s (micro): 0.4950, 0.4761, 0.4061, 0.5105,  => 0.4719

precs (weighed): 0.6009, 0.5751, 0.4510, 0.7204,  => 0.5869
recalls (weighed): 0.4950, 0.4761, 0.4061, 0.5105,  => 0.4719
f1s (weighed): 0.5421, 0.5209, 0.4226, 0.5974,  => 0.5207


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 132.1436, 

accs: 0.7237, 0.6523, 0.5817, 0.8556,  => 0.7033

precs (macro): 0.5076, 0.6720, 0.3349, 0.2852,  => 0.4499
recalls (macro): 0.3443, 0.5659, 0.3329, 0.3333,  => 0.3941
f1s (macro): 0.3024, 0.5293, 0.3013, 0.3074,  => 0.3601

precs (micro): 0.7237, 0.6523, 0.5817, 0.8556,  => 0.7033
recalls (micro): 0.7237, 0.6523, 0.5817, 0.8556,  => 0.7033
f1s (micro): 0.7237, 0.6523, 0.5817, 0.8556,  => 0.7033

precs (weighed): 0.7373, 0.6664, 0.5014, 0.7321,  => 0.6593
recalls (weighed): 0.7237, 0.6523, 0.5817, 0.8556,  => 0.7033
f1s (weighed): 0.6191, 0.5834, 0.4953, 0.7890,  => 0.6217



Epoch 12/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 12/20, Loss: 1089.5629, 

accs: 0.5067, 0.4884, 0.4106, 0.5125,  => 0.4796

precs (macro): 0.2612, 0.2852, 0.2832, 0.2132,  => 0.2607
recalls (macro): 0.2616, 0.2353, 0.2965, 0.2758,  => 0.2673
f1s (macro): 0.2417, 0.2578, 0.2716, 0.1769,  => 0.2370

precs (micro): 0.5067, 0.4884, 0.4106, 0.5125,  => 0.4796
recalls (micro): 0.5067, 0.4884, 0.4106, 0.5125,  => 0.4796
f1s (micro): 0.5067, 0.4884, 0.4106, 0.5125,  => 0.4796

precs (weighed): 0.6077, 0.5942, 0.4628, 0.7237,  => 0.5971
recalls (weighed): 0.5067, 0.4884, 0.4106, 0.5125,  => 0.4796
f1s (weighed): 0.5515, 0.5361, 0.4310, 0.5999,  => 0.5296


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 126.7307, 

accs: 0.7187, 0.6573, 0.6448, 0.8556,  => 0.7191

precs (macro): 0.5061, 0.6464, 0.3369, 0.2852,  => 0.4437
recalls (macro): 0.3370, 0.5881, 0.3877, 0.3333,  => 0.4115
f1s (macro): 0.2865, 0.5751, 0.3547, 0.3074,  => 0.3809

precs (micro): 0.7187, 0.6573, 0.6448, 0.8556,  => 0.7191
recalls (micro): 0.7187, 0.6573, 0.6448, 0.8556,  => 0.7191
f1s (micro): 0.7187, 0.6573, 0.6448, 0.8556,  => 0.7191

precs (weighed): 0.7342, 0.6501, 0.5310, 0.7321,  => 0.6619
recalls (weighed): 0.7187, 0.6573, 0.6448, 0.8556,  => 0.7191
f1s (weighed): 0.6050, 0.6171, 0.5804, 0.7890,  => 0.6479



Epoch 13/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 13/20, Loss: 1095.3167, 

accs: 0.4941, 0.4893, 0.4099, 0.5044,  => 0.4744

precs (macro): 0.2573, 0.2855, 0.2868, 0.2125,  => 0.2605
recalls (macro): 0.2511, 0.4021, 0.3099, 0.1485,  => 0.2779
f1s (macro): 0.2351, 0.2586, 0.2753, 0.1749,  => 0.2360

precs (micro): 0.4941, 0.4893, 0.4099, 0.5044,  => 0.4744
recalls (micro): 0.4941, 0.4893, 0.4099, 0.5044,  => 0.4744
f1s (micro): 0.4941, 0.4893, 0.4099, 0.5044,  => 0.4744

precs (weighed): 0.6015, 0.5937, 0.4649, 0.7219,  => 0.5955
recalls (weighed): 0.4941, 0.4893, 0.4099, 0.5044,  => 0.4744
f1s (weighed): 0.5417, 0.5363, 0.4302, 0.5939,  => 0.5255


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 128.3644, 

accs: 0.7394, 0.6672, 0.6340, 0.8556,  => 0.7241

precs (macro): 0.4789, 0.6474, 0.3154, 0.2143,  => 0.4140
recalls (macro): 0.3752, 0.6186, 0.3718, 0.2500,  => 0.4039
f1s (macro): 0.3636, 0.6192, 0.3379, 0.2308,  => 0.3879

precs (micro): 0.7394, 0.6672, 0.6340, 0.8556,  => 0.7241
recalls (micro): 0.7394, 0.6672, 0.6340, 0.8556,  => 0.7241
f1s (micro): 0.7394, 0.6672, 0.6340, 0.8556,  => 0.7241

precs (weighed): 0.7224, 0.6566, 0.5182, 0.7333,  => 0.6576
recalls (weighed): 0.7394, 0.6672, 0.6340, 0.8556,  => 0.7241
f1s (weighed): 0.6717, 0.6496, 0.5651, 0.7897,  => 0.6690



Epoch 14/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 14/20, Loss: 1090.7813, 

accs: 0.5170, 0.4756, 0.4160, 0.5069,  => 0.4789

precs (macro): 0.2708, 0.2777, 0.2867, 0.2119,  => 0.2618
recalls (macro): 0.2807, 0.3110, 0.3119, 0.2742,  => 0.2944
f1s (macro): 0.2511, 0.2505, 0.2772, 0.1753,  => 0.2385

precs (micro): 0.5170, 0.4756, 0.4160, 0.5069,  => 0.4789
recalls (micro): 0.5170, 0.4756, 0.4160, 0.5069,  => 0.4789
f1s (micro): 0.5170, 0.4756, 0.4160, 0.5069,  => 0.4789

precs (weighed): 0.6238, 0.5800, 0.4641, 0.7192,  => 0.5968
recalls (weighed): 0.5170, 0.4756, 0.4160, 0.5069,  => 0.4789
f1s (weighed): 0.5639, 0.5226, 0.4338, 0.5945,  => 0.5287


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 129.6217, 

accs: 0.7361, 0.6664, 0.6415, 0.8556,  => 0.7249

precs (macro): 0.4562, 0.6504, 0.3157, 0.2852,  => 0.4269
recalls (macro): 0.3805, 0.6093, 0.3813, 0.3333,  => 0.4261
f1s (macro): 0.3743, 0.6062, 0.3449, 0.3074,  => 0.4082

precs (micro): 0.7361, 0.6664, 0.6415, 0.8556,  => 0.7249
recalls (micro): 0.7361, 0.6664, 0.6415, 0.8556,  => 0.7249
f1s (micro): 0.7361, 0.6664, 0.6415, 0.8556,  => 0.7249

precs (weighed): 0.7054, 0.6569, 0.5237, 0.7321,  => 0.6545
recalls (weighed): 0.7361, 0.6664, 0.6415, 0.8556,  => 0.7249
f1s (weighed): 0.6787, 0.6408, 0.5759, 0.7890,  => 0.6711



Epoch 15/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 15/20, Loss: 1092.1088, 

accs: 0.4940, 0.4880, 0.4118, 0.5207,  => 0.4786

precs (macro): 0.2551, 0.2854, 0.2833, 0.2126,  => 0.2591
recalls (macro): 0.2543, 0.2352, 0.3024, 0.3407,  => 0.2831
f1s (macro): 0.2347, 0.2579, 0.2736, 0.1785,  => 0.2362

precs (micro): 0.4940, 0.4880, 0.4118, 0.5207,  => 0.4786
recalls (micro): 0.4940, 0.4880, 0.4118, 0.5207,  => 0.4786
f1s (micro): 0.4940, 0.4880, 0.4118, 0.5207,  => 0.4786

precs (weighed): 0.5984, 0.5950, 0.4618, 0.7214,  => 0.5942
recalls (weighed): 0.4940, 0.4880, 0.4118, 0.5207,  => 0.4786
f1s (weighed): 0.5402, 0.5361, 0.4304, 0.6046,  => 0.5278


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 127.7405, 

accs: 0.7353, 0.6838, 0.6307, 0.8556,  => 0.7263

precs (macro): 0.5094, 0.6723, 0.3274, 0.2852,  => 0.4486
recalls (macro): 0.3615, 0.6314, 0.3731, 0.3333,  => 0.4248
f1s (macro): 0.3368, 0.6320, 0.3433, 0.3074,  => 0.4049

precs (micro): 0.7353, 0.6838, 0.6307, 0.8556,  => 0.7263
recalls (micro): 0.7353, 0.6838, 0.6307, 0.8556,  => 0.7263
f1s (micro): 0.7353, 0.6838, 0.6307, 0.8556,  => 0.7263

precs (weighed): 0.7435, 0.6772, 0.5230, 0.7321,  => 0.6689
recalls (weighed): 0.7353, 0.6838, 0.6307, 0.8556,  => 0.7263
f1s (weighed): 0.6496, 0.6631, 0.5652, 0.7890,  => 0.6667

Early stopping triggered
cnn + uitnlp/visobert

[10:30:21] task: task-2                                                                                my_import.py:133
           model_type: cnn                                                                             my_import.py:133
           model_name: uitnlp/visobert                                                                 my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           num_channels: 64                                                                            my_import.py:133
           kernel_size: 64                                                                             my_import.py:133
           padding: 64                                                                                 my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/cnn_visobert                                                   my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/visobert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Training ...

Epoch 1/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 1/20, Loss: 1145.9146, 

accs: 0.5037, 0.4608, 0.3997, 0.5087,  => 0.4682

precs (macro): 0.2528, 0.2584, 0.2493, 0.2130,  => 0.2434
recalls (macro): 0.2632, 0.2155, 0.2767, 0.2122,  => 0.2419
f1s (macro): 0.2351, 0.2347, 0.2519, 0.1760,  => 0.2244

precs (micro): 0.5037, 0.4608, 0.3997, 0.5087,  => 0.4682
recalls (micro): 0.5037, 0.4608, 0.3997, 0.5087,  => 0.4682
f1s (micro): 0.5037, 0.4608, 0.3997, 0.5087,  => 0.4682

precs (weighed): 0.5939, 0.5437, 0.4018, 0.7234,  => 0.5657
recalls (weighed): 0.5037, 0.4608, 0.3997, 0.5087,  => 0.4682
f1s (weighed): 0.5440, 0.4982, 0.3982, 0.5973,  => 0.5094


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 128.2039, 

accs: 0.7162, 0.6647, 0.6124, 0.8556,  => 0.7122

precs (macro): 0.2387, 0.6557, 0.3016, 0.2852,  => 0.3703
recalls (macro): 0.3333, 0.5989, 0.3718, 0.3333,  => 0.4094
f1s (macro): 0.2782, 0.5898, 0.3326, 0.3074,  => 0.3770

precs (micro): 0.7162, 0.6647, 0.6124, 0.8556,  => 0.7122
recalls (micro): 0.7162, 0.6647, 0.6124, 0.8556,  => 0.7122
f1s (micro): 0.7162, 0.6647, 0.6124, 0.8556,  => 0.7122

precs (weighed): 0.5129, 0.6589, 0.5036, 0.7321,  => 0.6019
recalls (weighed): 0.7162, 0.6647, 0.6124, 0.8556,  => 0.7122
f1s (weighed): 0.5977, 0.6292, 0.5520, 0.7890,  => 0.6420

Saved the best model to path: ./models/task_2/cnn_visobert_0.pth


Epoch 2/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 2/20, Loss: 1112.9227, 

accs: 0.5011, 0.4732, 0.4129, 0.5259,  => 0.4783

precs (macro): 0.2526, 0.2677, 0.2673, 0.2134,  => 0.2503
recalls (macro): 0.2429, 0.2254, 0.2949, 0.2173,  => 0.2451
f1s (macro): 0.2330, 0.2447, 0.2671, 0.1796,  => 0.2311

precs (micro): 0.5011, 0.4732, 0.4129, 0.5259,  => 0.4783
recalls (micro): 0.5011, 0.4732, 0.4129, 0.5259,  => 0.4783
f1s (micro): 0.5011, 0.4732, 0.4129, 0.5259,  => 0.4783

precs (weighed): 0.5960, 0.5609, 0.4314, 0.7246,  => 0.5782
recalls (weighed): 0.5011, 0.4732, 0.4129, 0.5259,  => 0.4783
f1s (weighed): 0.5438, 0.5133, 0.4190, 0.6094,  => 0.5214


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 127.7018, 

accs: 0.7228, 0.6465, 0.6216, 0.8556,  => 0.7116

precs (macro): 0.5183, 0.6315, 0.3070, 0.2852,  => 0.4355
recalls (macro): 0.3426, 0.6347, 0.3668, 0.3333,  => 0.4194
f1s (macro): 0.2986, 0.6325, 0.3318, 0.3074,  => 0.3926

precs (micro): 0.7228, 0.6465, 0.6216, 0.8556,  => 0.7116
recalls (micro): 0.7228, 0.6465, 0.6216, 0.8556,  => 0.7116
f1s (micro): 0.7228, 0.6465, 0.6216, 0.8556,  => 0.7116

precs (weighed): 0.7458, 0.6519, 0.5037, 0.7321,  => 0.6584
recalls (weighed): 0.7228, 0.6465, 0.6216, 0.8556,  => 0.7116
f1s (weighed): 0.6159, 0.6486, 0.5527, 0.7890,  => 0.6515

Saved the best model to path: ./models/task_2/cnn_visobert_1.pth


Epoch 3/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 3/20, Loss: 1100.8945, 

accs: 0.4995, 0.4803, 0.4180, 0.5089,  => 0.4767

precs (macro): 0.2575, 0.2800, 0.2789, 0.2111,  => 0.2569
recalls (macro): 0.2682, 0.3142, 0.3001, 0.3372,  => 0.3049
f1s (macro): 0.2366, 0.2533, 0.2735, 0.1756,  => 0.2348

precs (micro): 0.4995, 0.4803, 0.4180, 0.5089,  => 0.4767
recalls (micro): 0.4995, 0.4803, 0.4180, 0.5089,  => 0.4767
f1s (micro): 0.4995, 0.4803, 0.4180, 0.5089,  => 0.4767

precs (weighed): 0.6023, 0.5836, 0.4516, 0.7164,  => 0.5885
recalls (weighed): 0.4995, 0.4803, 0.4180, 0.5089,  => 0.4767
f1s (weighed): 0.5449, 0.5268, 0.4307, 0.5949,  => 0.5243


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 125.9571, 

accs: 0.7203, 0.6913, 0.6232, 0.8556,  => 0.7226

precs (macro): 0.4366, 0.6824, 0.3085, 0.2852,  => 0.4282
recalls (macro): 0.3458, 0.6395, 0.3804, 0.3333,  => 0.4248
f1s (macro): 0.3097, 0.6410, 0.3394, 0.3074,  => 0.3994

precs (micro): 0.7203, 0.6913, 0.6232, 0.8556,  => 0.7226
recalls (micro): 0.7203, 0.6913, 0.6232, 0.8556,  => 0.7226
f1s (micro): 0.7203, 0.6913, 0.6232, 0.8556,  => 0.7226

precs (weighed): 0.6793, 0.6862, 0.5176, 0.7321,  => 0.6538
recalls (weighed): 0.7203, 0.6913, 0.6232, 0.8556,  => 0.7226
f1s (weighed): 0.6238, 0.6712, 0.5634, 0.7890,  => 0.6619

Saved the best model to path: ./models/task_2/cnn_visobert_2.pth


Epoch 4/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 4/20, Loss: 1096.4085, 

accs: 0.5029, 0.4713, 0.4050, 0.5185,  => 0.4744

precs (macro): 0.2607, 0.2743, 0.2793, 0.2126,  => 0.2567
recalls (macro): 0.2610, 0.3098, 0.3030, 0.3401,  => 0.3035
f1s (macro): 0.2396, 0.2483, 0.2699, 0.1780,  => 0.2339

precs (micro): 0.5029, 0.4713, 0.4050, 0.5185,  => 0.4744
recalls (micro): 0.5029, 0.4713, 0.4050, 0.5185,  => 0.4744
f1s (micro): 0.5029, 0.4713, 0.4050, 0.5185,  => 0.4744

precs (weighed): 0.6081, 0.5739, 0.4526, 0.7213,  => 0.5890
recalls (weighed): 0.5029, 0.4713, 0.4050, 0.5185,  => 0.4744
f1s (weighed): 0.5495, 0.5174, 0.4226, 0.6031,  => 0.5232


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 128.1560, 

accs: 0.7369, 0.6531, 0.5867, 0.8556,  => 0.7081

precs (macro): 0.4483, 0.6693, 0.4073, 0.2852,  => 0.4525
recalls (macro): 0.3920, 0.5682, 0.3577, 0.3333,  => 0.4128
f1s (macro): 0.3921, 0.5341, 0.3421, 0.3074,  => 0.3939

precs (micro): 0.7369, 0.6531, 0.5867, 0.8556,  => 0.7081
recalls (micro): 0.7369, 0.6531, 0.5867, 0.8556,  => 0.7081
f1s (micro): 0.7369, 0.6531, 0.5867, 0.8556,  => 0.7081

precs (weighed): 0.7029, 0.6647, 0.5128, 0.7321,  => 0.6531
recalls (weighed): 0.7369, 0.6531, 0.5867, 0.8556,  => 0.7081
f1s (weighed): 0.6925, 0.5871, 0.5137, 0.7890,  => 0.6456



Epoch 5/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 5/20, Loss: 1101.8730, 

accs: 0.4896, 0.4816, 0.4112, 0.5041,  => 0.4716

precs (macro): 0.2607, 0.2784, 0.2870, 0.2117,  => 0.2594
recalls (macro): 0.2636, 0.2305, 0.3105, 0.3358,  => 0.2851
f1s (macro): 0.2362, 0.2522, 0.2757, 0.1748,  => 0.2347

precs (micro): 0.4896, 0.4816, 0.4112, 0.5041,  => 0.4716
recalls (micro): 0.4896, 0.4816, 0.4112, 0.5041,  => 0.4716
f1s (micro): 0.4896, 0.4816, 0.4112, 0.5041,  => 0.4716

precs (weighed): 0.6062, 0.5808, 0.4627, 0.7184,  => 0.5920
recalls (weighed): 0.4896, 0.4816, 0.4112, 0.5041,  => 0.4716
f1s (weighed): 0.5405, 0.5266, 0.4303, 0.5922,  => 0.5224


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 128.1405, 

accs: 0.7245, 0.6697, 0.6365, 0.8556,  => 0.7216

precs (macro): 0.4777, 0.6874, 0.3294, 0.2852,  => 0.4449
recalls (macro): 0.3477, 0.5924, 0.3820, 0.3333,  => 0.4139
f1s (macro): 0.3111, 0.5723, 0.3515, 0.3074,  => 0.3856

precs (micro): 0.7245, 0.6697, 0.6365, 0.8556,  => 0.7216
recalls (micro): 0.7245, 0.6697, 0.6365, 0.8556,  => 0.7216
f1s (micro): 0.7245, 0.6697, 0.6365, 0.8556,  => 0.7216

precs (weighed): 0.7137, 0.6819, 0.5298, 0.7321,  => 0.6644
recalls (weighed): 0.7245, 0.6697, 0.6365, 0.8556,  => 0.7216
f1s (weighed): 0.6262, 0.6182, 0.5760, 0.7890,  => 0.6524



Epoch 6/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 6/20, Loss: 1088.2932, 

accs: 0.4989, 0.4926, 0.4152, 0.5008,  => 0.4769

precs (macro): 0.2622, 0.2851, 0.2907, 0.2124,  => 0.2626
recalls (macro): 0.2651, 0.4032, 0.3075, 0.2724,  => 0.3120
f1s (macro): 0.2409, 0.2591, 0.2779, 0.1743,  => 0.2380

precs (micro): 0.4989, 0.4926, 0.4152, 0.5008,  => 0.4769
recalls (micro): 0.4989, 0.4926, 0.4152, 0.5008,  => 0.4769
f1s (micro): 0.4989, 0.4926, 0.4152, 0.5008,  => 0.4769

precs (weighed): 0.6098, 0.5929, 0.4712, 0.7211,  => 0.5987
recalls (weighed): 0.4989, 0.4926, 0.4152, 0.5008,  => 0.4769
f1s (weighed): 0.5475, 0.5380, 0.4366, 0.5909,  => 0.5282


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 121.0347, 

accs: 0.7311, 0.6913, 0.6398, 0.8556,  => 0.7295

precs (macro): 0.4735, 0.6954, 0.3310, 0.2852,  => 0.4463
recalls (macro): 0.3608, 0.6292, 0.3837, 0.3333,  => 0.4268
f1s (macro): 0.3379, 0.6261, 0.3533, 0.3074,  => 0.4062

precs (micro): 0.7311, 0.6913, 0.6398, 0.8556,  => 0.7295
recalls (micro): 0.7311, 0.6913, 0.6398, 0.8556,  => 0.7295
f1s (micro): 0.7311, 0.6913, 0.6398, 0.8556,  => 0.7295

precs (weighed): 0.7138, 0.6939, 0.5332, 0.7321,  => 0.6682
recalls (weighed): 0.7311, 0.6913, 0.6398, 0.8556,  => 0.7295
f1s (weighed): 0.6491, 0.6612, 0.5794, 0.7890,  => 0.6697

Saved the best model to path: ./models/task_2/cnn_visobert_5.pth


Epoch 7/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 7/20, Loss: 1086.8054, 

accs: 0.4996, 0.4732, 0.4268, 0.5093,  => 0.4772

precs (macro): 0.2659, 0.2801, 0.2998, 0.2120,  => 0.2644
recalls (macro): 0.2635, 0.2286, 0.3243, 0.1500,  => 0.2416
f1s (macro): 0.2429, 0.2517, 0.2878, 0.1756,  => 0.2395

precs (micro): 0.4996, 0.4732, 0.4268, 0.5093,  => 0.4772
recalls (micro): 0.4996, 0.4732, 0.4268, 0.5093,  => 0.4772
f1s (micro): 0.4996, 0.4732, 0.4268, 0.5093,  => 0.4772

precs (weighed): 0.6151, 0.5834, 0.4844, 0.7199,  => 0.6007
recalls (weighed): 0.4996, 0.4732, 0.4268, 0.5093,  => 0.4772
f1s (weighed): 0.5501, 0.5224, 0.4480, 0.5966,  => 0.5293


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 125.5006, 

accs: 0.7452, 0.6996, 0.6365, 0.8556,  => 0.7342

precs (macro): 0.4721, 0.6828, 0.3317, 0.2141,  => 0.4252
recalls (macro): 0.3897, 0.6675, 0.3883, 0.2500,  => 0.4239
f1s (macro): 0.3871, 0.6714, 0.3576, 0.2306,  => 0.4117

precs (micro): 0.7452, 0.6996, 0.6365, 0.8556,  => 0.7342
recalls (micro): 0.7452, 0.6996, 0.6365, 0.8556,  => 0.7342
f1s (micro): 0.7452, 0.6996, 0.6365, 0.8556,  => 0.7342

precs (weighed): 0.7215, 0.6933, 0.5463, 0.7327,  => 0.6734
recalls (weighed): 0.7452, 0.6996, 0.6365, 0.8556,  => 0.7342
f1s (weighed): 0.6917, 0.6930, 0.5878, 0.7894,  => 0.6905



Epoch 8/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 8/20, Loss: 1089.7690, 

accs: 0.5024, 0.4805, 0.4118, 0.5071,  => 0.4754

precs (macro): 0.2692, 0.2860, 0.2931, 0.2133,  => 0.2654
recalls (macro): 0.2458, 0.3158, 0.3102, 0.2742,  => 0.2865
f1s (macro): 0.2441, 0.2567, 0.2781, 0.1758,  => 0.2387

precs (micro): 0.5024, 0.4805, 0.4118, 0.5071,  => 0.4754
recalls (micro): 0.5024, 0.4805, 0.4118, 0.5071,  => 0.4754
f1s (micro): 0.5024, 0.4805, 0.4118, 0.5071,  => 0.4754

precs (weighed): 0.6197, 0.5931, 0.4758, 0.7240,  => 0.6032
recalls (weighed): 0.5024, 0.4805, 0.4118, 0.5071,  => 0.4754
f1s (weighed): 0.5543, 0.5308, 0.4359, 0.5962,  => 0.5293


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 125.6891, 

accs: 0.7419, 0.6880, 0.6158, 0.8556,  => 0.7253

precs (macro): 0.5041, 0.6699, 0.3259, 0.2141,  => 0.4285
recalls (macro): 0.3726, 0.6654, 0.3839, 0.2500,  => 0.4180
f1s (macro): 0.3574, 0.6672, 0.3463, 0.2306,  => 0.4004

precs (micro): 0.7419, 0.6880, 0.6158, 0.8556,  => 0.7253
recalls (micro): 0.7419, 0.6880, 0.6158, 0.8556,  => 0.7253
f1s (micro): 0.7419, 0.6880, 0.6158, 0.8556,  => 0.7253

precs (weighed): 0.7423, 0.6847, 0.5317, 0.7327,  => 0.6728
recalls (weighed): 0.7419, 0.6880, 0.6158, 0.8556,  => 0.7253
f1s (weighed): 0.6677, 0.6859, 0.5644, 0.7894,  => 0.6768



Epoch 9/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 9/20, Loss: 1085.4770, 

accs: 0.4988, 0.4807, 0.4125, 0.5085,  => 0.4751

precs (macro): 0.2653, 0.2817, 0.2894, 0.2152,  => 0.2629
recalls (macro): 0.2800, 0.2329, 0.3100, 0.1497,  => 0.2432
f1s (macro): 0.2437, 0.2549, 0.2771, 0.1766,  => 0.2381

precs (micro): 0.4988, 0.4807, 0.4125, 0.5085,  => 0.4751
recalls (micro): 0.4988, 0.4807, 0.4125, 0.5085,  => 0.4751
f1s (micro): 0.4988, 0.4807, 0.4125, 0.5085,  => 0.4751

precs (weighed): 0.6110, 0.5868, 0.4690, 0.7309,  => 0.5994
recalls (weighed): 0.4988, 0.4807, 0.4125, 0.5085,  => 0.4751
f1s (weighed): 0.5476, 0.5283, 0.4337, 0.5997,  => 0.5273


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 124.4008, 

accs: 0.7328, 0.6888, 0.6390, 0.8548,  => 0.7288

precs (macro): 0.5380, 0.6938, 0.3532, 0.2140,  => 0.4497
recalls (macro): 0.3553, 0.6252, 0.3855, 0.2498,  => 0.4040
f1s (macro): 0.3237, 0.6209, 0.3533, 0.2305,  => 0.3821

precs (micro): 0.7328, 0.6888, 0.6390, 0.8548,  => 0.7288
recalls (micro): 0.7328, 0.6888, 0.6390, 0.8548,  => 0.7288
f1s (micro): 0.7328, 0.6888, 0.6390, 0.8548,  => 0.7288

precs (weighed): 0.7654, 0.6920, 0.5278, 0.7326,  => 0.6794
recalls (weighed): 0.7328, 0.6888, 0.6390, 0.8548,  => 0.7288
f1s (weighed): 0.6386, 0.6570, 0.5737, 0.7890,  => 0.6646



Epoch 10/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 10/20, Loss: 1085.2127, 

accs: 0.4984, 0.4861, 0.4131, 0.5049,  => 0.4756

precs (macro): 0.2673, 0.2868, 0.2916, 0.2135,  => 0.2648
recalls (macro): 0.2606, 0.2346, 0.3068, 0.2111,  => 0.2533
f1s (macro): 0.2431, 0.2581, 0.2769, 0.1754,  => 0.2384

precs (micro): 0.4984, 0.4861, 0.4131, 0.5049,  => 0.4756
recalls (micro): 0.4984, 0.4861, 0.4131, 0.5049,  => 0.4756
f1s (micro): 0.4984, 0.4861, 0.4131, 0.5049,  => 0.4756

precs (weighed): 0.6165, 0.5966, 0.4733, 0.7249,  => 0.6028
recalls (weighed): 0.4984, 0.4861, 0.4131, 0.5049,  => 0.4756
f1s (weighed): 0.5501, 0.5356, 0.4361, 0.5951,  => 0.5292


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 126.8856, 

accs: 0.7427, 0.7062, 0.6299, 0.8548,  => 0.7334

precs (macro): 0.4753, 0.6901, 0.3394, 0.2140,  => 0.4297
recalls (macro): 0.3836, 0.6760, 0.3917, 0.2498,  => 0.4253
f1s (macro): 0.3777, 0.6800, 0.3627, 0.2305,  => 0.4127

precs (micro): 0.7427, 0.7062, 0.6299, 0.8548,  => 0.7334
recalls (micro): 0.7427, 0.7062, 0.6299, 0.8548,  => 0.7334
f1s (micro): 0.7427, 0.7062, 0.6299, 0.8548,  => 0.7334

precs (weighed): 0.7219, 0.7005, 0.5588, 0.7326,  => 0.6784
recalls (weighed): 0.7427, 0.7062, 0.6299, 0.8548,  => 0.7334
f1s (weighed): 0.6835, 0.7006, 0.5908, 0.7890,  => 0.6910



Epoch 11/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 11/20, Loss: 1077.9833, 

accs: 0.5091, 0.4876, 0.4106, 0.5069,  => 0.4785

precs (macro): 0.2713, 0.2900, 0.2967, 0.2139,  => 0.2680
recalls (macro): 0.2668, 0.4031, 0.3198, 0.1493,  => 0.2847
f1s (macro): 0.2471, 0.2609, 0.2799, 0.1758,  => 0.2409

precs (micro): 0.5091, 0.4876, 0.4106, 0.5069,  => 0.4785
recalls (micro): 0.5091, 0.4876, 0.4106, 0.5069,  => 0.4785
f1s (micro): 0.5091, 0.4876, 0.4106, 0.5069,  => 0.4785

precs (weighed): 0.6245, 0.6012, 0.4807, 0.7265,  => 0.6082
recalls (weighed): 0.5091, 0.4876, 0.4106, 0.5069,  => 0.4785
f1s (weighed): 0.5598, 0.5382, 0.4362, 0.5972,  => 0.5329


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 121.5891, 

accs: 0.7303, 0.7037, 0.6357, 0.8556,  => 0.7313

precs (macro): 0.5144, 0.6872, 0.3438, 0.2852,  => 0.4576
recalls (macro): 0.3536, 0.6740, 0.3824, 0.3333,  => 0.4358
f1s (macro): 0.3212, 0.6778, 0.3506, 0.3074,  => 0.4143

precs (micro): 0.7303, 0.7037, 0.6357, 0.8556,  => 0.7313
recalls (micro): 0.7303, 0.7037, 0.6357, 0.8556,  => 0.7313
f1s (micro): 0.7303, 0.7037, 0.6357, 0.8556,  => 0.7313

precs (weighed): 0.7455, 0.6980, 0.5241, 0.7321,  => 0.6749
recalls (weighed): 0.7303, 0.7037, 0.6357, 0.8556,  => 0.7313
f1s (weighed): 0.6359, 0.6983, 0.5706, 0.7890,  => 0.6735

Early stopping triggered
uitnlp/CafeBERT

[12:43:58] task: task-2                                                                                my_import.py:133
           model_type: cnn                                                                             my_import.py:133
           model_name: uitnlp/CafeBERT                                                                 my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           num_channels: 64                                                                            my_import.py:133
           kernel_size: 64                                                                             my_import.py:133
           padding: 64                                                                                 my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/cnn_CafeBERT                                                   my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/CafeBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Training ...
Epoch 1/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [04:03<00:00,  1.08it/s]
Epoch 1/20, Loss: 1163.8611,

accs: 0.4960, 0.4719, 0.3750, 0.5079,  => 0.4627

precs (macro): 0.2505, 0.2605, 0.2199, 0.2102,  => 0.2353
recalls (macro): 0.2429, 0.2189, 0.2523, 0.2745,  => 0.2471
f1s (macro): 0.2306, 0.2371, 0.2271, 0.1750,  => 0.2174

precs (micro): 0.4960, 0.4719, 0.3750, 0.5079,  => 0.4627
recalls (micro): 0.4960, 0.4719, 0.3750, 0.5079,  => 0.4627
f1s (micro): 0.4960, 0.4719, 0.3750, 0.5079,  => 0.4627

precs (weighed): 0.5926, 0.5475, 0.3557, 0.7135,  => 0.5523
recalls (weighed): 0.4960, 0.4719, 0.3750, 0.5079,  => 0.4627
f1s (weighed): 0.5393, 0.5054, 0.3622, 0.5932,  => 0.5000

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.23it/s]
Evaluation, Loss: 143.6015,

accs: 0.7162, 0.6149, 0.5228, 0.8556,  => 0.6774

precs (macro): 0.2387, 0.6018, 0.2639, 0.2852,  => 0.3474
recalls (macro): 0.3333, 0.5060, 0.3196, 0.3333,  => 0.3731
f1s (macro): 0.2782, 0.4002, 0.2854, 0.3074,  => 0.3178

precs (micro): 0.7162, 0.6149, 0.5228, 0.8556,  => 0.6774
recalls (micro): 0.7162, 0.6149, 0.5228, 0.8556,  => 0.6774
f1s (micro): 0.7162, 0.6149, 0.5228, 0.8556,  => 0.6774

precs (weighed): 0.5129, 0.6048, 0.4505, 0.7321,  => 0.5751
recalls (weighed): 0.7162, 0.6149, 0.5228, 0.8556,  => 0.6774
f1s (weighed): 0.5977, 0.4809, 0.4782, 0.7890,  => 0.5865

Saved the best model to path: ./models/task_2/cnn_CafeBERT_0.pth

Epoch 2/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [03:53<00:00,  1.13it/s]
Epoch 2/20, Loss: 1144.7840,

accs: 0.5027, 0.4561, 0.3987, 0.5067,  => 0.4660

precs (macro): 0.2563, 0.2556, 0.2407, 0.2121,  => 0.2412
recalls (macro): 0.2541, 0.2138, 0.2770, 0.2741,  => 0.2548
f1s (macro): 0.2368, 0.2327, 0.2499, 0.1754,  => 0.2237

precs (micro): 0.5027, 0.4561, 0.3987, 0.5067,  => 0.4660
recalls (micro): 0.5027, 0.4561, 0.3987, 0.5067,  => 0.4660
f1s (micro): 0.5027, 0.4561, 0.3987, 0.5067,  => 0.4660

precs (weighed): 0.6002, 0.5383, 0.3889, 0.7200,  => 0.5619
recalls (weighed): 0.5027, 0.4561, 0.3987, 0.5067,  => 0.4660
f1s (weighed): 0.5462, 0.4934, 0.3919, 0.5947,  => 0.5066

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 137.6588,

accs: 0.7336, 0.6216, 0.5079, 0.8556,  => 0.6797

precs (macro): 0.4504, 0.5851, 0.2247, 0.2852,  => 0.3863
recalls (macro): 0.3787, 0.5393, 0.2713, 0.3333,  => 0.3806
f1s (macro): 0.3720, 0.5038, 0.2154, 0.3074,  => 0.3497

precs (micro): 0.7336, 0.6216, 0.5079, 0.8556,  => 0.6797
recalls (micro): 0.7336, 0.6216, 0.5079, 0.8556,  => 0.6797
f1s (micro): 0.7336, 0.6216, 0.5079, 0.8556,  => 0.6797

precs (weighed): 0.7001, 0.5960, 0.3778, 0.7321,  => 0.6015
recalls (weighed): 0.7336, 0.6216, 0.5079, 0.8556,  => 0.6797
f1s (weighed): 0.6761, 0.5582, 0.3897, 0.7890,  => 0.6033

Saved the best model to path: ./models/task_2/cnn_CafeBERT_1.pth

Epoch 3/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [03:53<00:00,  1.13it/s]
Epoch 3/20, Loss: 1125.5566,

accs: 0.5015, 0.4659, 0.4020, 0.5150,  => 0.4711

precs (macro): 0.2622, 0.2624, 0.2462, 0.2123,  => 0.2458
recalls (macro): 0.2502, 0.2189, 0.2806, 0.2141,  => 0.2410
f1s (macro): 0.2395, 0.2385, 0.2536, 0.1770,  => 0.2272

precs (micro): 0.5015, 0.4659, 0.4020, 0.5150,  => 0.4711
recalls (micro): 0.5015, 0.4659, 0.4020, 0.5150,  => 0.4711
f1s (micro): 0.5015, 0.4659, 0.4020, 0.5150,  => 0.4711

precs (weighed): 0.6111, 0.5522, 0.3998, 0.7208,  => 0.5710
recalls (weighed): 0.5015, 0.4659, 0.4020, 0.5150,  => 0.4711
f1s (weighed): 0.5501, 0.5052, 0.3985, 0.6007,  => 0.5136

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.23it/s]
Evaluation, Loss: 136.6002,

accs: 0.7220, 0.6340, 0.5535, 0.8556,  => 0.6913

precs (macro): 0.5131, 0.6088, 0.2776, 0.2852,  => 0.4212
recalls (macro): 0.3416, 0.5577, 0.3375, 0.3333,  => 0.3925
f1s (macro): 0.2966, 0.5332, 0.3015, 0.3074,  => 0.3597

precs (micro): 0.7220, 0.6340, 0.5535, 0.8556,  => 0.6913
recalls (micro): 0.7220, 0.6340, 0.5535, 0.8556,  => 0.6913
f1s (micro): 0.7220, 0.6340, 0.5535, 0.8556,  => 0.6913

precs (weighed): 0.7412, 0.6169, 0.4728, 0.7321,  => 0.6407
recalls (weighed): 0.7220, 0.6340, 0.5535, 0.8556,  => 0.6913
f1s (weighed): 0.6140, 0.5820, 0.5052, 0.7890,  => 0.6226

Saved the best model to path: ./models/task_2/cnn_CafeBERT_2.pth

Epoch 4/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [03:53<00:00,  1.13it/s]
Epoch 4/20, Loss: 1127.8428,

accs: 0.4977, 0.4785, 0.4018, 0.5009,  => 0.4697

precs (macro): 0.2636, 0.2676, 0.2467, 0.2104,  => 0.2471
recalls (macro): 0.2525, 0.2260, 0.2820, 0.2724,  => 0.2582
f1s (macro): 0.2399, 0.2449, 0.2540, 0.1736,  => 0.2281

precs (micro): 0.4977, 0.4785, 0.4018, 0.5009,  => 0.4697
recalls (micro): 0.4977, 0.4785, 0.4018, 0.5009,  => 0.4697
f1s (micro): 0.4977, 0.4785, 0.4018, 0.5009,  => 0.4697

precs (weighed): 0.6126, 0.5619, 0.3992, 0.7142,  => 0.5720
recalls (weighed): 0.4977, 0.4785, 0.4018, 0.5009,  => 0.4697
f1s (weighed): 0.5482, 0.5167, 0.3983, 0.5887,  => 0.5130

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 134.2174,

accs: 0.7187, 0.6349, 0.5602, 0.8556,  => 0.6923

precs (macro): 0.5061, 0.6100, 0.2867, 0.2852,  => 0.4220
recalls (macro): 0.3370, 0.5592, 0.3451, 0.3333,  => 0.3937
f1s (macro): 0.2865, 0.5356, 0.3071, 0.3074,  => 0.3592

precs (micro): 0.7187, 0.6349, 0.5602, 0.8556,  => 0.6923
recalls (micro): 0.7187, 0.6349, 0.5602, 0.8556,  => 0.6923
f1s (micro): 0.7187, 0.6349, 0.5602, 0.8556,  => 0.6923

precs (weighed): 0.7342, 0.6180, 0.4903, 0.7321,  => 0.6437
recalls (weighed): 0.7187, 0.6349, 0.5602, 0.8556,  => 0.6923
f1s (weighed): 0.6050, 0.5839, 0.5136, 0.7890,  => 0.6229

Saved the best model to path: ./models/task_2/cnn_CafeBERT_3.pth

Epoch 5/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [03:53<00:00,  1.13it/s]
Epoch 5/20, Loss: 1125.6920,

accs: 0.4926, 0.4716, 0.3981, 0.5127,  => 0.4688

precs (macro): 0.2597, 0.2680, 0.2486, 0.2120,  => 0.2471
recalls (macro): 0.2535, 0.3070, 0.2781, 0.2134,  => 0.2630
f1s (macro): 0.2373, 0.2441, 0.2525, 0.1765,  => 0.2276

precs (micro): 0.4926, 0.4716, 0.3981, 0.5127,  => 0.4688
recalls (micro): 0.4926, 0.4716, 0.3981, 0.5127,  => 0.4688
f1s (micro): 0.4926, 0.4716, 0.3981, 0.5127,  => 0.4688

precs (weighed): 0.6058, 0.5611, 0.4058, 0.7199,  => 0.5732
recalls (weighed): 0.4926, 0.4716, 0.3981, 0.5127,  => 0.4688
f1s (weighed): 0.5422, 0.5123, 0.3993, 0.5988,  => 0.5132

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 137.4314,

accs: 0.7544, 0.6382, 0.5054, 0.8556,  => 0.6884

precs (macro): 0.4623, 0.6142, 0.2272, 0.2852,  => 0.3973
recalls (macro): 0.4374, 0.5658, 0.2688, 0.3333,  => 0.4013
f1s (macro): 0.4439, 0.5468, 0.2101, 0.3074,  => 0.3771

precs (micro): 0.7544, 0.6382, 0.5054, 0.8556,  => 0.6884
recalls (micro): 0.7544, 0.6382, 0.5054, 0.8556,  => 0.6884
f1s (micro): 0.7544, 0.6382, 0.5054, 0.8556,  => 0.6884

precs (weighed): 0.7311, 0.6222, 0.3800, 0.7321,  => 0.6164
recalls (weighed): 0.7544, 0.6382, 0.5054, 0.8556,  => 0.6884
f1s (weighed): 0.7365, 0.5926, 0.3820, 0.7890,  => 0.6250


Epoch 6/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [03:52<00:00,  1.13it/s]
Epoch 6/20, Loss: 1125.1846,

accs: 0.4831, 0.4780, 0.4078, 0.5041,  => 0.4683

precs (macro): 0.2582, 0.2710, 0.2549, 0.2121,  => 0.2491
recalls (macro): 0.2410, 0.3928, 0.2915, 0.2733,  => 0.2997
f1s (macro): 0.2321, 0.2470, 0.2605, 0.1748,  => 0.2286

precs (micro): 0.4831, 0.4780, 0.4078, 0.5041,  => 0.4683
recalls (micro): 0.4831, 0.4780, 0.4078, 0.5041,  => 0.4683
f1s (micro): 0.4831, 0.4780, 0.4078, 0.5041,  => 0.4683

precs (weighed): 0.6031, 0.5665, 0.4127, 0.7199,  => 0.5756
recalls (weighed): 0.4831, 0.4780, 0.4078, 0.5041,  => 0.4683
f1s (weighed): 0.5357, 0.5182, 0.4074, 0.5928,  => 0.5135

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:31<00:00,  1.22it/s]
Evaluation, Loss: 136.8794,

accs: 0.7170, 0.6174, 0.5734, 0.8556,  => 0.6909

precs (macro): 0.5723, 0.6124, 0.2796, 0.2852,  => 0.4374
recalls (macro): 0.3343, 0.5107, 0.3431, 0.3333,  => 0.3804
f1s (macro): 0.2804, 0.4140, 0.3078, 0.3074,  => 0.3274

precs (micro): 0.7170, 0.6174, 0.5734, 0.8556,  => 0.6909
recalls (micro): 0.7170, 0.6174, 0.5734, 0.8556,  => 0.6909
f1s (micro): 0.7170, 0.6174, 0.5734, 0.8556,  => 0.6909

precs (weighed): 0.7880, 0.6136, 0.4726, 0.7321,  => 0.6516
recalls (weighed): 0.7170, 0.6174, 0.5734, 0.8556,  => 0.6909
f1s (weighed): 0.5997, 0.4916, 0.5177, 0.7890,  => 0.5995


Epoch 7/20, Batch 71/264:  27%|██████████████▌                                        | 70/264 [01:02<02:50,  1.14it/s]Epoch 7/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [03:53<00:00,  1.13it/s]
Epoch 7/20, Loss: 1127.8650,

accs: 0.4939, 0.4671, 0.4019, 0.4977,  => 0.4652

precs (macro): 0.2711, 0.2655, 0.2521, 0.2116,  => 0.2501
recalls (macro): 0.2821, 0.2228, 0.2744, 0.2090,  => 0.2471
f1s (macro): 0.2437, 0.2423, 0.2536, 0.1732,  => 0.2282

precs (micro): 0.4939, 0.4671, 0.4019, 0.4977,  => 0.4652
recalls (micro): 0.4939, 0.4671, 0.4019, 0.4977,  => 0.4652
f1s (micro): 0.4939, 0.4671, 0.4019, 0.4977,  => 0.4652

precs (weighed): 0.6205, 0.5564, 0.4151, 0.7184,  => 0.5776
recalls (weighed): 0.4939, 0.4671, 0.4019, 0.4977,  => 0.4652
f1s (weighed): 0.5484, 0.5079, 0.4059, 0.5879,  => 0.5125

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 136.3783,

accs: 0.7187, 0.6390, 0.5867, 0.8556,  => 0.7000

precs (macro): 0.4619, 0.6108, 0.2815, 0.2852,  => 0.4099
recalls (macro): 0.3382, 0.5799, 0.3448, 0.3333,  => 0.3991
f1s (macro): 0.2902, 0.5729, 0.3096, 0.3074,  => 0.3700

precs (micro): 0.7187, 0.6390, 0.5867, 0.8556,  => 0.7000
recalls (micro): 0.7187, 0.6390, 0.5867, 0.8556,  => 0.7000
f1s (micro): 0.7187, 0.6390, 0.5867, 0.8556,  => 0.7000

precs (weighed): 0.6981, 0.6221, 0.4733, 0.7321,  => 0.6314
recalls (weighed): 0.7187, 0.6390, 0.5867, 0.8556,  => 0.7000
f1s (weighed): 0.6080, 0.6107, 0.5235, 0.7890,  => 0.6328


Epoch 8/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [03:52<00:00,  1.13it/s]
Epoch 8/20, Loss: 1116.0356,

accs: 0.4989, 0.4850, 0.3994, 0.5151,  => 0.4746

precs (macro): 0.2707, 0.2758, 0.2513, 0.2124,  => 0.2525
recalls (macro): 0.2579, 0.2304, 0.2829, 0.1517,  => 0.2307
f1s (macro): 0.2443, 0.2510, 0.2555, 0.1770,  => 0.2319

precs (micro): 0.4989, 0.4850, 0.3994, 0.5151,  => 0.4746
recalls (micro): 0.4989, 0.4850, 0.3994, 0.5151,  => 0.4746
f1s (micro): 0.4989, 0.4850, 0.3994, 0.5151,  => 0.4746

precs (weighed): 0.6209, 0.5763, 0.4088, 0.7213,  => 0.5818
recalls (weighed): 0.4989, 0.4850, 0.3994, 0.5151,  => 0.4746
f1s (weighed): 0.5523, 0.5266, 0.4003, 0.6010,  => 0.5200

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 134.8249,

accs: 0.7286, 0.6232, 0.5369, 0.8556,  => 0.6861

precs (macro): 0.4741, 0.6367, 0.2529, 0.2852,  => 0.4122
recalls (macro): 0.3559, 0.5198, 0.2949, 0.3333,  => 0.3760
f1s (macro): 0.3283, 0.4354, 0.2511, 0.3074,  => 0.3305

precs (micro): 0.7286, 0.6232, 0.5369, 0.8556,  => 0.6861
recalls (micro): 0.7286, 0.6232, 0.5369, 0.8556,  => 0.6861
f1s (micro): 0.7286, 0.6232, 0.5369, 0.8556,  => 0.6861

precs (weighed): 0.7129, 0.6334, 0.4195, 0.7321,  => 0.6245
recalls (weighed): 0.7286, 0.6232, 0.5369, 0.8556,  => 0.6861
f1s (weighed): 0.6409, 0.5087, 0.4399, 0.7890,  => 0.5946


Epoch 9/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [03:52<00:00,  1.13it/s]
Epoch 9/20, Loss: 1120.4836,

accs: 0.4886, 0.4708, 0.4083, 0.5153,  => 0.4708

precs (macro): 0.2643, 0.2662, 0.2535, 0.2123,  => 0.2491
recalls (macro): 0.2470, 0.3065, 0.2850, 0.3391,  => 0.2944
f1s (macro): 0.2382, 0.2430, 0.2583, 0.1773,  => 0.2292

precs (micro): 0.4886, 0.4708, 0.4083, 0.5153,  => 0.4708
recalls (micro): 0.4886, 0.4708, 0.4083, 0.5153,  => 0.4708
f1s (micro): 0.4886, 0.4708, 0.4083, 0.5153,  => 0.4708

precs (weighed): 0.6118, 0.5575, 0.4131, 0.7202,  => 0.5756
recalls (weighed): 0.4886, 0.4708, 0.4083, 0.5153,  => 0.4708
f1s (weighed): 0.5423, 0.5103, 0.4083, 0.6005,  => 0.5154

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 133.8506,

accs: 0.7162, 0.6498, 0.5593, 0.8556,  => 0.6952

precs (macro): 0.2387, 0.6572, 0.2659, 0.2852,  => 0.3617
recalls (macro): 0.3333, 0.5659, 0.3137, 0.3333,  => 0.3866
f1s (macro): 0.2782, 0.5328, 0.2753, 0.3074,  => 0.3484

precs (micro): 0.7162, 0.6498, 0.5593, 0.8556,  => 0.6952
recalls (micro): 0.7162, 0.6498, 0.5593, 0.8556,  => 0.6952
f1s (micro): 0.7162, 0.6498, 0.5593, 0.8556,  => 0.6952

precs (weighed): 0.5129, 0.6550, 0.4410, 0.7321,  => 0.5852
recalls (weighed): 0.7162, 0.6498, 0.5593, 0.8556,  => 0.6952
f1s (weighed): 0.5977, 0.5854, 0.4746, 0.7890,  => 0.6117

Saved the best model to path: ./models/task_2/cnn_CafeBERT_8.pth

Epoch 10/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [03:52<00:00,  1.13it/s]
Epoch 10/20, Loss: 1120.1174,

accs: 0.4909, 0.4690, 0.4014, 0.5164,  => 0.4695

precs (macro): 0.2697, 0.2666, 0.2536, 0.2134,  => 0.2508
recalls (macro): 0.2505, 0.2221, 0.2867, 0.2770,  => 0.2591
f1s (macro): 0.2408, 0.2423, 0.2571, 0.1778,  => 0.2295

precs (micro): 0.4909, 0.4690, 0.4014, 0.5164,  => 0.4695
recalls (micro): 0.4909, 0.4690, 0.4014, 0.5164,  => 0.4695
f1s (micro): 0.4909, 0.4690, 0.4014, 0.5164,  => 0.4695

precs (weighed): 0.6188, 0.5600, 0.4133, 0.7242,  => 0.5791
recalls (weighed): 0.4909, 0.4690, 0.4014, 0.5164,  => 0.4695
f1s (weighed): 0.5466, 0.5104, 0.4039, 0.6028,  => 0.5159

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 134.3113,

accs: 0.7228, 0.6448, 0.5768, 0.8556,  => 0.7000

precs (macro): 0.4744, 0.6354, 0.2737, 0.2852,  => 0.4172
recalls (macro): 0.3451, 0.5653, 0.3334, 0.3333,  => 0.3943
f1s (macro): 0.3056, 0.5382, 0.2983, 0.3074,  => 0.3624

precs (micro): 0.7228, 0.6448, 0.5768, 0.8556,  => 0.7000
recalls (micro): 0.7228, 0.6448, 0.5768, 0.8556,  => 0.7000
f1s (micro): 0.7228, 0.6448, 0.5768, 0.8556,  => 0.7000

precs (weighed): 0.7102, 0.6383, 0.4589, 0.7321,  => 0.6349
recalls (weighed): 0.7228, 0.6448, 0.5768, 0.8556,  => 0.7000
f1s (weighed): 0.6213, 0.5881, 0.5077, 0.7890,  => 0.6265


Epoch 11/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [03:51<00:00,  1.14it/s]
Epoch 11/20, Loss: 1116.6777,

accs: 0.5010, 0.4716, 0.4100, 0.5029,  => 0.4714

precs (macro): 0.2692, 0.2698, 0.2602, 0.2120,  => 0.2528
recalls (macro): 0.2563, 0.2254, 0.2931, 0.3979,  => 0.2932
f1s (macro): 0.2443, 0.2456, 0.2633, 0.1747,  => 0.2320

precs (micro): 0.5010, 0.4716, 0.4100, 0.5029,  => 0.4714
recalls (micro): 0.5010, 0.4716, 0.4100, 0.5029,  => 0.4714
f1s (micro): 0.5010, 0.4716, 0.4100, 0.5029,  => 0.4714

precs (weighed): 0.6201, 0.5648, 0.4234, 0.7190,  => 0.5818
recalls (weighed): 0.5010, 0.4716, 0.4100, 0.5029,  => 0.4714
f1s (weighed): 0.5532, 0.5140, 0.4131, 0.5915,  => 0.5180

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 131.6914,

accs: 0.7228, 0.6423, 0.5842, 0.8556,  => 0.7012

precs (macro): 0.4594, 0.6315, 0.2790, 0.2852,  => 0.4138
recalls (macro): 0.3470, 0.5617, 0.3401, 0.3333,  => 0.3955
f1s (macro): 0.3106, 0.5325, 0.3050, 0.3074,  => 0.3639

precs (micro): 0.7228, 0.6423, 0.5842, 0.8556,  => 0.7012
recalls (micro): 0.7228, 0.6423, 0.5842, 0.8556,  => 0.7012
f1s (micro): 0.7228, 0.6423, 0.5842, 0.8556,  => 0.7012

precs (weighed): 0.6984, 0.6347, 0.4673, 0.7321,  => 0.6331
recalls (weighed): 0.7228, 0.6423, 0.5842, 0.8556,  => 0.7012
f1s (weighed): 0.6252, 0.5835, 0.5170, 0.7890,  => 0.6287

Saved the best model to path: ./models/task_2/cnn_CafeBERT_10.pth

Epoch 12/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [03:53<00:00,  1.13it/s]
Epoch 12/20, Loss: 1118.6717,

accs: 0.5021, 0.4697, 0.3981, 0.5076,  => 0.4694

precs (macro): 0.2717, 0.2655, 0.2548, 0.2122,  => 0.2511
recalls (macro): 0.2925, 0.3059, 0.2872, 0.3369,  => 0.3056
f1s (macro): 0.2475, 0.2424, 0.2567, 0.1757,  => 0.2306

precs (micro): 0.5021, 0.4697, 0.3981, 0.5076,  => 0.4694
recalls (micro): 0.5021, 0.4697, 0.3981, 0.5076,  => 0.4694
f1s (micro): 0.5021, 0.4697, 0.3981, 0.5076,  => 0.4694

precs (weighed): 0.6220, 0.5573, 0.4148, 0.7199,  => 0.5785
recalls (weighed): 0.5021, 0.4697, 0.3981, 0.5076,  => 0.4694
f1s (weighed): 0.5539, 0.5097, 0.4027, 0.5952,  => 0.5154

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 132.4753,

accs: 0.7378, 0.6415, 0.5768, 0.8556,  => 0.7029

precs (macro): 0.4756, 0.6243, 0.2745, 0.2852,  => 0.4149
recalls (macro): 0.3732, 0.5650, 0.3330, 0.3333,  => 0.4011
f1s (macro): 0.3604, 0.5414, 0.2980, 0.3074,  => 0.3768

precs (micro): 0.7378, 0.6415, 0.5768, 0.8556,  => 0.7029
recalls (micro): 0.7378, 0.6415, 0.5768, 0.8556,  => 0.7029
f1s (micro): 0.7378, 0.6415, 0.5768, 0.8556,  => 0.7029

precs (weighed): 0.7190, 0.6297, 0.4588, 0.7321,  => 0.6349
recalls (weighed): 0.7378, 0.6415, 0.5768, 0.8556,  => 0.7029
f1s (weighed): 0.6687, 0.5896, 0.5066, 0.7890,  => 0.6385


Epoch 13/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [03:53<00:00,  1.13it/s]
Epoch 13/20, Loss: 1117.3644,

accs: 0.4963, 0.4733, 0.4046, 0.5050,  => 0.4698

precs (macro): 0.2723, 0.2687, 0.2542, 0.2118,  => 0.2517
recalls (macro): 0.2747, 0.3903, 0.2925, 0.2112,  => 0.2922
f1s (macro): 0.2443, 0.2446, 0.2597, 0.1748,  => 0.2308

precs (micro): 0.4963, 0.4733, 0.4046, 0.5050,  => 0.4698
recalls (micro): 0.4963, 0.4733, 0.4046, 0.5050,  => 0.4698
f1s (micro): 0.4963, 0.4733, 0.4046, 0.5050,  => 0.4698

precs (weighed): 0.6228, 0.5623, 0.4112, 0.7191,  => 0.5789
recalls (weighed): 0.4963, 0.4733, 0.4046, 0.5050,  => 0.4698
f1s (weighed): 0.5510, 0.5136, 0.4047, 0.5933,  => 0.5156

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 132.9993,

accs: 0.7212, 0.6432, 0.5759, 0.8556,  => 0.6990

precs (macro): 0.5733, 0.6296, 0.2736, 0.2852,  => 0.4404
recalls (macro): 0.3394, 0.5652, 0.3320, 0.3333,  => 0.3925
f1s (macro): 0.2909, 0.5398, 0.2968, 0.3074,  => 0.3587

precs (micro): 0.7212, 0.6432, 0.5759, 0.8556,  => 0.6990
recalls (micro): 0.7212, 0.6432, 0.5759, 0.8556,  => 0.6990
f1s (micro): 0.7212, 0.6432, 0.5759, 0.8556,  => 0.6990

precs (weighed): 0.7902, 0.6338, 0.4576, 0.7321,  => 0.6534
recalls (weighed): 0.7212, 0.6432, 0.5759, 0.8556,  => 0.6990
f1s (weighed): 0.6093, 0.5889, 0.5052, 0.7890,  => 0.6231


Epoch 14/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [03:52<00:00,  1.13it/s]
Epoch 14/20, Loss: 1117.1844,

accs: 0.4989, 0.4720, 0.3978, 0.5094,  => 0.4695

precs (macro): 0.2753, 0.2668, 0.2537, 0.2127,  => 0.2521
recalls (macro): 0.2729, 0.2244, 0.2837, 0.2749,  => 0.2640
f1s (macro): 0.2474, 0.2437, 0.2554, 0.1761,  => 0.2307

precs (micro): 0.4989, 0.4720, 0.3978, 0.5094,  => 0.4695
recalls (micro): 0.4989, 0.4720, 0.3978, 0.5094,  => 0.4695
f1s (micro): 0.4989, 0.4720, 0.3978, 0.5094,  => 0.4695

precs (weighed): 0.6281, 0.5594, 0.4135, 0.7221,  => 0.5808
recalls (weighed): 0.4989, 0.4720, 0.3978, 0.5094,  => 0.4695
f1s (weighed): 0.5547, 0.5120, 0.4022, 0.5972,  => 0.5165

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 133.2947,

accs: 0.7187, 0.6390, 0.5701, 0.8556,  => 0.6959

precs (macro): 0.5727, 0.6471, 0.2919, 0.2852,  => 0.4492
recalls (macro): 0.3364, 0.5480, 0.3517, 0.3333,  => 0.3923
f1s (macro): 0.2846, 0.4996, 0.3127, 0.3074,  => 0.3511

precs (micro): 0.7187, 0.6390, 0.5701, 0.8556,  => 0.6959
recalls (micro): 0.7187, 0.6390, 0.5701, 0.8556,  => 0.6959
f1s (micro): 0.7187, 0.6390, 0.5701, 0.8556,  => 0.6959

precs (weighed): 0.7889, 0.6449, 0.4987, 0.7321,  => 0.6661
recalls (weighed): 0.7187, 0.6390, 0.5701, 0.8556,  => 0.6959
f1s (weighed): 0.6035, 0.5590, 0.5223, 0.7890,  => 0.6185


Epoch 15/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [03:51<00:00,  1.14it/s]
Epoch 15/20, Loss: 1118.0243,

accs: 0.4995, 0.4707, 0.4039, 0.5121,  => 0.4716

precs (macro): 0.2706, 0.2676, 0.2538, 0.2117,  => 0.2509
recalls (macro): 0.2539, 0.2243, 0.2870, 0.2757,  => 0.2602
f1s (macro): 0.2434, 0.2440, 0.2582, 0.1763,  => 0.2305

precs (micro): 0.4995, 0.4707, 0.4039, 0.5121,  => 0.4716
recalls (micro): 0.4995, 0.4707, 0.4039, 0.5121,  => 0.4716
f1s (micro): 0.4995, 0.4707, 0.4039, 0.5121,  => 0.4716

precs (weighed): 0.6213, 0.5616, 0.4116, 0.7184,  => 0.5782
recalls (weighed): 0.4995, 0.4707, 0.4039, 0.5121,  => 0.4716
f1s (weighed): 0.5529, 0.5121, 0.4048, 0.5978,  => 0.5169

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 132.7121,

accs: 0.7328, 0.6349, 0.4971, 0.8556,  => 0.6801

precs (macro): 0.4470, 0.6442, 0.2272, 0.2852,  => 0.4009
recalls (macro): 0.3777, 0.5407, 0.2604, 0.3333,  => 0.3780
f1s (macro): 0.3705, 0.4844, 0.1906, 0.3074,  => 0.3382

precs (micro): 0.7328, 0.6349, 0.4971, 0.8556,  => 0.6801
recalls (micro): 0.7328, 0.6349, 0.4971, 0.8556,  => 0.6801
f1s (micro): 0.7328, 0.6349, 0.4971, 0.8556,  => 0.6801

precs (weighed): 0.6973, 0.6418, 0.3776, 0.7321,  => 0.6122
recalls (weighed): 0.7328, 0.6349, 0.4971, 0.8556,  => 0.6801
f1s (weighed): 0.6748, 0.5470, 0.3549, 0.7890,  => 0.5914


Epoch 16/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [03:51<00:00,  1.14it/s]
Epoch 16/20, Loss: 1115.9506,

accs: 0.4932, 0.4781, 0.4029, 0.5085,  => 0.4707

precs (macro): 0.2728, 0.2717, 0.2569, 0.2127,  => 0.2535
recalls (macro): 0.2643, 0.2278, 0.2887, 0.2122,  => 0.2482
f1s (macro): 0.2443, 0.2478, 0.2590, 0.1758,  => 0.2317

precs (micro): 0.4932, 0.4781, 0.4029, 0.5085,  => 0.4707
recalls (micro): 0.4932, 0.4781, 0.4029, 0.5085,  => 0.4707
f1s (micro): 0.4932, 0.4781, 0.4029, 0.5085,  => 0.4707

precs (weighed): 0.6220, 0.5692, 0.4184, 0.7222,  => 0.5829
recalls (weighed): 0.4932, 0.4781, 0.4029, 0.5085,  => 0.4707
f1s (weighed): 0.5490, 0.5197, 0.4071, 0.5967,  => 0.5181

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 131.0742,

accs: 0.7295, 0.6324, 0.5950, 0.8556,  => 0.7031

precs (macro): 0.5046, 0.6315, 0.2857, 0.2852,  => 0.4267
recalls (macro): 0.3532, 0.5391, 0.3491, 0.3333,  => 0.3937
f1s (macro): 0.3209, 0.4841, 0.3135, 0.3074,  => 0.3565

precs (micro): 0.7295, 0.6324, 0.5950, 0.8556,  => 0.7031
recalls (micro): 0.7295, 0.6324, 0.5950, 0.8556,  => 0.7031
f1s (micro): 0.7295, 0.6324, 0.5950, 0.8556,  => 0.7031

precs (weighed): 0.7373, 0.6317, 0.4787, 0.7321,  => 0.6450
recalls (weighed): 0.7295, 0.6324, 0.5950, 0.8556,  => 0.7031
f1s (weighed): 0.6354, 0.5463, 0.5296, 0.7890,  => 0.6251

Saved the best model to path: ./models/task_2/cnn_CafeBERT_15.pth

Epoch 17/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [03:51<00:00,  1.14it/s]
Epoch 17/20, Loss: 1119.5334,

accs: 0.4952, 0.4758, 0.3962, 0.5074,  => 0.4686

precs (macro): 0.2745, 0.2717, 0.2521, 0.2122,  => 0.2526
recalls (macro): 0.2773, 0.2270, 0.2858, 0.2743,  => 0.2661
f1s (macro): 0.2467, 0.2473, 0.2548, 0.1755,  => 0.2311

precs (micro): 0.4952, 0.4758, 0.3962, 0.5074,  => 0.4686
recalls (micro): 0.4952, 0.4758, 0.3962, 0.5074,  => 0.4686
f1s (micro): 0.4952, 0.4758, 0.3962, 0.5074,  => 0.4686

precs (weighed): 0.6264, 0.5698, 0.4102, 0.7202,  => 0.5816
recalls (weighed): 0.4952, 0.4758, 0.3962, 0.5074,  => 0.4686
f1s (weighed): 0.5516, 0.5186, 0.3997, 0.5952,  => 0.5163

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 135.0066,

accs: 0.7237, 0.6415, 0.5925, 0.8556,  => 0.7033

precs (macro): 0.5739, 0.6529, 0.2832, 0.2852,  => 0.4488
recalls (macro): 0.3424, 0.5512, 0.3442, 0.3333,  => 0.3928
f1s (macro): 0.2971, 0.5049, 0.3086, 0.3074,  => 0.3545

precs (micro): 0.7237, 0.6415, 0.5925, 0.8556,  => 0.7033
recalls (micro): 0.7237, 0.6415, 0.5925, 0.8556,  => 0.7033
f1s (micro): 0.7237, 0.6415, 0.5925, 0.8556,  => 0.7033

precs (weighed): 0.7915, 0.6498, 0.4733, 0.7321,  => 0.6616
recalls (weighed): 0.7237, 0.6415, 0.5925, 0.8556,  => 0.7033
f1s (weighed): 0.6149, 0.5634, 0.5231, 0.7890,  => 0.6226


Epoch 18/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [03:52<00:00,  1.14it/s]
Epoch 18/20, Loss: 1110.7330,

accs: 0.5025, 0.4753, 0.4071, 0.5146,  => 0.4749

precs (macro): 0.2756, 0.2714, 0.2587, 0.2130,  => 0.2547
recalls (macro): 0.2629, 0.2279, 0.2907, 0.2765,  => 0.2645
f1s (macro): 0.2477, 0.2477, 0.2612, 0.1773,  => 0.2335

precs (micro): 0.5025, 0.4753, 0.4071, 0.5146,  => 0.4749
recalls (micro): 0.5025, 0.4753, 0.4071, 0.5146,  => 0.4749
f1s (micro): 0.5025, 0.4753, 0.4071, 0.5146,  => 0.4749

precs (weighed): 0.6300, 0.5678, 0.4218, 0.7230,  => 0.5857
recalls (weighed): 0.5025, 0.4753, 0.4071, 0.5146,  => 0.4749
f1s (weighed): 0.5581, 0.5174, 0.4109, 0.6011,  => 0.5219

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 133.4515,

accs: 0.7344, 0.6349, 0.6017, 0.8556,  => 0.7066

precs (macro): 0.5251, 0.6583, 0.2933, 0.2852,  => 0.4405
recalls (macro): 0.3586, 0.5380, 0.3600, 0.3333,  => 0.3975
f1s (macro): 0.3305, 0.4749, 0.3230, 0.3074,  => 0.3590

precs (micro): 0.7344, 0.6349, 0.6017, 0.8556,  => 0.7066
recalls (micro): 0.7344, 0.6349, 0.6017, 0.8556,  => 0.7066
f1s (micro): 0.7344, 0.6349, 0.6017, 0.8556,  => 0.7066

precs (weighed): 0.7556, 0.6523, 0.4954, 0.7321,  => 0.6588
recalls (weighed): 0.7344, 0.6349, 0.6017, 0.8556,  => 0.7066
f1s (weighed): 0.6445, 0.5401, 0.5430, 0.7890,  => 0.6291


Epoch 19/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [03:52<00:00,  1.14it/s]
Epoch 19/20, Loss: 1110.8209,

accs: 0.4966, 0.4867, 0.4082, 0.5084,  => 0.4750

precs (macro): 0.2732, 0.2777, 0.2576, 0.2120,  => 0.2551
recalls (macro): 0.2552, 0.3155, 0.2932, 0.2746,  => 0.2846
f1s (macro): 0.2451, 0.2532, 0.2622, 0.1757,  => 0.2340

precs (micro): 0.4966, 0.4867, 0.4082, 0.5084,  => 0.4750
recalls (micro): 0.4966, 0.4867, 0.4082, 0.5084,  => 0.4750
f1s (micro): 0.4966, 0.4867, 0.4082, 0.5084,  => 0.4750

precs (weighed): 0.6250, 0.5800, 0.4174, 0.7195,  => 0.5855
recalls (weighed): 0.4966, 0.4867, 0.4082, 0.5084,  => 0.4750
f1s (weighed): 0.5525, 0.5292, 0.4094, 0.5956,  => 0.5217

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.24it/s]
Evaluation, Loss: 131.2405,

accs: 0.7278, 0.6373, 0.5676, 0.8556,  => 0.6971

precs (macro): 0.5061, 0.6369, 0.2695, 0.2852,  => 0.4244
recalls (macro): 0.3505, 0.5478, 0.3217, 0.3333,  => 0.3883
f1s (macro): 0.3155, 0.5021, 0.2849, 0.3074,  => 0.3525

precs (micro): 0.7278, 0.6373, 0.5676, 0.8556,  => 0.6971
recalls (micro): 0.7278, 0.6373, 0.5676, 0.8556,  => 0.6971
f1s (micro): 0.7278, 0.6373, 0.5676, 0.8556,  => 0.6971

precs (weighed): 0.7378, 0.6370, 0.4482, 0.7321,  => 0.6388
recalls (weighed): 0.7278, 0.6373, 0.5676, 0.8556,  => 0.6971
f1s (weighed): 0.6307, 0.5605, 0.4884, 0.7890,  => 0.6171


Epoch 20/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [03:51<00:00,  1.14it/s]
Epoch 20/20, Loss: 1114.1670,

accs: 0.4997, 0.4861, 0.3924, 0.5035,  => 0.4704

precs (macro): 0.2754, 0.2763, 0.2519, 0.2114,  => 0.2538
recalls (macro): 0.2755, 0.3975, 0.2801, 0.2107,  => 0.2909
f1s (macro): 0.2476, 0.2521, 0.2524, 0.1744,  => 0.2316

precs (micro): 0.4997, 0.4861, 0.3924, 0.5035,  => 0.4704
recalls (micro): 0.4997, 0.4861, 0.3924, 0.5035,  => 0.4704
f1s (micro): 0.4997, 0.4861, 0.3924, 0.5035,  => 0.4704

precs (weighed): 0.6283, 0.5774, 0.4119, 0.7178,  => 0.5838
recalls (weighed): 0.4997, 0.4861, 0.3924, 0.5035,  => 0.4704
f1s (weighed): 0.5553, 0.5276, 0.3986, 0.5918,  => 0.5183

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:30<00:00,  1.25it/s]
Evaluation, Loss: 130.3870,

accs: 0.7411, 0.6556, 0.5336, 0.8556,  => 0.6965

precs (macro): 0.5213, 0.6577, 0.2590, 0.2852,  => 0.4308
recalls (macro): 0.3685, 0.5773, 0.2908, 0.3333,  => 0.3925
f1s (macro): 0.3491, 0.5536, 0.2439, 0.3074,  => 0.3635

precs (micro): 0.7411, 0.6556, 0.5336, 0.8556,  => 0.6965
recalls (micro): 0.7411, 0.6556, 0.5336, 0.8556,  => 0.6965
f1s (micro): 0.7411, 0.6556, 0.5336, 0.8556,  => 0.6965

precs (weighed): 0.7552, 0.6570, 0.4255, 0.7321,  => 0.6424
recalls (weighed): 0.7411, 0.6556, 0.5336, 0.8556,  => 0.6965
f1s (weighed): 0.6610, 0.6016, 0.4294, 0.7890,  => 0.6203

Saved the best model to path: ./models/task_2/cnn_CafeBERT_19.pth

xlm-roberta-base

[14:15:08] task: task-2                                                                                my_import.py:133
           model_type: cnn                                                                             my_import.py:133
           model_name: xlm-roberta-base                                                                my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           num_channels: 64                                                                            my_import.py:133
           kernel_size: 64                                                                             my_import.py:133
           padding: 64                                                                                 my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/cnn_xlm-roberta-base                                           my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

Training ...
Epoch 1/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:23<00:00,  3.16it/s]
Epoch 1/20, Loss: 1188.8773,

accs: 0.5053, 0.4595, 0.2214, 0.5110,  => 0.4243

precs (macro): 0.2532, 0.2525, 0.2194, 0.2375,  => 0.2407
recalls (macro): 0.2651, 0.2108, 0.2473, 0.2134,  => 0.2341
f1s (macro): 0.2365, 0.2284, 0.1644, 0.1772,  => 0.2016

precs (micro): 0.5053, 0.4595, 0.2214, 0.5110,  => 0.4243
recalls (micro): 0.5053, 0.4595, 0.2214, 0.5110,  => 0.4243
f1s (micro): 0.5053, 0.4595, 0.2214, 0.5110,  => 0.4243

precs (weighed): 0.5963, 0.5325, 0.3554, 0.7327,  => 0.5542
recalls (weighed): 0.5053, 0.4595, 0.2214, 0.5110,  => 0.4243
f1s (weighed): 0.5459, 0.4909, 0.2257, 0.5984,  => 0.4652

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:10<00:00,  3.70it/s]
Evaluation, Loss: 143.4969,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423

Saved the best model to path: ./models/task_2/cnn_xlm-roberta-base_0.pth

Epoch 2/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:19<00:00,  3.31it/s]
Epoch 2/20, Loss: 1160.4229,

accs: 0.5057, 0.4554, 0.3774, 0.5149,  => 0.4633

precs (macro): 0.2540, 0.2461, 0.2169, 0.2760,  => 0.2483
recalls (macro): 0.2425, 0.2894, 0.2521, 0.2767,  => 0.2652
f1s (macro): 0.2348, 0.2225, 0.2233, 0.1780,  => 0.2147

precs (micro): 0.5057, 0.4554, 0.3774, 0.5149,  => 0.4633
recalls (micro): 0.5057, 0.4554, 0.3774, 0.5149,  => 0.4633
f1s (micro): 0.5057, 0.4554, 0.3774, 0.5149,  => 0.4633

precs (weighed): 0.5999, 0.5219, 0.3499, 0.7531,  => 0.5562
recalls (weighed): 0.5057, 0.4554, 0.3774, 0.5149,  => 0.4633
f1s (weighed): 0.5482, 0.4827, 0.3574, 0.6020,  => 0.4976

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:10<00:00,  3.68it/s]
Evaluation, Loss: 146.2655,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423


Epoch 3/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:19<00:00,  3.33it/s]
Epoch 3/20, Loss: 1157.2301,

accs: 0.5025, 0.4627, 0.3782, 0.5135,  => 0.4642

precs (macro): 0.2517, 0.2508, 0.2135, 0.2127,  => 0.2322
recalls (macro): 0.2678, 0.2102, 0.2422, 0.2136,  => 0.2335
f1s (macro): 0.2345, 0.2266, 0.2190, 0.1768,  => 0.2142

precs (micro): 0.5025, 0.4627, 0.3782, 0.5135,  => 0.4642
recalls (micro): 0.5025, 0.4627, 0.3782, 0.5135,  => 0.4642
f1s (micro): 0.5025, 0.4627, 0.3782, 0.5135,  => 0.4642

precs (weighed): 0.5941, 0.5298, 0.3483, 0.7222,  => 0.5486
recalls (weighed): 0.5025, 0.4627, 0.3782, 0.5135,  => 0.4642
f1s (weighed): 0.5433, 0.4902, 0.3572, 0.6001,  => 0.4977

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:10<00:00,  3.72it/s]
Evaluation, Loss: 142.5175,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423

Saved the best model to path: ./models/task_2/cnn_xlm-roberta-base_2.pth

Epoch 4/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:18<00:00,  3.35it/s]
Epoch 4/20, Loss: 1161.0029,

accs: 0.4961, 0.4646, 0.3751, 0.5100,  => 0.4615

precs (macro): 0.2524, 0.2517, 0.2169, 0.2125,  => 0.2334
recalls (macro): 0.2679, 0.2108, 0.2496, 0.2126,  => 0.2352
f1s (macro): 0.2347, 0.2271, 0.2228, 0.1761,  => 0.2152

precs (micro): 0.4961, 0.4646, 0.3751, 0.5100,  => 0.4615
recalls (micro): 0.4961, 0.4646, 0.3751, 0.5100,  => 0.4615
f1s (micro): 0.4961, 0.4646, 0.3751, 0.5100,  => 0.4615

precs (weighed): 0.5928, 0.5315, 0.3505, 0.7216,  => 0.5491
recalls (weighed): 0.4961, 0.4646, 0.3751, 0.5100,  => 0.4615
f1s (weighed): 0.5388, 0.4916, 0.3574, 0.5976,  => 0.4964

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:10<00:00,  3.73it/s]
Evaluation, Loss: 145.6572,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423


Epoch 5/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:19<00:00,  3.34it/s]
Epoch 5/20, Loss: 1158.1775,

accs: 0.4947, 0.4647, 0.3777, 0.5047,  => 0.4605

precs (macro): 0.2475, 0.2546, 0.2158, 0.2126,  => 0.2326
recalls (macro): 0.2503, 0.2950, 0.2458, 0.2735,  => 0.2662
f1s (macro): 0.2295, 0.2295, 0.2206, 0.1751,  => 0.2137

precs (micro): 0.4947, 0.4647, 0.3777, 0.5047,  => 0.4605
recalls (micro): 0.4947, 0.4647, 0.3777, 0.5047,  => 0.4605
f1s (micro): 0.4947, 0.4647, 0.3777, 0.5047,  => 0.4605

precs (weighed): 0.5883, 0.5367, 0.3492, 0.7216,  => 0.5490
recalls (weighed): 0.4947, 0.4647, 0.3777, 0.5047,  => 0.4605
f1s (weighed): 0.5365, 0.4945, 0.3566, 0.5938,  => 0.4954

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:10<00:00,  3.72it/s]
Evaluation, Loss: 143.3694,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423


Epoch 6/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:18<00:00,  3.35it/s]
Epoch 6/20, Loss: 1156.7021,

accs: 0.4991, 0.4624, 0.3821, 0.5080,  => 0.4629

precs (macro): 0.2511, 0.2514, 0.2198, 0.2127,  => 0.2337
recalls (macro): 0.2480, 0.2102, 0.2507, 0.2745,  => 0.2458
f1s (macro): 0.2325, 0.2268, 0.2250, 0.1758,  => 0.2150

precs (micro): 0.4991, 0.4624, 0.3821, 0.5080,  => 0.4629
recalls (micro): 0.4991, 0.4624, 0.3821, 0.5080,  => 0.4629
f1s (micro): 0.4991, 0.4624, 0.3821, 0.5080,  => 0.4629

precs (weighed): 0.5946, 0.5306, 0.3548, 0.7219,  => 0.5505
recalls (weighed): 0.4991, 0.4624, 0.3821, 0.5080,  => 0.4629
f1s (weighed): 0.5418, 0.4904, 0.3621, 0.5962,  => 0.4976

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:10<00:00,  3.72it/s]
Evaluation, Loss: 145.4714,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423


Epoch 7/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:18<00:00,  3.35it/s]
Epoch 7/20, Loss: 1158.9596,

accs: 0.4944, 0.4620, 0.3857, 0.5100,  => 0.4630

precs (macro): 0.2500, 0.2483, 0.2215, 0.2126,  => 0.2331
recalls (macro): 0.2450, 0.2932, 0.2521, 0.2751,  => 0.2663
f1s (macro): 0.2316, 0.2258, 0.2261, 0.1762,  => 0.2149

precs (micro): 0.4944, 0.4620, 0.3857, 0.5100,  => 0.4630
recalls (micro): 0.4944, 0.4620, 0.3857, 0.5100,  => 0.4630
f1s (micro): 0.4944, 0.4620, 0.3857, 0.5100,  => 0.4630

precs (weighed): 0.5924, 0.5250, 0.3584, 0.7217,  => 0.5494
recalls (weighed): 0.4944, 0.4620, 0.3857, 0.5100,  => 0.4630
f1s (weighed): 0.5380, 0.4880, 0.3651, 0.5975,  => 0.4971

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:10<00:00,  3.64it/s]
Evaluation, Loss: 143.5033,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423


Epoch 8/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:19<00:00,  3.31it/s]
Epoch 8/20, Loss: 1155.7986,

accs: 0.5003, 0.4620, 0.3776, 0.5092,  => 0.4623

precs (macro): 0.2512, 0.2501, 0.2159, 0.2136,  => 0.2327
recalls (macro): 0.2407, 0.2092, 0.2465, 0.1499,  => 0.2116
f1s (macro): 0.2325, 0.2253, 0.2206, 0.1762,  => 0.2137

precs (micro): 0.5003, 0.4620, 0.3776, 0.5092,  => 0.4623
recalls (micro): 0.5003, 0.4620, 0.3776, 0.5092,  => 0.4623
f1s (micro): 0.5003, 0.4620, 0.3776, 0.5092,  => 0.4623

precs (weighed): 0.5946, 0.5285, 0.3506, 0.7255,  => 0.5498
recalls (weighed): 0.5003, 0.4620, 0.3776, 0.5092,  => 0.4623
f1s (weighed): 0.5427, 0.4887, 0.3575, 0.5984,  => 0.4968

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:10<00:00,  3.73it/s]
Evaluation, Loss: 146.2049,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423

Early stopping triggered
bert-base-multilingual-cased    

[14:33:25] task: task-2                                                                                my_import.py:133
           model_type: cnn                                                                             my_import.py:133
           model_name: bert-base-multilingual-cased                                                    my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           num_channels: 64                                                                            my_import.py:133
           kernel_size: 64                                                                             my_import.py:133
           padding: 64                                                                                 my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/cnn_bert-base-multilingual-cased                               my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

Training ...
Epoch 1/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:15<00:00,  3.50it/s]
Epoch 1/20, Loss: 1176.6555,

accs: 0.4390, 0.4530, 0.3744, 0.5055,  => 0.4430

precs (macro): 0.1848, 0.2490, 0.2154, 0.2283,  => 0.2194
recalls (macro): 0.2457, 0.2084, 0.2499, 0.2115,  => 0.2289
f1s (macro): 0.1699, 0.2259, 0.2222, 0.1757,  => 0.1984

precs (micro): 0.4390, 0.4530, 0.3744, 0.5055,  => 0.4430
recalls (micro): 0.4390, 0.4530, 0.3744, 0.5055,  => 0.4430
f1s (micro): 0.4390, 0.4530, 0.3744, 0.5055,  => 0.4430

precs (weighed): 0.5258, 0.5264, 0.3475, 0.7291,  => 0.5322
recalls (weighed): 0.4390, 0.4530, 0.3744, 0.5055,  => 0.4430
f1s (weighed): 0.4763, 0.4852, 0.3554, 0.5947,  => 0.4779

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:09<00:00,  3.91it/s]
Evaluation, Loss: 145.2378,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423

Saved the best model to path: ./models/task_2/cnn_bert-base-multilingual-cased_0.pth

Epoch 2/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:11<00:00,  3.68it/s]
Epoch 2/20, Loss: 1161.7405,

accs: 0.4767, 0.4625, 0.3824, 0.5072,  => 0.4572

precs (macro): 0.2524, 0.2532, 0.2155, 0.2122,  => 0.2333
recalls (macro): 0.2530, 0.2951, 0.2399, 0.2743,  => 0.2656
f1s (macro): 0.2195, 0.2296, 0.2196, 0.1755,  => 0.2110

precs (micro): 0.4767, 0.4625, 0.3824, 0.5072,  => 0.4572
recalls (micro): 0.4767, 0.4625, 0.3824, 0.5072,  => 0.4572
f1s (micro): 0.4767, 0.4625, 0.3824, 0.5072,  => 0.4572

precs (weighed): 0.5942, 0.5345, 0.3539, 0.7201,  => 0.5507
recalls (weighed): 0.4767, 0.4625, 0.3824, 0.5072,  => 0.4572
f1s (weighed): 0.5260, 0.4934, 0.3620, 0.5950,  => 0.4941

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:09<00:00,  3.87it/s]
Evaluation, Loss: 146.0035,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423


Epoch 3/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:12<00:00,  3.64it/s]
Epoch 3/20, Loss: 1158.7147,

accs: 0.4938, 0.4511, 0.3813, 0.5114,  => 0.4594

precs (macro): 0.2465, 0.2493, 0.2169, 0.2133,  => 0.2315
recalls (macro): 0.2493, 0.2910, 0.2541, 0.1506,  => 0.2362
f1s (macro): 0.2285, 0.2259, 0.2242, 0.1765,  => 0.2138

precs (micro): 0.4938, 0.4511, 0.3813, 0.5114,  => 0.4594
recalls (micro): 0.4938, 0.4511, 0.3813, 0.5114,  => 0.4594
f1s (micro): 0.4938, 0.4511, 0.3813, 0.5114,  => 0.4594

precs (weighed): 0.5865, 0.5262, 0.3491, 0.7244,  => 0.5465
recalls (weighed): 0.4938, 0.4511, 0.3813, 0.5114,  => 0.4594
f1s (weighed): 0.5352, 0.4841, 0.3583, 0.5996,  => 0.4943

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:09<00:00,  3.84it/s]
Evaluation, Loss: 144.8401,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423

Saved the best model to path: ./models/task_2/cnn_bert-base-multilingual-cased_2.pth

Epoch 4/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:12<00:00,  3.64it/s]
Epoch 4/20, Loss: 1156.7144,

accs: 0.4969, 0.4575, 0.3841, 0.5067,  => 0.4613

precs (macro): 0.2490, 0.2492, 0.2183, 0.2118,  => 0.2321
recalls (macro): 0.2466, 0.2094, 0.2511, 0.1492,  => 0.2141
f1s (macro): 0.2308, 0.2263, 0.2241, 0.1751,  => 0.2141

precs (micro): 0.4969, 0.4575, 0.3841, 0.5067,  => 0.4613
recalls (micro): 0.4969, 0.4575, 0.3841, 0.5067,  => 0.4613
f1s (micro): 0.4969, 0.4575, 0.3841, 0.5067,  => 0.4613

precs (weighed): 0.5906, 0.5268, 0.3527, 0.7194,  => 0.5474
recalls (weighed): 0.4969, 0.4575, 0.3841, 0.5067,  => 0.4613
f1s (weighed): 0.5388, 0.4874, 0.3612, 0.5946,  => 0.4955

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:09<00:00,  3.87it/s]
Evaluation, Loss: 142.7481,

accs: 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683

precs (macro): 0.2387, 0.7240, 0.1214, 0.2852,  => 0.3423
recalls (macro): 0.3333, 0.5047, 0.2500, 0.3333,  => 0.3553
f1s (macro): 0.2782, 0.3911, 0.1634, 0.3074,  => 0.2850

precs (micro): 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683
recalls (micro): 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683
f1s (micro): 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683

precs (weighed): 0.5129, 0.6994, 0.2357, 0.7321,  => 0.5450
recalls (weighed): 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683
f1s (weighed): 0.5977, 0.4742, 0.3173, 0.7890,  => 0.5446

Saved the best model to path: ./models/task_2/cnn_bert-base-multilingual-cased_3.pth

Epoch 5/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:14<00:00,  3.53it/s]
Epoch 5/20, Loss: 1157.6996,

accs: 0.4875, 0.4660, 0.3837, 0.5004,  => 0.4594

precs (macro): 0.2454, 0.2567, 0.2184, 0.2119,  => 0.2331
recalls (macro): 0.2394, 0.2153, 0.2535, 0.2098,  => 0.2295
f1s (macro): 0.2264, 0.2333, 0.2243, 0.1739,  => 0.2145

precs (micro): 0.4875, 0.4660, 0.3837, 0.5004,  => 0.4594
recalls (micro): 0.4875, 0.4660, 0.3837, 0.5004,  => 0.4594
f1s (micro): 0.4875, 0.4660, 0.3837, 0.5004,  => 0.4594

precs (weighed): 0.5853, 0.5410, 0.3520, 0.7194,  => 0.5494
recalls (weighed): 0.4875, 0.4660, 0.3837, 0.5004,  => 0.4594
f1s (weighed): 0.5311, 0.4992, 0.3601, 0.5902,  => 0.4951

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:10<00:00,  3.70it/s]
Evaluation, Loss: 143.7275,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423


Epoch 6/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:17<00:00,  3.42it/s]
Epoch 6/20, Loss: 1154.1315,

accs: 0.5048, 0.4658, 0.3838, 0.5074,  => 0.4654

precs (macro): 0.2505, 0.2560, 0.2189, 0.2124,  => 0.2345
recalls (macro): 0.2534, 0.2150, 0.2497, 0.2743,  => 0.2481
f1s (macro): 0.2343, 0.2327, 0.2233, 0.1756,  => 0.2165

precs (micro): 0.5048, 0.4658, 0.3838, 0.5074,  => 0.4654
recalls (micro): 0.5048, 0.4658, 0.3838, 0.5074,  => 0.4654
f1s (micro): 0.5048, 0.4658, 0.3838, 0.5074,  => 0.4654

precs (weighed): 0.5922, 0.5393, 0.3543, 0.7209,  => 0.5517
recalls (weighed): 0.5048, 0.4658, 0.3838, 0.5074,  => 0.4654
f1s (weighed): 0.5441, 0.4981, 0.3614, 0.5954,  => 0.4997

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:10<00:00,  3.61it/s]
Evaluation, Loss: 144.8112,

accs: 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683

precs (macro): 0.2387, 0.5774, 0.1214, 0.2852,  => 0.3057
recalls (macro): 0.3333, 0.5570, 0.2500, 0.3333,  => 0.3684
f1s (macro): 0.2782, 0.5482, 0.1634, 0.3074,  => 0.3243

precs (micro): 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683
recalls (micro): 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683
f1s (micro): 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683

precs (weighed): 0.5129, 0.5931, 0.2357, 0.7321,  => 0.5184
recalls (weighed): 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683
f1s (weighed): 0.5977, 0.5875, 0.3173, 0.7890,  => 0.5729


Epoch 7/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:16<00:00,  3.43it/s]
Epoch 7/20, Loss: 1156.8980,

accs: 0.4945, 0.4644, 0.3782, 0.4985,  => 0.4589

precs (macro): 0.2467, 0.2547, 0.2160, 0.2119,  => 0.2323
recalls (macro): 0.2548, 0.2981, 0.2494, 0.2092,  => 0.2529
f1s (macro): 0.2290, 0.2327, 0.2217, 0.1735,  => 0.2142

precs (micro): 0.4945, 0.4644, 0.3782, 0.4985,  => 0.4589
recalls (micro): 0.4945, 0.4644, 0.3782, 0.4985,  => 0.4589
f1s (micro): 0.4945, 0.4644, 0.3782, 0.4985,  => 0.4589

precs (weighed): 0.5872, 0.5373, 0.3480, 0.7196,  => 0.5480
recalls (weighed): 0.4945, 0.4644, 0.3782, 0.4985,  => 0.4589
f1s (weighed): 0.5358, 0.4969, 0.3559, 0.5889,  => 0.4944

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:10<00:00,  3.64it/s]
Evaluation, Loss: 143.1992,

accs: 0.7162, 0.6282, 0.4855, 0.8556,  => 0.6714

precs (macro): 0.2387, 0.6781, 0.1214, 0.2852,  => 0.3309
recalls (macro): 0.3333, 0.5247, 0.2500, 0.3333,  => 0.3603
f1s (macro): 0.2782, 0.4413, 0.1634, 0.3074,  => 0.2976

precs (micro): 0.7162, 0.6282, 0.4855, 0.8556,  => 0.6714
recalls (micro): 0.7162, 0.6282, 0.4855, 0.8556,  => 0.6714
f1s (micro): 0.7162, 0.6282, 0.4855, 0.8556,  => 0.6714

precs (weighed): 0.5129, 0.6661, 0.2357, 0.7321,  => 0.5367
recalls (weighed): 0.7162, 0.6282, 0.4855, 0.8556,  => 0.6714
f1s (weighed): 0.5977, 0.5140, 0.3173, 0.7890,  => 0.5545


Epoch 8/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:14<00:00,  3.52it/s]
Epoch 8/20, Loss: 1155.9987,

accs: 0.4953, 0.4595, 0.3827, 0.5008,  => 0.4596

precs (macro): 0.2498, 0.2529, 0.2164, 0.2129,  => 0.2330
recalls (macro): 0.2536, 0.2129, 0.2481, 0.2099,  => 0.2311
f1s (macro): 0.2324, 0.2305, 0.2223, 0.1743,  => 0.2149

precs (micro): 0.4953, 0.4595, 0.3827, 0.5008,  => 0.4596
recalls (micro): 0.4953, 0.4595, 0.3827, 0.5008,  => 0.4596
f1s (micro): 0.4953, 0.4595, 0.3827, 0.5008,  => 0.4596

precs (weighed): 0.5902, 0.5333, 0.3514, 0.7228,  => 0.5494
recalls (weighed): 0.4953, 0.4595, 0.3827, 0.5008,  => 0.4596
f1s (weighed): 0.5375, 0.4925, 0.3604, 0.5916,  => 0.4955

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:10<00:00,  3.65it/s]
Evaluation, Loss: 143.8354,

accs: 0.7162, 0.6199, 0.4855, 0.8556,  => 0.6693

precs (macro): 0.2387, 0.7088, 0.1214, 0.2852,  => 0.3385
recalls (macro): 0.3333, 0.5108, 0.2500, 0.3333,  => 0.3569
f1s (macro): 0.2782, 0.4061, 0.1634, 0.3074,  => 0.2888

precs (micro): 0.7162, 0.6199, 0.4855, 0.8556,  => 0.6693
recalls (micro): 0.7162, 0.6199, 0.4855, 0.8556,  => 0.6693
f1s (micro): 0.7162, 0.6199, 0.4855, 0.8556,  => 0.6693

precs (weighed): 0.5129, 0.6883, 0.2357, 0.7321,  => 0.5422
recalls (weighed): 0.7162, 0.6199, 0.4855, 0.8556,  => 0.6693
f1s (weighed): 0.5977, 0.4863, 0.3173, 0.7890,  => 0.5476


Epoch 9/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [01:15<00:00,  3.50it/s]
Epoch 9/20, Loss: 1156.2911,

accs: 0.4999, 0.4593, 0.3789, 0.5037,  => 0.4605

precs (macro): 0.2539, 0.2569, 0.2176, 0.2115,  => 0.2350
recalls (macro): 0.2630, 0.2143, 0.2463, 0.2108,  => 0.2336
f1s (macro): 0.2353, 0.2332, 0.2225, 0.1745,  => 0.2164

precs (micro): 0.4999, 0.4593, 0.3789, 0.5037,  => 0.4605
recalls (micro): 0.4999, 0.4593, 0.3789, 0.5037,  => 0.4605
f1s (micro): 0.4999, 0.4593, 0.3789, 0.5037,  => 0.4605

precs (weighed): 0.5959, 0.5404, 0.3530, 0.7181,  => 0.5519
recalls (weighed): 0.4999, 0.4593, 0.3789, 0.5037,  => 0.4605
f1s (weighed): 0.5426, 0.4958, 0.3601, 0.5920,  => 0.4976

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:10<00:00,  3.70it/s]
Evaluation, Loss: 145.5904,

accs: 0.7162, 0.6241, 0.4855, 0.8556,  => 0.6703

precs (macro): 0.2387, 0.6028, 0.1214, 0.2852,  => 0.3120
recalls (macro): 0.3333, 0.5299, 0.2500, 0.3333,  => 0.3616
f1s (macro): 0.2782, 0.4711, 0.1634, 0.3074,  => 0.3050

precs (micro): 0.7162, 0.6241, 0.4855, 0.8556,  => 0.6703
recalls (micro): 0.7162, 0.6241, 0.4855, 0.8556,  => 0.6703
f1s (micro): 0.7162, 0.6241, 0.4855, 0.8556,  => 0.6703

precs (weighed): 0.5129, 0.6084, 0.2357, 0.7321,  => 0.5223
recalls (weighed): 0.7162, 0.6241, 0.4855, 0.8556,  => 0.6703
f1s (weighed): 0.5977, 0.5351, 0.3173, 0.7890,  => 0.5598

Early stopping triggered
distilbert-base-multilingual-cased

[15:06:37] task: task-2                                                                                my_import.py:133
           model_type: cnn                                                                             my_import.py:133
           model_name: distilbert-base-multilingual-cased                                              my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           num_channels: 64                                                                            my_import.py:133
           kernel_size: 64                                                                             my_import.py:133
           padding: 64                                                                                 my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/cnn_distilbert-base-multilingual-cased                         my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133
Training ...
Epoch 1/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:56<00:00,  4.71it/s]
Epoch 1/20, Loss: 1166.4143,

accs: 0.4393, 0.4742, 0.3760, 0.5106,  => 0.4500

precs (macro): 0.1814, 0.2625, 0.2448, 0.2118,  => 0.2251
recalls (macro): 0.2345, 0.3054, 0.2540, 0.2128,  => 0.2517
f1s (macro): 0.1683, 0.2406, 0.2296, 0.1759,  => 0.2036

precs (micro): 0.4393, 0.4742, 0.3760, 0.5106,  => 0.4500
recalls (micro): 0.4393, 0.4742, 0.3760, 0.5106,  => 0.4500
f1s (micro): 0.4393, 0.4742, 0.3760, 0.5106,  => 0.4500

precs (weighed): 0.5166, 0.5516, 0.3690, 0.7191,  => 0.5390
recalls (weighed): 0.4393, 0.4742, 0.3760, 0.5106,  => 0.4500
f1s (weighed): 0.4729, 0.5094, 0.3638, 0.5971,  => 0.4858

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.68it/s]
Evaluation, Loss: 140.0192,

accs: 0.7162, 0.6373, 0.5021, 0.8556,  => 0.6778

precs (macro): 0.2387, 0.6293, 0.2714, 0.2852,  => 0.3562
recalls (macro): 0.3333, 0.5510, 0.2640, 0.3333,  => 0.3704
f1s (macro): 0.2782, 0.5113, 0.1962, 0.3074,  => 0.3233

precs (micro): 0.7162, 0.6373, 0.5021, 0.8556,  => 0.6778
recalls (micro): 0.7162, 0.6373, 0.5021, 0.8556,  => 0.6778
f1s (micro): 0.7162, 0.6373, 0.5021, 0.8556,  => 0.6778

precs (weighed): 0.5129, 0.6316, 0.4355, 0.7321,  => 0.5780
recalls (weighed): 0.7162, 0.6373, 0.5021, 0.8556,  => 0.6778
f1s (weighed): 0.5977, 0.5671, 0.3620, 0.7890,  => 0.5790

Saved the best model to path: ./models/task_2/cnn_distilbert-base-multilingual-cased_0.pth

Epoch 2/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:43<00:00,  6.02it/s]
Epoch 2/20, Loss: 1129.3126,

accs: 0.4440, 0.4766, 0.4113, 0.5036,  => 0.4589

precs (macro): 0.1835, 0.2701, 0.2562, 0.2108,  => 0.2302
recalls (macro): 0.2390, 0.2267, 0.2836, 0.3357,  => 0.2712
f1s (macro): 0.1702, 0.2465, 0.2598, 0.1744,  => 0.2127

precs (micro): 0.4440, 0.4766, 0.4113, 0.5036,  => 0.4589
recalls (micro): 0.4440, 0.4766, 0.4113, 0.5036,  => 0.4589
f1s (micro): 0.4440, 0.4766, 0.4113, 0.5036,  => 0.4589

precs (weighed): 0.5227, 0.5654, 0.4160, 0.7153,  => 0.5549
recalls (weighed): 0.4440, 0.4766, 0.4113, 0.5036,  => 0.4589
f1s (weighed): 0.4782, 0.5172, 0.4115, 0.5908,  => 0.4994

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.59it/s]
Evaluation, Loss: 134.9694,

accs: 0.7162, 0.6465, 0.6207, 0.8556,  => 0.7098

precs (macro): 0.2387, 0.6469, 0.3089, 0.2852,  => 0.3699
recalls (macro): 0.3333, 0.5632, 0.3804, 0.3333,  => 0.4025
f1s (macro): 0.2782, 0.5304, 0.3386, 0.3074,  => 0.3637

precs (micro): 0.7162, 0.6465, 0.6207, 0.8556,  => 0.7098
recalls (micro): 0.7162, 0.6465, 0.6207, 0.8556,  => 0.7098
f1s (micro): 0.7162, 0.6465, 0.6207, 0.8556,  => 0.7098

precs (weighed): 0.5129, 0.6468, 0.5200, 0.7321,  => 0.6029
recalls (weighed): 0.7162, 0.6465, 0.6207, 0.8556,  => 0.7098
f1s (weighed): 0.5977, 0.5829, 0.5624, 0.7890,  => 0.6330

Saved the best model to path: ./models/task_2/cnn_distilbert-base-multilingual-cased_1.pth

Epoch 3/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:44<00:00,  5.95it/s]
Epoch 3/20, Loss: 1114.0113,

accs: 0.4447, 0.4695, 0.4198, 0.5107,  => 0.4612

precs (macro): 0.1835, 0.2644, 0.3581, 0.2134,  => 0.2549
recalls (macro): 0.2449, 0.2223, 0.3154, 0.2128,  => 0.2489
f1s (macro): 0.1706, 0.2415, 0.2973, 0.1765,  => 0.2215

precs (micro): 0.4447, 0.4695, 0.4198, 0.5107,  => 0.4612
recalls (micro): 0.4447, 0.4695, 0.4198, 0.5107,  => 0.4612
f1s (micro): 0.4447, 0.4695, 0.4198, 0.5107,  => 0.4612

precs (weighed): 0.5222, 0.5556, 0.4887, 0.7246,  => 0.5728
recalls (weighed): 0.4447, 0.4695, 0.4198, 0.5107,  => 0.4612
f1s (weighed): 0.4783, 0.5088, 0.4382, 0.5991,  => 0.5061

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.59it/s]
Evaluation, Loss: 140.2518,

accs: 0.7162, 0.6415, 0.5867, 0.8556,  => 0.7000

precs (macro): 0.2387, 0.6147, 0.4632, 0.2852,  => 0.4004
recalls (macro): 0.3333, 0.5815, 0.4062, 0.3333,  => 0.4136
f1s (macro): 0.2782, 0.5742, 0.3942, 0.3074,  => 0.3885

precs (micro): 0.7162, 0.6415, 0.5867, 0.8556,  => 0.7000
recalls (micro): 0.7162, 0.6415, 0.5867, 0.8556,  => 0.7000
f1s (micro): 0.7162, 0.6415, 0.5867, 0.8556,  => 0.7000

precs (weighed): 0.5129, 0.6252, 0.6135, 0.7321,  => 0.6209
recalls (weighed): 0.7162, 0.6415, 0.5867, 0.8556,  => 0.7000
f1s (weighed): 0.5977, 0.6123, 0.5637, 0.7890,  => 0.6407


Epoch 4/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:44<00:00,  5.95it/s]
Epoch 4/20, Loss: 1102.2461,

accs: 0.4357, 0.4707, 0.4459, 0.5178,  => 0.4675

precs (macro): 0.1839, 0.2667, 0.3960, 0.2127,  => 0.2648
recalls (macro): 0.2616, 0.2235, 0.3574, 0.2774,  => 0.2800
f1s (macro): 0.1693, 0.2432, 0.3613, 0.1778,  => 0.2379

precs (micro): 0.4357, 0.4707, 0.4459, 0.5178,  => 0.4675
recalls (micro): 0.4357, 0.4707, 0.4459, 0.5178,  => 0.4675
f1s (micro): 0.4357, 0.4707, 0.4459, 0.5178,  => 0.4675

precs (weighed): 0.5220, 0.5606, 0.5236, 0.7220,  => 0.5820
recalls (weighed): 0.4357, 0.4707, 0.4459, 0.5178,  => 0.4675
f1s (weighed): 0.4724, 0.5117, 0.4783, 0.6029,  => 0.5163

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.58it/s]
Evaluation, Loss: 130.0219,

accs: 0.7162, 0.6490, 0.6822, 0.8556,  => 0.7257

precs (macro): 0.2387, 0.6236, 0.4896, 0.2143,  => 0.3915
recalls (macro): 0.3333, 0.6077, 0.5068, 0.2500,  => 0.4245
f1s (macro): 0.2782, 0.6088, 0.4972, 0.2308,  => 0.4037

precs (micro): 0.7162, 0.6490, 0.6822, 0.8556,  => 0.7257
recalls (micro): 0.7162, 0.6490, 0.6822, 0.8556,  => 0.7257
f1s (micro): 0.7162, 0.6490, 0.6822, 0.8556,  => 0.7257

precs (weighed): 0.5129, 0.6373, 0.6548, 0.7333,  => 0.6346
recalls (weighed): 0.7162, 0.6490, 0.6822, 0.8556,  => 0.7257
f1s (weighed): 0.5977, 0.6370, 0.6671, 0.7897,  => 0.6729

Saved the best model to path: ./models/task_2/cnn_distilbert-base-multilingual-cased_3.pth

Epoch 5/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:44<00:00,  5.88it/s]
Epoch 5/20, Loss: 1091.6742,

accs: 0.4404, 0.4824, 0.4530, 0.5104,  => 0.4716

precs (macro): 0.1812, 0.2716, 0.4277, 0.2145,  => 0.2738
recalls (macro): 0.2264, 0.3121, 0.3705, 0.2752,  => 0.2961
f1s (macro): 0.1682, 0.2486, 0.3836, 0.1769,  => 0.2443

precs (micro): 0.4404, 0.4824, 0.4530, 0.5104,  => 0.4716
recalls (micro): 0.4404, 0.4824, 0.4530, 0.5104,  => 0.4716
f1s (micro): 0.4404, 0.4824, 0.4530, 0.5104,  => 0.4716

precs (weighed): 0.5168, 0.5690, 0.5449, 0.7282,  => 0.5897
recalls (weighed): 0.4404, 0.4824, 0.4530, 0.5104,  => 0.4716
f1s (weighed): 0.4739, 0.5220, 0.4916, 0.6000,  => 0.5219

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.53it/s]
Evaluation, Loss: 130.2608,

accs: 0.7162, 0.6548, 0.6456, 0.8556,  => 0.7180

precs (macro): 0.2387, 0.6392, 0.4743, 0.2852,  => 0.4093
recalls (macro): 0.3333, 0.5884, 0.5402, 0.3333,  => 0.4488
f1s (macro): 0.2782, 0.5776, 0.4784, 0.3074,  => 0.4104

precs (micro): 0.7162, 0.6548, 0.6456, 0.8556,  => 0.7180
recalls (micro): 0.7162, 0.6548, 0.6456, 0.8556,  => 0.7180
f1s (micro): 0.7162, 0.6548, 0.6456, 0.8556,  => 0.7180

precs (weighed): 0.5129, 0.6447, 0.6788, 0.7321,  => 0.6421
recalls (weighed): 0.7162, 0.6548, 0.6456, 0.8556,  => 0.7180
f1s (weighed): 0.5977, 0.6182, 0.6430, 0.7890,  => 0.6620


Epoch 6/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:44<00:00,  5.92it/s]
Epoch 6/20, Loss: 1098.2719,

accs: 0.4280, 0.4806, 0.4523, 0.5023,  => 0.4658

precs (macro): 0.1812, 0.2727, 0.4321, 0.2140,  => 0.2750
recalls (macro): 0.2419, 0.2295, 0.3785, 0.2728,  => 0.2807
f1s (macro): 0.1661, 0.2493, 0.3852, 0.1751,  => 0.2439

precs (micro): 0.4280, 0.4806, 0.4523, 0.5023,  => 0.4658
recalls (micro): 0.4280, 0.4806, 0.4523, 0.5023,  => 0.4658
f1s (micro): 0.4280, 0.4806, 0.4523, 0.5023,  => 0.4658

precs (weighed): 0.5156, 0.5712, 0.5460, 0.7263,  => 0.5898
recalls (weighed): 0.4280, 0.4806, 0.4523, 0.5023,  => 0.4658
f1s (weighed): 0.4655, 0.5220, 0.4904, 0.5937,  => 0.5179

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.54it/s]
Evaluation, Loss: 128.8361,

accs: 0.7162, 0.6581, 0.6838, 0.8556,  => 0.7284

precs (macro): 0.2387, 0.6384, 0.5370, 0.2146,  => 0.4072
recalls (macro): 0.3333, 0.6363, 0.5019, 0.2500,  => 0.4304
f1s (macro): 0.2782, 0.6372, 0.5072, 0.2310,  => 0.4134

precs (micro): 0.7162, 0.6581, 0.6838, 0.8556,  => 0.7284
recalls (micro): 0.7162, 0.6581, 0.6838, 0.8556,  => 0.7284
f1s (micro): 0.7162, 0.6581, 0.6838, 0.8556,  => 0.7284

precs (weighed): 0.5129, 0.6558, 0.6715, 0.7345,  => 0.6437
recalls (weighed): 0.7162, 0.6581, 0.6838, 0.8556,  => 0.7284
f1s (weighed): 0.5977, 0.6568, 0.6685, 0.7904,  => 0.6784

Saved the best model to path: ./models/task_2/cnn_distilbert-base-multilingual-cased_5.pth

Epoch 7/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:44<00:00,  5.90it/s]
Epoch 7/20, Loss: 1080.1486,

accs: 0.4393, 0.4809, 0.4641, 0.5126,  => 0.4742

precs (macro): 0.1840, 0.2755, 0.4510, 0.2153,  => 0.2814
recalls (macro): 0.2458, 0.3143, 0.3921, 0.1509,  => 0.2758
f1s (macro): 0.1696, 0.2515, 0.4000, 0.1774,  => 0.2497

precs (micro): 0.4393, 0.4809, 0.4641, 0.5126,  => 0.4742
recalls (micro): 0.4393, 0.4809, 0.4641, 0.5126,  => 0.4742
f1s (micro): 0.4393, 0.4809, 0.4641, 0.5126,  => 0.4742

precs (weighed): 0.5235, 0.5762, 0.5649, 0.7311,  => 0.5989
recalls (weighed): 0.4393, 0.4809, 0.4641, 0.5126,  => 0.4742
f1s (weighed): 0.4755, 0.5241, 0.5049, 0.6027,  => 0.5268

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.42it/s]
Evaluation, Loss: 126.4471,

accs: 0.7162, 0.6896, 0.7095, 0.8556,  => 0.7427

precs (macro): 0.2387, 0.7035, 0.5415, 0.2150,  => 0.4247
recalls (macro): 0.3333, 0.6220, 0.5255, 0.2500,  => 0.4327
f1s (macro): 0.2782, 0.6148, 0.5287, 0.2312,  => 0.4132

precs (micro): 0.7162, 0.6896, 0.7095, 0.8556,  => 0.7427
recalls (micro): 0.7162, 0.6896, 0.7095, 0.8556,  => 0.7427
f1s (micro): 0.7162, 0.6896, 0.7095, 0.8556,  => 0.7427

precs (weighed): 0.5129, 0.6987, 0.6765, 0.7357,  => 0.6560
recalls (weighed): 0.7162, 0.6896, 0.7095, 0.8556,  => 0.7427
f1s (weighed): 0.5977, 0.6530, 0.6870, 0.7911,  => 0.6822

Saved the best model to path: ./models/task_2/cnn_distilbert-base-multilingual-cased_6.pth

Epoch 8/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:45<00:00,  5.84it/s]
Epoch 8/20, Loss: 1075.5605,

accs: 0.4301, 0.4730, 0.4767, 0.5168,  => 0.4742

precs (macro): 0.1839, 0.2726, 0.4588, 0.2182,  => 0.2834
recalls (macro): 0.2767, 0.2273, 0.4066, 0.3395,  => 0.3125
f1s (macro): 0.1686, 0.2479, 0.4113, 0.1796,  => 0.2518

precs (micro): 0.4301, 0.4730, 0.4767, 0.5168,  => 0.4742
recalls (micro): 0.4301, 0.4730, 0.4767, 0.5168,  => 0.4742
f1s (micro): 0.4301, 0.4730, 0.4767, 0.5168,  => 0.4742

precs (weighed): 0.5209, 0.5699, 0.5754, 0.7402,  => 0.6016
recalls (weighed): 0.4301, 0.4730, 0.4767, 0.5168,  => 0.4742
f1s (weighed): 0.4682, 0.5169, 0.5169, 0.6084,  => 0.5276

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.52it/s]
Evaluation, Loss: 126.7689,

accs: 0.7162, 0.6739, 0.6307, 0.8556,  => 0.7191

precs (macro): 0.2387, 0.6581, 0.5496, 0.2852,  => 0.4329
recalls (macro): 0.3333, 0.6217, 0.4164, 0.3333,  => 0.4262
f1s (macro): 0.2782, 0.6215, 0.4018, 0.3074,  => 0.4022

precs (micro): 0.7162, 0.6739, 0.6307, 0.8556,  => 0.7191
recalls (micro): 0.7162, 0.6739, 0.6307, 0.8556,  => 0.7191
f1s (micro): 0.7162, 0.6739, 0.6307, 0.8556,  => 0.7191

precs (weighed): 0.5129, 0.6649, 0.6685, 0.7321,  => 0.6446
recalls (weighed): 0.7162, 0.6739, 0.6307, 0.8556,  => 0.7191
f1s (weighed): 0.5977, 0.6532, 0.5967, 0.7890,  => 0.6592


Epoch 9/20, Batch 264/264: 100%|█████████████████████████████████████████████████████| 264/264 [00:44<00:00,  5.92it/s]
Epoch 9/20, Loss: 1077.9465,

accs: 0.4316, 0.4828, 0.4570, 0.5073,  => 0.4697

precs (macro): 0.1824, 0.2780, 0.4545, 0.2173,  => 0.2831
recalls (macro): 0.2375, 0.3140, 0.3896, 0.1494,  => 0.2726
f1s (macro): 0.1671, 0.2524, 0.3986, 0.1770,  => 0.2488

precs (micro): 0.4316, 0.4828, 0.4570, 0.5073,  => 0.4697
recalls (micro): 0.4316, 0.4828, 0.4570, 0.5073,  => 0.4697
f1s (micro): 0.4316, 0.4828, 0.4570, 0.5073,  => 0.4697

precs (weighed): 0.5193, 0.5815, 0.5700, 0.7382,  => 0.6023
recalls (weighed): 0.4316, 0.4828, 0.4570, 0.5073,  => 0.4697
f1s (weighed): 0.4693, 0.5275, 0.5023, 0.6013,  => 0.5251

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.42it/s]
Evaluation, Loss: 126.6522,

accs: 0.7137, 0.6763, 0.6755, 0.8548,  => 0.7301

precs (macro): 0.2423, 0.6875, 0.5648, 0.2177,  => 0.4281
recalls (macro): 0.3322, 0.6045, 0.4576, 0.2498,  => 0.4110
f1s (macro): 0.2802, 0.5916, 0.4740, 0.2326,  => 0.3946

precs (micro): 0.7137, 0.6763, 0.6755, 0.8548,  => 0.7301
recalls (micro): 0.7137, 0.6763, 0.6755, 0.8548,  => 0.7301
f1s (micro): 0.7137, 0.6763, 0.6755, 0.8548,  => 0.7301

precs (weighed): 0.5206, 0.6838, 0.6693, 0.7449,  => 0.6547
recalls (weighed): 0.7137, 0.6763, 0.6755, 0.8548,  => 0.7301
f1s (weighed): 0.6021, 0.6334, 0.6411, 0.7961,  => 0.6682


Epoch 10/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [00:44<00:00,  5.92it/s]
Epoch 10/20, Loss: 1074.6246,

accs: 0.4353, 0.4871, 0.4511, 0.5075,  => 0.4703

precs (macro): 0.1849, 0.2830, 0.4523, 0.2175,  => 0.2844
recalls (macro): 0.2700, 0.2341, 0.3912, 0.1494,  => 0.2612
f1s (macro): 0.1699, 0.2562, 0.3961, 0.1771,  => 0.2498

precs (micro): 0.4353, 0.4871, 0.4511, 0.5075,  => 0.4703
recalls (micro): 0.4353, 0.4871, 0.4511, 0.5075,  => 0.4703
f1s (micro): 0.4353, 0.4871, 0.4511, 0.5075,  => 0.4703

precs (weighed): 0.5244, 0.5910, 0.5627, 0.7387,  => 0.6042
recalls (weighed): 0.4353, 0.4871, 0.4511, 0.5075,  => 0.4703
f1s (weighed): 0.4729, 0.5340, 0.4955, 0.6017,  => 0.5260

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.56it/s]
Evaluation, Loss: 125.1910,

accs: 0.7154, 0.6141, 0.6780, 0.8556,  => 0.7158

precs (macro): 0.2404, 0.6288, 0.5713, 0.2150,  => 0.4139
recalls (macro): 0.3329, 0.6338, 0.4473, 0.2500,  => 0.4160
f1s (macro): 0.2792, 0.6129, 0.4498, 0.2312,  => 0.3933

precs (micro): 0.7154, 0.6141, 0.6780, 0.8556,  => 0.7158
recalls (micro): 0.7154, 0.6141, 0.6780, 0.8556,  => 0.7158
f1s (micro): 0.7154, 0.6141, 0.6780, 0.8556,  => 0.7158

precs (weighed): 0.5166, 0.6574, 0.6799, 0.7357,  => 0.6474
recalls (weighed): 0.7154, 0.6141, 0.6780, 0.8556,  => 0.7158
f1s (weighed): 0.6000, 0.6177, 0.6432, 0.7911,  => 0.6630

Saved the best model to path: ./models/task_2/cnn_distilbert-base-multilingual-cased_9.pth

Epoch 11/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [00:44<00:00,  5.90it/s]
Epoch 11/20, Loss: 1070.7585,

accs: 0.4427, 0.4876, 0.4633, 0.5007,  => 0.4736

precs (macro): 0.1865, 0.2846, 0.4571, 0.2176,  => 0.2865
recalls (macro): 0.2584, 0.3192, 0.3901, 0.2099,  => 0.2944
f1s (macro): 0.1717, 0.2582, 0.4022, 0.1758,  => 0.2520

precs (micro): 0.4427, 0.4876, 0.4633, 0.5007,  => 0.4736
recalls (micro): 0.4427, 0.4876, 0.4633, 0.5007,  => 0.4736
f1s (micro): 0.4427, 0.4876, 0.4633, 0.5007,  => 0.4736

precs (weighed): 0.5300, 0.5926, 0.5742, 0.7389,  => 0.6089
recalls (weighed): 0.4427, 0.4876, 0.4633, 0.5007,  => 0.4736
f1s (weighed): 0.4800, 0.5348, 0.5085, 0.5968,  => 0.5300

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.55it/s]
Evaluation, Loss: 130.7258,

accs: 0.7145, 0.6199, 0.6398, 0.8548,  => 0.7073

precs (macro): 0.2410, 0.6369, 0.5275, 0.2168,  => 0.4055
recalls (macro): 0.3326, 0.6417, 0.5204, 0.2498,  => 0.4361
f1s (macro): 0.2795, 0.6191, 0.5039, 0.2321,  => 0.4086

precs (micro): 0.7145, 0.6199, 0.6398, 0.8548,  => 0.7073
recalls (micro): 0.7145, 0.6199, 0.6398, 0.8548,  => 0.7073
f1s (micro): 0.7145, 0.6199, 0.6398, 0.8548,  => 0.7073

precs (weighed): 0.5177, 0.6662, 0.6769, 0.7418,  => 0.6507
recalls (weighed): 0.7145, 0.6199, 0.6398, 0.8548,  => 0.7073
f1s (weighed): 0.6004, 0.6231, 0.6276, 0.7943,  => 0.6614


Epoch 12/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [00:44<00:00,  5.92it/s]
Epoch 12/20, Loss: 1067.9896,

accs: 0.4395, 0.4833, 0.4649, 0.5011,  => 0.4722

precs (macro): 0.1867, 0.2822, 0.4660, 0.2186,  => 0.2884
recalls (macro): 0.2516, 0.3168, 0.4083, 0.2100,  => 0.2967
f1s (macro): 0.1709, 0.2557, 0.4130, 0.1763,  => 0.2540

precs (micro): 0.4395, 0.4833, 0.4649, 0.5011,  => 0.4722
recalls (micro): 0.4395, 0.4833, 0.4649, 0.5011,  => 0.4722
f1s (micro): 0.4395, 0.4833, 0.4649, 0.5011,  => 0.4722

precs (weighed): 0.5312, 0.5882, 0.5813, 0.7422,  => 0.6107
recalls (weighed): 0.4395, 0.4833, 0.4649, 0.5011,  => 0.4722
f1s (weighed): 0.4787, 0.5304, 0.5115, 0.5982,  => 0.5297

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:06<00:00,  6.33it/s]
Evaluation, Loss: 126.8215,

accs: 0.7137, 0.7071, 0.7095, 0.8531,  => 0.7459

precs (macro): 0.2429, 0.6905, 0.5766, 0.2204,  => 0.4326
recalls (macro): 0.3322, 0.6814, 0.5132, 0.2493,  => 0.4440
f1s (macro): 0.2806, 0.6845, 0.5250, 0.2340,  => 0.4310

precs (micro): 0.7137, 0.7071, 0.7095, 0.8531,  => 0.7459
recalls (micro): 0.7137, 0.7071, 0.7095, 0.8531,  => 0.7459
f1s (micro): 0.7137, 0.7071, 0.7095, 0.8531,  => 0.7459

precs (weighed): 0.5220, 0.7025, 0.6988, 0.7543,  => 0.6694
recalls (weighed): 0.7137, 0.7071, 0.7095, 0.8531,  => 0.7459
f1s (weighed): 0.6030, 0.7035, 0.6913, 0.8007,  => 0.6996


Epoch 13/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [00:44<00:00,  5.92it/s]
Epoch 13/20, Loss: 1064.2923,

accs: 0.4397, 0.4906, 0.4656, 0.5071,  => 0.4757

precs (macro): 0.1854, 0.2889, 0.4568, 0.2189,  => 0.2875
recalls (macro): 0.2602, 0.2364, 0.3968, 0.2742,  => 0.2919
f1s (macro): 0.1707, 0.2600, 0.4053, 0.1777,  => 0.2534

precs (micro): 0.4397, 0.4906, 0.4656, 0.5071,  => 0.4757
recalls (micro): 0.4397, 0.4906, 0.4656, 0.5071,  => 0.4757
f1s (micro): 0.4397, 0.4906, 0.4656, 0.5071,  => 0.4757

precs (weighed): 0.5267, 0.6018, 0.5725, 0.7430,  => 0.6110
recalls (weighed): 0.4397, 0.4906, 0.4656, 0.5071,  => 0.4757
f1s (weighed): 0.4768, 0.5405, 0.5090, 0.6026,  => 0.5322

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.55it/s]
Evaluation, Loss: 126.2429,

accs: 0.7145, 0.6938, 0.6946, 0.8548,  => 0.7394

precs (macro): 0.2418, 0.6853, 0.5615, 0.2220,  => 0.4276
recalls (macro): 0.3326, 0.6427, 0.5245, 0.2498,  => 0.4374
f1s (macro): 0.2800, 0.6447, 0.5281, 0.2351,  => 0.4219

precs (micro): 0.7145, 0.6938, 0.6946, 0.8548,  => 0.7394
recalls (micro): 0.7145, 0.6938, 0.6946, 0.8548,  => 0.7394
f1s (micro): 0.7145, 0.6938, 0.6946, 0.8548,  => 0.7394

precs (weighed): 0.5195, 0.6890, 0.6983, 0.7597,  => 0.6666
recalls (weighed): 0.7145, 0.6938, 0.6946, 0.8548,  => 0.7394
f1s (weighed): 0.6016, 0.6744, 0.6828, 0.8044,  => 0.6908


Epoch 14/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [00:44<00:00,  5.91it/s]
Epoch 14/20, Loss: 1065.6315,

accs: 0.4279, 0.4901, 0.4742, 0.5059,  => 0.4745

precs (macro): 0.1855, 0.2857, 0.4707, 0.2182,  => 0.2900
recalls (macro): 0.2560, 0.2372, 0.4120, 0.2739,  => 0.2948
f1s (macro): 0.1681, 0.2591, 0.4196, 0.1772,  => 0.2560

precs (micro): 0.4279, 0.4901, 0.4742, 0.5059,  => 0.4745
recalls (micro): 0.4279, 0.4901, 0.4742, 0.5059,  => 0.4745
f1s (micro): 0.4279, 0.4901, 0.4742, 0.5059,  => 0.4745

precs (weighed): 0.5271, 0.5961, 0.5833, 0.7406,  => 0.6118
recalls (weighed): 0.4279, 0.4901, 0.4742, 0.5059,  => 0.4745
f1s (weighed): 0.4698, 0.5377, 0.5188, 0.6010,  => 0.5318

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.41it/s]
Evaluation, Loss: 122.1861,

accs: 0.7112, 0.6896, 0.7278, 0.8531,  => 0.7454

precs (macro): 0.2450, 0.7240, 0.5841, 0.2167,  => 0.4424
recalls (macro): 0.3310, 0.6149, 0.5303, 0.2493,  => 0.4314
f1s (macro): 0.2816, 0.6014, 0.5416, 0.2318,  => 0.4141

precs (micro): 0.7112, 0.6896, 0.7278, 0.8531,  => 0.7454
recalls (micro): 0.7112, 0.6896, 0.7278, 0.8531,  => 0.7454
f1s (micro): 0.7112, 0.6896, 0.7278, 0.8531,  => 0.7454

precs (weighed): 0.5264, 0.7132, 0.7107, 0.7416,  => 0.6730
recalls (weighed): 0.7112, 0.6896, 0.7278, 0.8531,  => 0.7454
f1s (weighed): 0.6050, 0.6436, 0.7091, 0.7935,  => 0.6878

Saved the best model to path: ./models/task_2/cnn_distilbert-base-multilingual-cased_13.pth

Epoch 15/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [00:44<00:00,  5.89it/s]
Epoch 15/20, Loss: 1056.9738,

accs: 0.4358, 0.4977, 0.4666, 0.5124,  => 0.4781

precs (macro): 0.1859, 0.2952, 0.4738, 0.2202,  => 0.2938
recalls (macro): 0.2390, 0.2413, 0.4044, 0.2133,  => 0.2745
f1s (macro): 0.1694, 0.2655, 0.4143, 0.1791,  => 0.2571

precs (micro): 0.4358, 0.4977, 0.4666, 0.5124,  => 0.4781
recalls (micro): 0.4358, 0.4977, 0.4666, 0.5124,  => 0.4781
f1s (micro): 0.4358, 0.4977, 0.4666, 0.5124,  => 0.4781

precs (weighed): 0.5295, 0.6124, 0.5833, 0.7476,  => 0.6182
recalls (weighed): 0.4358, 0.4977, 0.4666, 0.5124,  => 0.4781
f1s (weighed): 0.4760, 0.5491, 0.5135, 0.6079,  => 0.5366

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.53it/s]
Evaluation, Loss: 123.8969,

accs: 0.7154, 0.6822, 0.7195, 0.8531,  => 0.7425

precs (macro): 0.2413, 0.6683, 0.5502, 0.2161,  => 0.4190
recalls (macro): 0.3329, 0.6320, 0.5552, 0.2493,  => 0.4424
f1s (macro): 0.2798, 0.6333, 0.5495, 0.2315,  => 0.4235

precs (micro): 0.7154, 0.6822, 0.7195, 0.8531,  => 0.7425
recalls (micro): 0.7154, 0.6822, 0.7195, 0.8531,  => 0.7425
f1s (micro): 0.7154, 0.6822, 0.7195, 0.8531,  => 0.7425

precs (weighed): 0.5183, 0.6745, 0.6978, 0.7397,  => 0.6576
recalls (weighed): 0.7154, 0.6822, 0.7195, 0.8531,  => 0.7425
f1s (weighed): 0.6011, 0.6634, 0.7051, 0.7924,  => 0.6905


Epoch 16/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [00:44<00:00,  5.94it/s]
Epoch 16/20, Loss: 1062.9618,

accs: 0.4358, 0.4862, 0.4697, 0.5033,  => 0.4737

precs (macro): 0.1866, 0.2889, 0.4714, 0.2201,  => 0.2918
recalls (macro): 0.2560, 0.2352, 0.4050, 0.2106,  => 0.2767
f1s (macro): 0.1702, 0.2592, 0.4137, 0.1772,  => 0.2551

precs (micro): 0.4358, 0.4862, 0.4697, 0.5033,  => 0.4737
recalls (micro): 0.4358, 0.4862, 0.4697, 0.5033,  => 0.4737
f1s (micro): 0.4358, 0.4862, 0.4697, 0.5033,  => 0.4737

precs (weighed): 0.5305, 0.6001, 0.5851, 0.7474,  => 0.6158
recalls (weighed): 0.4358, 0.4862, 0.4697, 0.5033,  => 0.4737
f1s (weighed): 0.4761, 0.5371, 0.5158, 0.6014,  => 0.5326

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.53it/s]
Evaluation, Loss: 130.1516,

accs: 0.7120, 0.6871, 0.7129, 0.8448,  => 0.7392

precs (macro): 0.2438, 0.6704, 0.5115, 0.2276,  => 0.4133
recalls (macro): 0.3314, 0.6703, 0.5683, 0.2468,  => 0.4542
f1s (macro): 0.2809, 0.6703, 0.5331, 0.2369,  => 0.4303

precs (micro): 0.7120, 0.6871, 0.7129, 0.8448,  => 0.7392
recalls (micro): 0.7120, 0.6871, 0.7129, 0.8448,  => 0.7392
f1s (micro): 0.7120, 0.6871, 0.7129, 0.8448,  => 0.7392

precs (weighed): 0.5239, 0.6870, 0.6875, 0.7791,  => 0.6694
recalls (weighed): 0.7120, 0.6871, 0.7129, 0.8448,  => 0.7392
f1s (weighed): 0.6036, 0.6871, 0.6961, 0.8106,  => 0.6994


Epoch 17/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [00:44<00:00,  5.91it/s]
Epoch 17/20, Loss: 1058.7850,

accs: 0.4343, 0.4935, 0.4715, 0.5075,  => 0.4767

precs (macro): 0.1876, 0.2895, 0.4761, 0.2196,  => 0.2932
recalls (macro): 0.2611, 0.2384, 0.4127, 0.3368,  => 0.3123
f1s (macro): 0.1704, 0.2614, 0.4197, 0.1781,  => 0.2574

precs (micro): 0.4343, 0.4935, 0.4715, 0.5075,  => 0.4767
recalls (micro): 0.4343, 0.4935, 0.4715, 0.5075,  => 0.4767
f1s (micro): 0.4343, 0.4935, 0.4715, 0.5075,  => 0.4767

precs (weighed): 0.5330, 0.6027, 0.5912, 0.7453,  => 0.6181
recalls (weighed): 0.4343, 0.4935, 0.4715, 0.5075,  => 0.4767
f1s (weighed): 0.4760, 0.5426, 0.5196, 0.6036,  => 0.5354

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.43it/s]
Evaluation, Loss: 120.1054,

accs: 0.7137, 0.7029, 0.7386, 0.8548,  => 0.7525

precs (macro): 0.2415, 0.6901, 0.5659, 0.2144,  => 0.4280
recalls (macro): 0.3322, 0.6619, 0.5648, 0.2498,  => 0.4522
f1s (macro): 0.2797, 0.6663, 0.5637, 0.2307,  => 0.4351

precs (micro): 0.7137, 0.7029, 0.7386, 0.8548,  => 0.7525
recalls (micro): 0.7137, 0.7029, 0.7386, 0.8548,  => 0.7525
f1s (micro): 0.7137, 0.7029, 0.7386, 0.8548,  => 0.7525

precs (weighed): 0.5189, 0.6967, 0.7059, 0.7338,  => 0.6638
recalls (weighed): 0.7137, 0.7029, 0.7386, 0.8548,  => 0.7525
f1s (weighed): 0.6009, 0.6912, 0.7206, 0.7897,  => 0.7006

Saved the best model to path: ./models/task_2/cnn_distilbert-base-multilingual-cased_16.pth

Epoch 18/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [00:44<00:00,  5.89it/s]
Epoch 18/20, Loss: 1056.6847,

accs: 0.4319, 0.4948, 0.4677, 0.5084,  => 0.4757

precs (macro): 0.1858, 0.2979, 0.4735, 0.2218,  => 0.2947
recalls (macro): 0.2461, 0.3230, 0.4025, 0.2121,  => 0.2959
f1s (macro): 0.1688, 0.2658, 0.4156, 0.1788,  => 0.2573

precs (micro): 0.4319, 0.4948, 0.4677, 0.5084,  => 0.4757
recalls (micro): 0.4319, 0.4948, 0.4677, 0.5084,  => 0.4757
f1s (micro): 0.4319, 0.4948, 0.4677, 0.5084,  => 0.4757

precs (weighed): 0.5287, 0.6170, 0.5913, 0.7531,  => 0.6225
recalls (weighed): 0.4319, 0.4948, 0.4677, 0.5084,  => 0.4757
f1s (weighed): 0.4731, 0.5491, 0.5179, 0.6069,  => 0.5368

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.46it/s]
Evaluation, Loss: 126.2793,

accs: 0.6971, 0.6880, 0.6714, 0.8556,  => 0.7280

precs (macro): 0.2506, 0.6713, 0.5623, 0.2146,  => 0.4247
recalls (macro): 0.3544, 0.6713, 0.4379, 0.2500,  => 0.4284
f1s (macro): 0.2880, 0.6713, 0.4469, 0.2310,  => 0.4093

precs (micro): 0.6971, 0.6880, 0.6714, 0.8556,  => 0.7280
recalls (micro): 0.6971, 0.6880, 0.6714, 0.8556,  => 0.7280
f1s (micro): 0.6971, 0.6880, 0.6714, 0.8556,  => 0.7280

precs (weighed): 0.5277, 0.6880, 0.6664, 0.7345,  => 0.6541
recalls (weighed): 0.6971, 0.6880, 0.6714, 0.8556,  => 0.7280
f1s (weighed): 0.6005, 0.6880, 0.6315, 0.7904,  => 0.6776


Epoch 19/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [00:47<00:00,  5.55it/s]
Epoch 19/20, Loss: 1061.4290,

accs: 0.4231, 0.4836, 0.4678, 0.5126,  => 0.4718

precs (macro): 0.1848, 0.2873, 0.4766, 0.2225,  => 0.2928
recalls (macro): 0.2487, 0.2337, 0.4051, 0.2134,  => 0.2752
f1s (macro): 0.1666, 0.2577, 0.4158, 0.1800,  => 0.2550

precs (micro): 0.4231, 0.4836, 0.4678, 0.5126,  => 0.4718
recalls (micro): 0.4231, 0.4836, 0.4678, 0.5126,  => 0.4718
f1s (micro): 0.4231, 0.4836, 0.4678, 0.5126,  => 0.4718

precs (weighed): 0.5256, 0.5981, 0.5901, 0.7556,  => 0.6174
recalls (weighed): 0.4231, 0.4836, 0.4678, 0.5126,  => 0.4718
f1s (weighed): 0.4664, 0.5347, 0.5169, 0.6108,  => 0.5322

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:06<00:00,  5.96it/s]
Evaluation, Loss: 123.8882,

accs: 0.7154, 0.7054, 0.7344, 0.8481,  => 0.7508

precs (macro): 0.2408, 0.7167, 0.5653, 0.2235,  => 0.4366
recalls (macro): 0.3329, 0.6447, 0.5579, 0.2478,  => 0.4458
f1s (macro): 0.2795, 0.6441, 0.5592, 0.2351,  => 0.4295

precs (micro): 0.7154, 0.7054, 0.7344, 0.8481,  => 0.7508
recalls (micro): 0.7154, 0.7054, 0.7344, 0.8481,  => 0.7508
f1s (micro): 0.7154, 0.7054, 0.7344, 0.8481,  => 0.7508

precs (weighed): 0.5175, 0.7125, 0.7015, 0.7650,  => 0.6741
recalls (weighed): 0.7154, 0.7054, 0.7344, 0.8481,  => 0.7508
f1s (weighed): 0.6005, 0.6773, 0.7154, 0.8044,  => 0.6994


Epoch 20/20, Batch 264/264: 100%|████████████████████████████████████████████████████| 264/264 [00:48<00:00,  5.43it/s]
Epoch 20/20, Loss: 1050.2442,

accs: 0.4256, 0.4969, 0.4781, 0.5194,  => 0.4800

precs (macro): 0.1849, 0.2941, 0.4880, 0.2211,  => 0.2970
recalls (macro): 0.2468, 0.2400, 0.4132, 0.2778,  => 0.2945
f1s (macro): 0.1671, 0.2643, 0.4266, 0.1810,  => 0.2597

precs (micro): 0.4256, 0.4969, 0.4781, 0.5194,  => 0.4800
recalls (micro): 0.4256, 0.4969, 0.4781, 0.5194,  => 0.4800
f1s (micro): 0.4256, 0.4969, 0.4781, 0.5194,  => 0.4800

precs (weighed): 0.5260, 0.6109, 0.5985, 0.7504,  => 0.6214
recalls (weighed): 0.4256, 0.4969, 0.4781, 0.5194,  => 0.4800
f1s (weighed): 0.4681, 0.5480, 0.5269, 0.6137,  => 0.5392

Evaluation, Batch 38/38: 100%|█████████████████████████████████████████████████████████| 38/38 [00:06<00:00,  5.96it/s]
Evaluation, Loss: 126.1371,

accs: 0.7145, 0.7046, 0.6979, 0.8506,  => 0.7419

precs (macro): 0.2406, 0.6945, 0.5790, 0.2244,  => 0.4346
recalls (macro): 0.3326, 0.6601, 0.4757, 0.2485,  => 0.4292
f1s (macro): 0.2792, 0.6644, 0.4815, 0.2358,  => 0.4152

precs (micro): 0.7145, 0.7046, 0.6979, 0.8506,  => 0.7419
recalls (micro): 0.7145, 0.7046, 0.6979, 0.8506,  => 0.7419
f1s (micro): 0.7145, 0.7046, 0.6979, 0.8506,  => 0.7419

precs (weighed): 0.5169, 0.6993, 0.6987, 0.7679,  => 0.6707
recalls (weighed): 0.7145, 0.7046, 0.6979, 0.8506,  => 0.7419
f1s (weighed): 0.5998, 0.6905, 0.6702, 0.8072,  => 0.6919


Evaluating CNN
cnn + vinai/phobert-base

[16:49:02] task: task-2                                                                                   my_import.py:133
           model_type: cnn                                                                                my_import.py:133
           model_name: vinai/phobert-base                                                                 my_import.py:133
           padding_len: 256                                                                               my_import.py:131
           batch_size: 32                                                                                 my_import.py:133
           learning_rate: 0.001                                                                           my_import.py:133
           epochs: 10                                                                                     my_import.py:133
           fine_tune: True                                                                                my_import.py:133
           num_channels: 64                                                                               my_import.py:133
           kernel_size: 64                                                                                my_import.py:133
           padding: 64                                                                                    my_import.py:133
           device: cuda                                                                                   my_import.py:133
           saving_path: ./models/task_2/cnn_phobert-base                                                  my_import.py:133
           train_shape: (8437, 27)                                                                        my_import.py:133
           dev_shape: (1205, 27)                                                                          my_import.py:133
           test_shape: (2412, 27)                                                                         my_import.py:133

model_weight_path: ./models/task_2/cnn_phobert-base_9.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:15<00:00,  2.38it/s]
Evaluation, Loss: 125.9645,

accs: 0.7336, 0.6598, 0.6274, 0.8556,  => 0.7191

precs (macro): 0.5165, 0.6836, 0.3119, 0.2852,  => 0.4493
recalls (macro): 0.3582, 0.5760, 0.3656, 0.3333,  => 0.4083
f1s (macro): 0.3302, 0.5451, 0.3308, 0.3074,  => 0.3784

precs (micro): 0.7336, 0.6598, 0.6274, 0.8556,  => 0.7191
recalls (micro): 0.7336, 0.6598, 0.6274, 0.8556,  => 0.7191
f1s (micro): 0.7336, 0.6598, 0.6274, 0.8556,  => 0.7191

precs (weighed): 0.7484, 0.6767, 0.5098, 0.7321,  => 0.6668
recalls (weighed): 0.7336, 0.6598, 0.6274, 0.8556,  => 0.7191
f1s (weighed): 0.6440, 0.5964, 0.5536, 0.7890,  => 0.6458

Confusion Matrix of title aspect
[[  0  11   0]
 [  0 857   6]
 [  0 304  27]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.73      0.99      0.84       863
           2       0.82      0.08      0.15       331

    accuracy                           0.73      1205
   macro avg       0.52      0.36      0.33      1205
weighted avg       0.75      0.73      0.64      1205

Confusion Matrix of desc aspect
[[700  38]
 [372  95]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.65      0.95      0.77       738
           2       0.71      0.20      0.32       467

    accuracy                           0.66      1205
   macro avg       0.68      0.58      0.55      1205
weighted avg       0.68      0.66      0.60      1205

Confusion Matrix of company aspect
[[  0  33  23   0]
 [  0 546  39   0]
 [  0 187 210   0]
 [  1  96  70   0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.63      0.93      0.75       585
           2       0.61      0.53      0.57       397
           3       0.00      0.00      0.00       167

    accuracy                           0.63      1205
   macro avg       0.31      0.37      0.33      1205
weighted avg       0.51      0.63      0.55      1205

Confusion Matrix of other aspect
[[1031    0    0]
 [ 137    0    0]
 [  37    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.86      1.00      0.92      1031
           2       0.00      0.00      0.00       137
           3       0.00      0.00      0.00        37

    accuracy                           0.86      1205
   macro avg       0.29      0.33      0.31      1205
weighted avg       0.73      0.86      0.79      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:10<00:00,  7.33it/s]
Evaluation, Loss: 249.3061,

accs: 0.7264, 0.6729, 0.6223, 0.8520,  => 0.7184

precs (macro): 0.3631, 0.4653, 0.3064, 0.2840,  => 0.3547
recalls (macro): 0.2673, 0.3840, 0.3487, 0.3333,  => 0.3333
f1s (macro): 0.2455, 0.3646, 0.3201, 0.3067,  => 0.3092

precs (micro): 0.7264, 0.6729, 0.6223, 0.8520,  => 0.7184
recalls (micro): 0.7264, 0.6729, 0.6223, 0.8520,  => 0.7184
f1s (micro): 0.7264, 0.6729, 0.6223, 0.8520,  => 0.7184

precs (weighed): 0.7163, 0.6894, 0.5155, 0.7259,  => 0.6618
recalls (weighed): 0.7264, 0.6729, 0.6223, 0.8520,  => 0.7184
f1s (weighed): 0.6362, 0.6075, 0.5545, 0.7839,  => 0.6455

Confusion Matrix of title aspect
[[   0   28    1    0]
 [   0 1699   19    0]
 [   0  608   53    0]
 [   0    4    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.73      0.99      0.84      1718
           2       0.73      0.08      0.14       661
           3       0.00      0.00      0.00         4

    accuracy                           0.73      2412
   macro avg       0.36      0.27      0.25      2412
weighted avg       0.72      0.73      0.64      2412

Confusion Matrix of desc aspect
[[   0    2    0]
 [   0 1448   65]
 [   0  722  175]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.67      0.96      0.79      1513
           2       0.73      0.20      0.31       897

    accuracy                           0.67      2412
   macro avg       0.47      0.38      0.36      2412
weighted avg       0.69      0.67      0.61      2412

Confusion Matrix of company aspect
[[   0   65   34    0]
 [   0 1124  111    0]
 [   0  401  377    0]
 [   2  183  115    0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.63      0.91      0.75      1235
           2       0.59      0.48      0.53       778
           3       0.00      0.00      0.00       300

    accuracy                           0.62      2412
   macro avg       0.31      0.35      0.32      2412
weighted avg       0.52      0.62      0.55      2412

Confusion Matrix of other aspect
[[2055    0    0]
 [ 262    0    0]
 [  95    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.85      1.00      0.92      2055
           2       0.00      0.00      0.00       262
           3       0.00      0.00      0.00        95

    accuracy                           0.85      2412
   macro avg       0.28      0.33      0.31      2412
weighted avg       0.73      0.85      0.78      2412

cnn + uitnlp/visobert

[16:49:48] task: task-2                                                                                   my_import.py:133
           model_type: cnn                                                                                my_import.py:133
           model_name: uitnlp/visobert                                                                    my_import.py:133
           padding_len: 512                                                                               my_import.py:131
           batch_size: 32                                                                                 my_import.py:133
           learning_rate: 0.001                                                                           my_import.py:133
           epochs: 10                                                                                     my_import.py:133
           fine_tune: True                                                                                my_import.py:133
           num_channels: 64                                                                               my_import.py:133
           kernel_size: 64                                                                                my_import.py:133
           padding: 64                                                                                    my_import.py:133
           device: cuda                                                                                   my_import.py:133
           saving_path: ./models/task_2/cnn_visobert                                                      my_import.py:133
           train_shape: (8437, 27)                                                                        my_import.py:133
           dev_shape: (1205, 27)                                                                          my_import.py:133
           test_shape: (2412, 27)                                                                         my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/visobert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

model_weight_path: ./models/task_2/cnn_visobert_5.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:15<00:00,  2.45it/s]
Evaluation, Loss: 121.0347,

accs: 0.7311, 0.6913, 0.6398, 0.8556,  => 0.7295

precs (macro): 0.4735, 0.6954, 0.3310, 0.2852,  => 0.4463
recalls (macro): 0.3608, 0.6292, 0.3837, 0.3333,  => 0.4268
f1s (macro): 0.3379, 0.6261, 0.3533, 0.3074,  => 0.4062

precs (micro): 0.7311, 0.6913, 0.6398, 0.8556,  => 0.7295
recalls (micro): 0.7311, 0.6913, 0.6398, 0.8556,  => 0.7295
f1s (micro): 0.7311, 0.6913, 0.6398, 0.8556,  => 0.7295

precs (weighed): 0.7138, 0.6939, 0.5332, 0.7321,  => 0.6682
recalls (weighed): 0.7311, 0.6913, 0.6398, 0.8556,  => 0.7295
f1s (weighed): 0.6491, 0.6612, 0.5794, 0.7890,  => 0.6697

Confusion Matrix of title aspect
[[  0  11   0]
 [  0 848  15]
 [  0 298  33]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.73      0.98      0.84       863
           2       0.69      0.10      0.17       331

    accuracy                           0.73      1205
   macro avg       0.47      0.36      0.34      1205
weighted avg       0.71      0.73      0.65      1205

Confusion Matrix of desc aspect
[[668  70]
 [302 165]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.69      0.91      0.78       738
           2       0.70      0.35      0.47       467

    accuracy                           0.69      1205
   macro avg       0.70      0.63      0.63      1205
weighted avg       0.69      0.69      0.66      1205

Confusion Matrix of company aspect
[[  1  25  30   0]
 [  1 522  62   0]
 [  2 147 248   0]
 [ 28  75  64   0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.03      0.02      0.02        56
           1       0.68      0.89      0.77       585
           2       0.61      0.62      0.62       397
           3       0.00      0.00      0.00       167

    accuracy                           0.64      1205
   macro avg       0.33      0.38      0.35      1205
weighted avg       0.53      0.64      0.58      1205

Confusion Matrix of other aspect
[[1031    0    0]
 [ 137    0    0]
 [  37    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.86      1.00      0.92      1031
           2       0.00      0.00      0.00       137
           3       0.00      0.00      0.00        37

    accuracy                           0.86      1205
   macro avg       0.29      0.33      0.31      1205
weighted avg       0.73      0.86      0.79      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:20<00:00,  3.70it/s]
Evaluation, Loss: 240.2289,

accs: 0.7301, 0.7023, 0.6335, 0.8520,  => 0.7295

precs (macro): 0.3736, 0.4665, 0.3251, 0.2840,  => 0.3623
recalls (macro): 0.2707, 0.4216, 0.3685, 0.3333,  => 0.3485
f1s (macro): 0.2516, 0.4216, 0.3434, 0.3067,  => 0.3308

precs (micro): 0.7301, 0.7023, 0.6335, 0.8520,  => 0.7295
recalls (micro): 0.7301, 0.7023, 0.6335, 0.8520,  => 0.7295
f1s (micro): 0.7301, 0.7023, 0.6335, 0.8520,  => 0.7295

precs (weighed): 0.7289, 0.7002, 0.5377, 0.7259,  => 0.6732
recalls (weighed): 0.7301, 0.7023, 0.6335, 0.8520,  => 0.7295
f1s (weighed): 0.6436, 0.6729, 0.5797, 0.7839,  => 0.6700

Confusion Matrix of title aspect
[[   0   29    0    0]
 [   0 1699   19    0]
 [   0  599   62    0]
 [   0    4    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.73      0.99      0.84      1718
           2       0.77      0.09      0.17       661
           3       0.00      0.00      0.00         4

    accuracy                           0.73      2412
   macro avg       0.37      0.27      0.25      2412
weighted avg       0.73      0.73      0.64      2412

Confusion Matrix of desc aspect
[[   0    1    1]
 [   0 1374  139]
 [   0  577  320]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.70      0.91      0.79      1513
           2       0.70      0.36      0.47       897

    accuracy                           0.70      2412
   macro avg       0.47      0.42      0.42      2412
weighted avg       0.70      0.70      0.67      2412

Confusion Matrix of company aspect
[[   2   43   54    0]
 [   1 1067  167    0]
 [   6  313  459    0]
 [  46  151  103    0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.04      0.02      0.03        99
           1       0.68      0.86      0.76      1235
           2       0.59      0.59      0.59       778
           3       0.00      0.00      0.00       300

    accuracy                           0.63      2412
   macro avg       0.33      0.37      0.34      2412
weighted avg       0.54      0.63      0.58      2412

Confusion Matrix of other aspect
[[2055    0    0]
 [ 262    0    0]
 [  95    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.85      1.00      0.92      2055
           2       0.00      0.00      0.00       262
           3       0.00      0.00      0.00        95

    accuracy                           0.85      2412
   macro avg       0.28      0.33      0.31      2412
weighted avg       0.73      0.85      0.78      2412

cnn + uitnlp/CafeBERT

[16:50:45] task: task-2                                                                                   my_import.py:133
           model_type: cnn                                                                                my_import.py:133
           model_name: uitnlp/CafeBERT                                                                    my_import.py:133
           padding_len: 512                                                                               my_import.py:131
           batch_size: 32                                                                                 my_import.py:133
           learning_rate: 0.001                                                                           my_import.py:133
           epochs: 10                                                                                     my_import.py:133
           fine_tune: True                                                                                my_import.py:133
           num_channels: 64                                                                               my_import.py:133
           kernel_size: 64                                                                                my_import.py:133
           padding: 64                                                                                    my_import.py:133
           device: cuda                                                                                   my_import.py:133
           saving_path: ./models/task_2/cnn_CafeBERT                                                      my_import.py:133
           train_shape: (8437, 27)                                                                        my_import.py:133
           dev_shape: (1205, 27)                                                                          my_import.py:133
           test_shape: (2412, 27)                                                                         my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/CafeBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

model_weight_path: ./models/task_2/cnn_CafeBERT_19.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:32<00:00,  1.16it/s]
Evaluation, Loss: 130.3870,

accs: 0.7411, 0.6556, 0.5336, 0.8556,  => 0.6965

precs (macro): 0.5213, 0.6577, 0.2590, 0.2852,  => 0.4308
recalls (macro): 0.3685, 0.5773, 0.2908, 0.3333,  => 0.3925
f1s (macro): 0.3491, 0.5536, 0.2439, 0.3074,  => 0.3635

precs (micro): 0.7411, 0.6556, 0.5336, 0.8556,  => 0.6965
recalls (micro): 0.7411, 0.6556, 0.5336, 0.8556,  => 0.6965
f1s (micro): 0.7411, 0.6556, 0.5336, 0.8556,  => 0.6965

precs (weighed): 0.7552, 0.6570, 0.4255, 0.7321,  => 0.6424
recalls (weighed): 0.7411, 0.6556, 0.5336, 0.8556,  => 0.6965
f1s (weighed): 0.6610, 0.6016, 0.4294, 0.7890,  => 0.6203

Confusion Matrix of title aspect
[[  0  11   0]
 [  0 855   8]
 [  0 293  38]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.74      0.99      0.85       863
           2       0.83      0.11      0.20       331

    accuracy                           0.74      1205
   macro avg       0.52      0.37      0.35      1205
weighted avg       0.76      0.74      0.66      1205

Confusion Matrix of desc aspect
[[683  55]
 [360 107]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.65      0.93      0.77       738
           2       0.66      0.23      0.34       467

    accuracy                           0.66      1205
   macro avg       0.66      0.58      0.55      1205
weighted avg       0.66      0.66      0.60      1205

Confusion Matrix of company aspect
[[  0  41  15   0]
 [  0 564  21   0]
 [  0 318  79   0]
 [  0 123  44   0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.54      0.96      0.69       585
           2       0.50      0.20      0.28       397
           3       0.00      0.00      0.00       167

    accuracy                           0.53      1205
   macro avg       0.26      0.29      0.24      1205
weighted avg       0.43      0.53      0.43      1205

Confusion Matrix of other aspect
[[1031    0    0]
 [ 137    0    0]
 [  37    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.86      1.00      0.92      1031
           2       0.00      0.00      0.00       137
           3       0.00      0.00      0.00        37

    accuracy                           0.86      1205
   macro avg       0.29      0.33      0.31      1205
weighted avg       0.73      0.86      0.79      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [01:01<00:00,  1.24it/s]
Evaluation, Loss: 258.3808,

accs: 0.7326, 0.6683, 0.5543, 0.8520,  => 0.7018

precs (macro): 0.3725, 0.4438, 0.2677, 0.2840,  => 0.3420
recalls (macro): 0.2732, 0.3857, 0.2878, 0.3333,  => 0.3200
f1s (macro): 0.2561, 0.3717, 0.2452, 0.3067,  => 0.2949

precs (micro): 0.7326, 0.6683, 0.5543, 0.8520,  => 0.7018
recalls (micro): 0.7326, 0.6683, 0.5543, 0.8520,  => 0.7018
f1s (micro): 0.7326, 0.6683, 0.5543, 0.8520,  => 0.7018

precs (weighed): 0.7289, 0.6661, 0.4515, 0.7259,  => 0.6431
recalls (weighed): 0.7326, 0.6683, 0.5543, 0.8520,  => 0.7018
f1s (weighed): 0.6492, 0.6136, 0.4507, 0.7839,  => 0.6244

Confusion Matrix of title aspect
[[   0   27    2    0]
 [   0 1698   20    0]
 [   0  592   69    0]
 [   0    4    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.73      0.99      0.84      1718
           2       0.76      0.10      0.18       661
           3       0.00      0.00      0.00         4

    accuracy                           0.73      2412
   macro avg       0.37      0.27      0.26      2412
weighted avg       0.73      0.73      0.65      2412

Confusion Matrix of desc aspect
[[   0    2    0]
 [   0 1410  103]
 [   0  695  202]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.67      0.93      0.78      1513
           2       0.66      0.23      0.34       897

    accuracy                           0.67      2412
   macro avg       0.44      0.39      0.37      2412
weighted avg       0.67      0.67      0.61      2412

Confusion Matrix of company aspect
[[   0   85   14    0]
 [   0 1193   42    0]
 [   0  634  144    0]
 [   0  218   82    0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.56      0.97      0.71      1235
           2       0.51      0.19      0.27       778
           3       0.00      0.00      0.00       300

    accuracy                           0.55      2412
   macro avg       0.27      0.29      0.25      2412
weighted avg       0.45      0.55      0.45      2412

Confusion Matrix of other aspect
[[2055    0    0]
 [ 262    0    0]
 [  95    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.85      1.00      0.92      2055
           2       0.00      0.00      0.00       262
           3       0.00      0.00      0.00        95

    accuracy                           0.85      2412
   macro avg       0.28      0.33      0.31      2412
weighted avg       0.73      0.85      0.78      2412

cnn + xlm-roberta-base

[16:52:38] task: task-2                                                                                   my_import.py:133
           model_type: cnn                                                                                my_import.py:133
           model_name: xlm-roberta-base                                                                   my_import.py:133
           padding_len: 512                                                                               my_import.py:131
           batch_size: 32                                                                                 my_import.py:133
           learning_rate: 0.001                                                                           my_import.py:133
           epochs: 10                                                                                     my_import.py:133
           fine_tune: True                                                                                my_import.py:133
           num_channels: 64                                                                               my_import.py:133
           kernel_size: 64                                                                                my_import.py:133
           padding: 64                                                                                    my_import.py:133
           device: cuda                                                                                   my_import.py:133
           saving_path: ./models/task_2/cnn_xlm-roberta-base                                              my_import.py:133
           train_shape: (8437, 27)                                                                        my_import.py:133
           dev_shape: (1205, 27)                                                                          my_import.py:133
           test_shape: (2412, 27)                                                                         my_import.py:133

model_weight_path: ./models/task_2/cnn_xlm-roberta-base_2.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:13<00:00,  2.85it/s]
Evaluation, Loss: 142.5175,

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423

Confusion Matrix of title aspect
[[  0  11   0]
 [  0 863   0]
 [  0 331   0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.72      1.00      0.83       863
           2       0.00      0.00      0.00       331

    accuracy                           0.72      1205
   macro avg       0.24      0.33      0.28      1205
weighted avg       0.51      0.72      0.60      1205

Confusion Matrix of desc aspect
[[738   0]
 [467   0]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.61      1.00      0.76       738
           2       0.00      0.00      0.00       467

    accuracy                           0.61      1205
   macro avg       0.31      0.50      0.38      1205
weighted avg       0.38      0.61      0.47      1205

Confusion Matrix of company aspect
[[  0  56   0   0]
 [  0 585   0   0]
 [  0 397   0   0]
 [  0 167   0   0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.49      1.00      0.65       585
           2       0.00      0.00      0.00       397
           3       0.00      0.00      0.00       167

    accuracy                           0.49      1205
   macro avg       0.12      0.25      0.16      1205
weighted avg       0.24      0.49      0.32      1205

Confusion Matrix of other aspect
[[1031    0    0]
 [ 137    0    0]
 [  37    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.86      1.00      0.92      1031
           2       0.00      0.00      0.00       137
           3       0.00      0.00      0.00        37

    accuracy                           0.86      1205
   macro avg       0.29      0.33      0.31      1205
weighted avg       0.73      0.86      0.79      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:20<00:00,  3.73it/s]
Evaluation, Loss: 284.0310,

accs: 0.7123, 0.6273, 0.5120, 0.8520,  => 0.6759

precs (macro): 0.1781, 0.2091, 0.1280, 0.2840,  => 0.1998
recalls (macro): 0.2500, 0.3333, 0.2500, 0.3333,  => 0.2917
f1s (macro): 0.2080, 0.2570, 0.1693, 0.3067,  => 0.2352

precs (micro): 0.7123, 0.6273, 0.5120, 0.8520,  => 0.6759
recalls (micro): 0.7123, 0.6273, 0.5120, 0.8520,  => 0.6759
f1s (micro): 0.7123, 0.6273, 0.5120, 0.8520,  => 0.6759

precs (weighed): 0.5073, 0.3935, 0.2622, 0.7259,  => 0.4722
recalls (weighed): 0.7123, 0.6273, 0.5120, 0.8520,  => 0.6759
f1s (weighed): 0.5926, 0.4836, 0.3468, 0.7839,  => 0.5517

Confusion Matrix of title aspect
[[   0   29    0    0]
 [   0 1718    0    0]
 [   0  661    0    0]
 [   0    4    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.71      1.00      0.83      1718
           2       0.00      0.00      0.00       661
           3       0.00      0.00      0.00         4

    accuracy                           0.71      2412
   macro avg       0.18      0.25      0.21      2412
weighted avg       0.51      0.71      0.59      2412

Confusion Matrix of desc aspect
[[   0    2    0]
 [   0 1513    0]
 [   0  897    0]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.63      1.00      0.77      1513
           2       0.00      0.00      0.00       897

    accuracy                           0.63      2412
   macro avg       0.21      0.33      0.26      2412
weighted avg       0.39      0.63      0.48      2412

Confusion Matrix of company aspect
[[   0   99    0    0]
 [   0 1235    0    0]
 [   0  778    0    0]
 [   0  300    0    0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.51      1.00      0.68      1235
           2       0.00      0.00      0.00       778
           3       0.00      0.00      0.00       300

    accuracy                           0.51      2412
   macro avg       0.13      0.25      0.17      2412
weighted avg       0.26      0.51      0.35      2412

Confusion Matrix of other aspect
[[2055    0    0]
 [ 262    0    0]
 [  95    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.85      1.00      0.92      2055
           2       0.00      0.00      0.00       262
           3       0.00      0.00      0.00        95

    accuracy                           0.85      2412
   macro avg       0.28      0.33      0.31      2412
weighted avg       0.73      0.85      0.78      2412

cnn + bert-base-multilingual-cased

[16:53:25] task: task-2                                                                                   my_import.py:133
           model_type: cnn                                                                                my_import.py:133
           model_name: bert-base-multilingual-cased                                                       my_import.py:133
           padding_len: 512                                                                               my_import.py:131
           batch_size: 32                                                                                 my_import.py:133
           learning_rate: 0.001                                                                           my_import.py:133
           epochs: 10                                                                                     my_import.py:133
           fine_tune: True                                                                                my_import.py:133
           num_channels: 64                                                                               my_import.py:133
           kernel_size: 64                                                                                my_import.py:133
           padding: 64                                                                                    my_import.py:133
           device: cuda                                                                                   my_import.py:133
           saving_path: ./models/task_2/cnn_bert-base-multilingual-cased                                  my_import.py:133
           train_shape: (8437, 27)                                                                        my_import.py:133
           dev_shape: (1205, 27)                                                                          my_import.py:133
           test_shape: (2412, 27)                                                                         my_import.py:133

model_weight_path: ./models/task_2/cnn_bert-base-multilingual-cased_3.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:12<00:00,  3.06it/s]
Evaluation, Loss: 142.7481,

accs: 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683

precs (macro): 0.2387, 0.7240, 0.1214, 0.2852,  => 0.3423
recalls (macro): 0.3333, 0.5047, 0.2500, 0.3333,  => 0.3553
f1s (macro): 0.2782, 0.3911, 0.1634, 0.3074,  => 0.2850

precs (micro): 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683
recalls (micro): 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683
f1s (micro): 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683

precs (weighed): 0.5129, 0.6994, 0.2357, 0.7321,  => 0.5450
recalls (weighed): 0.7162, 0.6158, 0.4855, 0.8556,  => 0.6683
f1s (weighed): 0.5977, 0.4742, 0.3173, 0.7890,  => 0.5446

Confusion Matrix of title aspect
[[  0  11   0]
 [  0 863   0]
 [  0 331   0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.72      1.00      0.83       863
           2       0.00      0.00      0.00       331

    accuracy                           0.72      1205
   macro avg       0.24      0.33      0.28      1205
weighted avg       0.51      0.72      0.60      1205

Confusion Matrix of desc aspect
[[737   1]
 [462   5]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.61      1.00      0.76       738
           2       0.83      0.01      0.02       467

    accuracy                           0.62      1205
   macro avg       0.72      0.50      0.39      1205
weighted avg       0.70      0.62      0.47      1205

Confusion Matrix of company aspect
[[  0  56   0   0]
 [  0 585   0   0]
 [  0 397   0   0]
 [  0 167   0   0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.49      1.00      0.65       585
           2       0.00      0.00      0.00       397
           3       0.00      0.00      0.00       167

    accuracy                           0.49      1205
   macro avg       0.12      0.25      0.16      1205
weighted avg       0.24      0.49      0.32      1205

Confusion Matrix of other aspect
[[1031    0    0]
 [ 137    0    0]
 [  37    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.86      1.00      0.92      1031
           2       0.00      0.00      0.00       137
           3       0.00      0.00      0.00        37

    accuracy                           0.86      1205
   macro avg       0.29      0.33      0.31      1205
weighted avg       0.73      0.86      0.79      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:19<00:00,  3.92it/s]
Evaluation, Loss: 283.9986,

accs: 0.7123, 0.6318, 0.5120, 0.8520,  => 0.6770

precs (macro): 0.1781, 0.4503, 0.1280, 0.2840,  => 0.2601
recalls (macro): 0.2500, 0.3385, 0.2500, 0.3333,  => 0.2930
f1s (macro): 0.2080, 0.2705, 0.1693, 0.3067,  => 0.2386

precs (micro): 0.7123, 0.6318, 0.5120, 0.8520,  => 0.6770
recalls (micro): 0.7123, 0.6318, 0.5120, 0.8520,  => 0.6770
f1s (micro): 0.7123, 0.6318, 0.5120, 0.8520,  => 0.6770

precs (weighed): 0.5073, 0.6635, 0.2622, 0.7259,  => 0.5397
recalls (weighed): 0.7123, 0.6318, 0.5120, 0.8520,  => 0.6770
f1s (weighed): 0.5926, 0.4990, 0.3468, 0.7839,  => 0.5556

Confusion Matrix of title aspect
[[   0   29    0    0]
 [   0 1718    0    0]
 [   0  661    0    0]
 [   0    4    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.71      1.00      0.83      1718
           2       0.00      0.00      0.00       661
           3       0.00      0.00      0.00         4

    accuracy                           0.71      2412
   macro avg       0.18      0.25      0.21      2412
weighted avg       0.51      0.71      0.59      2412

Confusion Matrix of desc aspect
[[   0    2    0]
 [   0 1506    7]
 [   0  879   18]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.63      1.00      0.77      1513
           2       0.72      0.02      0.04       897

    accuracy                           0.63      2412
   macro avg       0.45      0.34      0.27      2412
weighted avg       0.66      0.63      0.50      2412

Confusion Matrix of company aspect
[[   0   99    0    0]
 [   0 1235    0    0]
 [   0  778    0    0]
 [   0  300    0    0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.51      1.00      0.68      1235
           2       0.00      0.00      0.00       778
           3       0.00      0.00      0.00       300

    accuracy                           0.51      2412
   macro avg       0.13      0.25      0.17      2412
weighted avg       0.26      0.51      0.35      2412

Confusion Matrix of other aspect
[[2055    0    0]
 [ 262    0    0]
 [  95    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.85      1.00      0.92      2055
           2       0.00      0.00      0.00       262
           3       0.00      0.00      0.00        95

    accuracy                           0.85      2412
   macro avg       0.28      0.33      0.31      2412
weighted avg       0.73      0.85      0.78      2412

cnn + distilbert-base-multilingual-cased

[16:54:10] task: task-2                                                                                   my_import.py:133
           model_type: cnn                                                                                my_import.py:133
           model_name: distilbert-base-multilingual-cased                                                 my_import.py:133
           padding_len: 512                                                                               my_import.py:131
           batch_size: 32                                                                                 my_import.py:133
           learning_rate: 0.001                                                                           my_import.py:133
           epochs: 10                                                                                     my_import.py:133
           fine_tune: True                                                                                my_import.py:133
           num_channels: 64                                                                               my_import.py:133
           kernel_size: 64                                                                                my_import.py:133
           padding: 64                                                                                    my_import.py:133
           device: cuda                                                                                   my_import.py:133
           saving_path: ./models/task_2/cnn_distilbert-base-multilingual-cased                            my_import.py:133
           train_shape: (8437, 27)                                                                        my_import.py:133
           dev_shape: (1205, 27)                                                                          my_import.py:133
           test_shape: (2412, 27)                                                                         my_import.py:133

model_weight_path: ./models/task_2/cnn_distilbert-base-multilingual-cased_16.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|██████████| 38/38 [00:09<00:00,  4.18it/s]
Evaluation, Loss: 120.1054,

accs: 0.7137, 0.7029, 0.7386, 0.8548,  => 0.7525

precs (macro): 0.2415, 0.6901, 0.5659, 0.2144,  => 0.4280
recalls (macro): 0.3322, 0.6619, 0.5648, 0.2498,  => 0.4522
f1s (macro): 0.2797, 0.6663, 0.5637, 0.2307,  => 0.4351

precs (micro): 0.7137, 0.7029, 0.7386, 0.8548,  => 0.7525
recalls (micro): 0.7137, 0.7029, 0.7386, 0.8548,  => 0.7525
f1s (micro): 0.7137, 0.7029, 0.7386, 0.8548,  => 0.7525

precs (weighed): 0.5189, 0.6967, 0.7059, 0.7338,  => 0.6638
recalls (weighed): 0.7137, 0.7029, 0.7386, 0.8548,  => 0.7525
f1s (weighed): 0.6009, 0.6912, 0.7206, 0.7897,  => 0.7006

Confusion Matrix of title aspect
[[  0  11   0]
 [  3 860   0]
 [ 15 316   0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.72      1.00      0.84       863
           2       0.00      0.00      0.00       331

    accuracy                           0.71      1205
   macro avg       0.24      0.33      0.28      1205
weighted avg       0.52      0.71      0.60      1205

Confusion Matrix of desc aspect
[[623 115]
 [243 224]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.72      0.84      0.78       738
           2       0.66      0.48      0.56       467

    accuracy                           0.70      1205
   macro avg       0.69      0.66      0.67      1205
weighted avg       0.70      0.70      0.69      1205

Confusion Matrix of company aspect
[[  0  17  36   3]
 [  0 493  79  13]
 [  0 112 277   8]
 [  0  23  24 120]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.76      0.84      0.80       585
           2       0.67      0.70      0.68       397
           3       0.83      0.72      0.77       167

    accuracy                           0.74      1205
   macro avg       0.57      0.56      0.56      1205
weighted avg       0.71      0.74      0.72      1205

Confusion Matrix of other aspect
[[   0    0    0    0]
 [   1 1030    0    0]
 [   1  136    0    0]
 [   2   35    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.86      1.00      0.92      1031
           2       0.00      0.00      0.00       137
           3       0.00      0.00      0.00        37

    accuracy                           0.85      1205
   macro avg       0.21      0.25      0.23      1205
weighted avg       0.73      0.85      0.79      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|██████████| 76/76 [00:11<00:00,  6.76it/s]
Evaluation, Loss: 240.0733,

accs: 0.7090, 0.7160, 0.7164, 0.8516,  => 0.7482

precs (macro): 0.1807, 0.4661, 0.5489, 0.2133,  => 0.3522
recalls (macro): 0.2488, 0.4467, 0.5459, 0.2499,  => 0.3729
f1s (macro): 0.2094, 0.4507, 0.5462, 0.2302,  => 0.3591

precs (micro): 0.7090, 0.7160, 0.7164, 0.8516,  => 0.7482
recalls (micro): 0.7090, 0.7160, 0.7164, 0.8516,  => 0.7482
f1s (micro): 0.7090, 0.7160, 0.7164, 0.8516,  => 0.7482

precs (weighed): 0.5148, 0.7081, 0.6861, 0.7270,  => 0.6590
recalls (weighed): 0.7090, 0.7160, 0.7164, 0.8516,  => 0.7482
f1s (weighed): 0.5965, 0.7046, 0.6997, 0.7844,  => 0.6963

Confusion Matrix of title aspect
[[   0   29    0    0]
 [   8 1710    0    0]
 [  37  624    0    0]
 [   1    3    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.72      1.00      0.84      1718
           2       0.00      0.00      0.00       661
           3       0.00      0.00      0.00         4

    accuracy                           0.71      2412
   macro avg       0.18      0.25      0.21      2412
weighted avg       0.51      0.71      0.60      2412

Confusion Matrix of desc aspect
[[   0    2    0]
 [   0 1289  224]
 [   0  459  438]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.74      0.85      0.79      1513
           2       0.66      0.49      0.56       897

    accuracy                           0.72      2412
   macro avg       0.47      0.45      0.45      2412
weighted avg       0.71      0.72      0.70      2412

Confusion Matrix of company aspect
[[   0   35   57    7]
 [   0 1030  192   13]
 [   0  274  477   27]
 [   0   42   37  221]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.75      0.83      0.79      1235
           2       0.63      0.61      0.62       778
           3       0.82      0.74      0.78       300

    accuracy                           0.72      2412
   macro avg       0.55      0.55      0.55      2412
weighted avg       0.69      0.72      0.70      2412

Confusion Matrix of other aspect
[[   0    0    0    0]
 [   1 2054    0    0]
 [   4  258    0    0]
 [   0   95    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.85      1.00      0.92      2055
           2       0.00      0.00      0.00       262
           3       0.00      0.00      0.00        95

    accuracy                           0.85      2412
   macro avg       0.21      0.25      0.23      2412
weighted avg       0.73      0.85      0.78      2412
