Training LSTM
lstm + vinai/phobert-base

[15:59:44] task: task-2                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: vinai/phobert-base                                                              my_import.py:133
           padding_len: 256                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/lstm_phobert-base                                              my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

Training ...

Epoch 1/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 1/20, Loss: 1151.4834, 

accs: 0.5063, 0.4697, 0.3898, 0.5144,  => 0.4701

precs (macro): 0.2538, 0.2539, 0.2718, 0.2138,  => 0.2483
recalls (macro): 0.2624, 0.2976, 0.2631, 0.2764,  => 0.2749
f1s (macro): 0.2366, 0.2309, 0.2398, 0.1775,  => 0.2212

precs (micro): 0.5063, 0.4697, 0.3898, 0.5144,  => 0.4701
recalls (micro): 0.5063, 0.4697, 0.3898, 0.5144,  => 0.4701
f1s (micro): 0.5063, 0.4697, 0.3898, 0.5144,  => 0.4701

precs (weighed): 0.5966, 0.5353, 0.3960, 0.7257,  => 0.5634
recalls (weighed): 0.5063, 0.4697, 0.3898, 0.5144,  => 0.4701
f1s (weighed): 0.5468, 0.4971, 0.3806, 0.6019,  => 0.5066


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 136.9352, 

accs: 0.7162, 0.6274, 0.5992, 0.8556,  => 0.6996

precs (macro): 0.2387, 0.6693, 0.2910, 0.2852,  => 0.3710
recalls (macro): 0.3333, 0.5240, 0.3563, 0.3333,  => 0.3867
f1s (macro): 0.2782, 0.4408, 0.3202, 0.3074,  => 0.3367

precs (micro): 0.7162, 0.6274, 0.5992, 0.8556,  => 0.6996
recalls (micro): 0.7162, 0.6274, 0.5992, 0.8556,  => 0.6996
f1s (micro): 0.7162, 0.6274, 0.5992, 0.8556,  => 0.6996

precs (weighed): 0.5129, 0.6591, 0.4862, 0.7321,  => 0.5976
recalls (weighed): 0.7162, 0.6274, 0.5992, 0.8556,  => 0.6996
f1s (weighed): 0.5977, 0.5135, 0.5367, 0.7890,  => 0.6092

Saved the best model to path: ./models/task_2/lstm_phobert-base_0.pth


Epoch 2/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 2/20, Loss: 1131.0181, 

accs: 0.4970, 0.4703, 0.4164, 0.5059,  => 0.4724

precs (macro): 0.2464, 0.2648, 0.2622, 0.2123,  => 0.2464
recalls (macro): 0.2460, 0.2221, 0.2894, 0.2114,  => 0.2422
f1s (macro): 0.2295, 0.2415, 0.2646, 0.1752,  => 0.2277

precs (micro): 0.4970, 0.4703, 0.4164, 0.5059,  => 0.4724
recalls (micro): 0.4970, 0.4703, 0.4164, 0.5059,  => 0.4724
f1s (micro): 0.4970, 0.4703, 0.4164, 0.5059,  => 0.4724

precs (weighed): 0.5868, 0.5569, 0.4279, 0.7209,  => 0.5731
recalls (weighed): 0.4970, 0.4703, 0.4164, 0.5059,  => 0.4724
f1s (weighed): 0.5373, 0.5099, 0.4195, 0.5945,  => 0.5153


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 136.2480, 

accs: 0.7162, 0.6332, 0.6191, 0.8556,  => 0.7060

precs (macro): 0.2387, 0.7418, 0.3008, 0.2852,  => 0.3916
recalls (macro): 0.3333, 0.5287, 0.3653, 0.3333,  => 0.3902
f1s (macro): 0.2782, 0.4439, 0.3289, 0.3074,  => 0.3396

precs (micro): 0.7162, 0.6332, 0.6191, 0.8556,  => 0.7060
recalls (micro): 0.7162, 0.6332, 0.6191, 0.8556,  => 0.7060
f1s (micro): 0.7162, 0.6332, 0.6191, 0.8556,  => 0.7060

precs (weighed): 0.5129, 0.7159, 0.4995, 0.7321,  => 0.6151
recalls (weighed): 0.7162, 0.6332, 0.6191, 0.8556,  => 0.7060
f1s (weighed): 0.5977, 0.5169, 0.5514, 0.7890,  => 0.6138

Saved the best model to path: ./models/task_2/lstm_phobert-base_1.pth


Epoch 3/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 3/20, Loss: 1118.3587, 

accs: 0.5053, 0.4797, 0.4114, 0.5054,  => 0.4754

precs (macro): 0.2541, 0.2714, 0.2701, 0.2109,  => 0.2516
recalls (macro): 0.2589, 0.2298, 0.2971, 0.2113,  => 0.2493
f1s (macro): 0.2359, 0.2489, 0.2679, 0.1746,  => 0.2318

precs (micro): 0.5053, 0.4797, 0.4114, 0.5054,  => 0.4754
recalls (micro): 0.5053, 0.4797, 0.4114, 0.5054,  => 0.4754
f1s (micro): 0.5053, 0.4797, 0.4114, 0.5054,  => 0.4754

precs (weighed): 0.5983, 0.5682, 0.4391, 0.7159,  => 0.5804
recalls (weighed): 0.5053, 0.4797, 0.4114, 0.5054,  => 0.4754
f1s (weighed): 0.5469, 0.5202, 0.4210, 0.5924,  => 0.5201


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 134.0257, 

accs: 0.7162, 0.6498, 0.6307, 0.8556,  => 0.7131

precs (macro): 0.2387, 0.6448, 0.3134, 0.2852,  => 0.3705
recalls (macro): 0.3333, 0.5714, 0.3808, 0.3333,  => 0.4047
f1s (macro): 0.2782, 0.5465, 0.3434, 0.3074,  => 0.3689

precs (micro): 0.7162, 0.6498, 0.6307, 0.8556,  => 0.7131
recalls (micro): 0.7162, 0.6498, 0.6307, 0.8556,  => 0.7131
f1s (micro): 0.7162, 0.6498, 0.6307, 0.8556,  => 0.7131

precs (weighed): 0.5129, 0.6464, 0.5259, 0.7321,  => 0.6043
recalls (weighed): 0.7162, 0.6498, 0.6307, 0.8556,  => 0.7131
f1s (weighed): 0.5977, 0.5952, 0.5729, 0.7890,  => 0.6387

Saved the best model to path: ./models/task_2/lstm_phobert-base_2.pth


Epoch 4/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 4/20, Loss: 1101.6598, 

accs: 0.5025, 0.4832, 0.4082, 0.5162,  => 0.4775

precs (macro): 0.2569, 0.2772, 0.2806, 0.2115,  => 0.2566
recalls (macro): 0.2450, 0.3158, 0.3079, 0.2769,  => 0.2864
f1s (macro): 0.2358, 0.2531, 0.2724, 0.1771,  => 0.2346

precs (micro): 0.5025, 0.4832, 0.4082, 0.5162,  => 0.4775
recalls (micro): 0.5025, 0.4832, 0.4082, 0.5162,  => 0.4775
f1s (micro): 0.5025, 0.4832, 0.4082, 0.5162,  => 0.4775

precs (weighed): 0.6033, 0.5786, 0.4536, 0.7180,  => 0.5884
recalls (weighed): 0.5025, 0.4832, 0.4082, 0.5162,  => 0.4775
f1s (weighed): 0.5476, 0.5265, 0.4247, 0.6004,  => 0.5248


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 128.3201, 

accs: 0.7162, 0.6631, 0.6183, 0.8556,  => 0.7133

precs (macro): 0.2387, 0.6411, 0.3102, 0.2852,  => 0.3688
recalls (macro): 0.3333, 0.6180, 0.3566, 0.3333,  => 0.4103
f1s (macro): 0.2782, 0.6192, 0.3225, 0.3074,  => 0.3818

precs (micro): 0.7162, 0.6631, 0.6183, 0.8556,  => 0.7133
recalls (micro): 0.7162, 0.6631, 0.6183, 0.8556,  => 0.7133
f1s (micro): 0.7162, 0.6631, 0.6183, 0.8556,  => 0.7133

precs (weighed): 0.5129, 0.6520, 0.5058, 0.7321,  => 0.6007
recalls (weighed): 0.7162, 0.6631, 0.6183, 0.8556,  => 0.7133
f1s (weighed): 0.5977, 0.6483, 0.5422, 0.7890,  => 0.6443

Saved the best model to path: ./models/task_2/lstm_phobert-base_3.pth


Epoch 5/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 5/20, Loss: 1096.2863, 

accs: 0.4998, 0.4864, 0.4172, 0.5086,  => 0.4780

precs (macro): 0.2581, 0.2792, 0.2814, 0.2115,  => 0.2576
recalls (macro): 0.2613, 0.3166, 0.3065, 0.2122,  => 0.2741
f1s (macro): 0.2372, 0.2544, 0.2748, 0.1754,  => 0.2355

precs (micro): 0.4998, 0.4864, 0.4172, 0.5086,  => 0.4780
recalls (micro): 0.4998, 0.4864, 0.4172, 0.5086,  => 0.4780
f1s (micro): 0.4998, 0.4864, 0.4172, 0.5086,  => 0.4780

precs (weighed): 0.6057, 0.5831, 0.4569, 0.7181,  => 0.5910
recalls (weighed): 0.4998, 0.4864, 0.4172, 0.5086,  => 0.4780
f1s (weighed): 0.5466, 0.5303, 0.4319, 0.5954,  => 0.5260


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 131.6799, 

accs: 0.7419, 0.6589, 0.6241, 0.8531,  => 0.7195

precs (macro): 0.4841, 0.6519, 0.3482, 0.2154,  => 0.4249
recalls (macro): 0.3770, 0.5879, 0.4015, 0.2493,  => 0.4039
f1s (macro): 0.3659, 0.5732, 0.3671, 0.2311,  => 0.3843

precs (micro): 0.7419, 0.6589, 0.6241, 0.8531,  => 0.7195
recalls (micro): 0.7419, 0.6589, 0.6241, 0.8531,  => 0.7195
f1s (micro): 0.7419, 0.6589, 0.6241, 0.8531,  => 0.7195

precs (weighed): 0.7274, 0.6542, 0.5518, 0.7373,  => 0.6677
recalls (weighed): 0.7419, 0.6589, 0.6241, 0.8531,  => 0.7195
f1s (weighed): 0.6745, 0.6162, 0.5783, 0.7910,  => 0.6650



Epoch 6/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 6/20, Loss: 1093.6191, 

accs: 0.5066, 0.4804, 0.4156, 0.5069,  => 0.4774

precs (macro): 0.2663, 0.2722, 0.2895, 0.2133,  => 0.2603
recalls (macro): 0.2608, 0.2295, 0.3185, 0.2117,  => 0.2551
f1s (macro): 0.2445, 0.2491, 0.2790, 0.1757,  => 0.2371

precs (micro): 0.5066, 0.4804, 0.4156, 0.5069,  => 0.4774
recalls (micro): 0.5066, 0.4804, 0.4156, 0.5069,  => 0.4774
f1s (micro): 0.5066, 0.4804, 0.4156, 0.5069,  => 0.4774

precs (weighed): 0.6190, 0.5706, 0.4682, 0.7241,  => 0.5955
recalls (weighed): 0.5066, 0.4804, 0.4156, 0.5069,  => 0.4774
f1s (weighed): 0.5560, 0.5216, 0.4346, 0.5963,  => 0.5271


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 127.8928, 

accs: 0.7212, 0.6614, 0.6415, 0.8556,  => 0.7199

precs (macro): 0.4903, 0.6821, 0.3568, 0.2852,  => 0.4536
recalls (macro): 0.3412, 0.5793, 0.4009, 0.3333,  => 0.4137
f1s (macro): 0.2964, 0.5513, 0.3741, 0.3074,  => 0.3823

precs (micro): 0.7212, 0.6614, 0.6415, 0.8556,  => 0.7199
recalls (micro): 0.7212, 0.6614, 0.6415, 0.8556,  => 0.7199
f1s (micro): 0.7212, 0.6614, 0.6415, 0.8556,  => 0.7199

precs (weighed): 0.7223, 0.6761, 0.5438, 0.7321,  => 0.6686
recalls (weighed): 0.7212, 0.6614, 0.6415, 0.8556,  => 0.7199
f1s (weighed): 0.6136, 0.6013, 0.5836, 0.7890,  => 0.6469

Saved the best model to path: ./models/task_2/lstm_phobert-base_5.pth


Epoch 7/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 7/20, Loss: 1079.4687, 

accs: 0.5063, 0.4867, 0.4138, 0.5223,  => 0.4823

precs (macro): 0.2710, 0.2784, 0.2923, 0.2155,  => 0.2643
recalls (macro): 0.2620, 0.3158, 0.3179, 0.2163,  => 0.2780
f1s (macro): 0.2470, 0.2536, 0.2794, 0.1796,  => 0.2399

precs (micro): 0.5063, 0.4867, 0.4138, 0.5223,  => 0.4823
recalls (micro): 0.5063, 0.4867, 0.4138, 0.5223,  => 0.4823
f1s (micro): 0.5063, 0.4867, 0.4138, 0.5223,  => 0.4823

precs (weighed): 0.6251, 0.5823, 0.4713, 0.7317,  => 0.6026
recalls (weighed): 0.5063, 0.4867, 0.4138, 0.5223,  => 0.4823
f1s (weighed): 0.5583, 0.5301, 0.4349, 0.6095,  => 0.5332


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 127.7652, 

accs: 0.7378, 0.6938, 0.6349, 0.8556,  => 0.7305

precs (macro): 0.5199, 0.6768, 0.3527, 0.2153,  => 0.4412
recalls (macro): 0.3639, 0.6580, 0.4005, 0.2500,  => 0.4181
f1s (macro): 0.3407, 0.6619, 0.3734, 0.2314,  => 0.4018

precs (micro): 0.7378, 0.6938, 0.6349, 0.8556,  => 0.7305
recalls (micro): 0.7378, 0.6938, 0.6349, 0.8556,  => 0.7305
f1s (micro): 0.7378, 0.6938, 0.6349, 0.8556,  => 0.7305

precs (weighed): 0.7528, 0.6866, 0.5366, 0.7369,  => 0.6783
recalls (weighed): 0.7378, 0.6938, 0.6349, 0.8556,  => 0.7305
f1s (weighed): 0.6535, 0.6852, 0.5799, 0.7919,  => 0.6776

Saved the best model to path: ./models/task_2/lstm_phobert-base_6.pth


Epoch 8/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 8/20, Loss: 1088.6073, 

accs: 0.5049, 0.4835, 0.4362, 0.5007,  => 0.4813

precs (macro): 0.2656, 0.2817, 0.4307, 0.2127,  => 0.2977
recalls (macro): 0.2678, 0.2329, 0.3436, 0.2723,  => 0.2792
f1s (macro): 0.2454, 0.2549, 0.3348, 0.1743,  => 0.2524

precs (micro): 0.5049, 0.4835, 0.4362, 0.5007,  => 0.4813
recalls (micro): 0.5049, 0.4835, 0.4362, 0.5007,  => 0.4813
f1s (micro): 0.5049, 0.4835, 0.4362, 0.5007,  => 0.4813

precs (weighed): 0.6141, 0.5879, 0.5518, 0.7219,  => 0.6189
recalls (weighed): 0.5049, 0.4835, 0.4362, 0.5007,  => 0.4813
f1s (weighed): 0.5527, 0.5305, 0.4707, 0.5911,  => 0.5363


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 127.3487, 

accs: 0.7212, 0.6788, 0.7087, 0.8398,  => 0.7371

precs (macro): 0.4903, 0.6927, 0.5585, 0.2206,  => 0.4905
recalls (macro): 0.3412, 0.6069, 0.5078, 0.2454,  => 0.4253
f1s (macro): 0.2964, 0.5943, 0.5197, 0.2323,  => 0.4107

precs (micro): 0.7212, 0.6788, 0.7087, 0.8398,  => 0.7371
recalls (micro): 0.7212, 0.6788, 0.7087, 0.8398,  => 0.7371
f1s (micro): 0.7212, 0.6788, 0.7087, 0.8398,  => 0.7371

precs (weighed): 0.7223, 0.6881, 0.6828, 0.7549,  => 0.7120
recalls (weighed): 0.7212, 0.6788, 0.7087, 0.8398,  => 0.7371
f1s (weighed): 0.6136, 0.6360, 0.6821, 0.7951,  => 0.6817

Saved the best model to path: ./models/task_2/lstm_phobert-base_7.pth


Epoch 9/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 9/20, Loss: 1074.3573, 

accs: 0.5043, 0.4995, 0.4560, 0.5068,  => 0.4916

precs (macro): 0.2680, 0.2884, 0.4421, 0.2126,  => 0.3028
recalls (macro): 0.2571, 0.3246, 0.3808, 0.2741,  => 0.3092
f1s (macro): 0.2444, 0.2630, 0.3877, 0.1756,  => 0.2677

precs (micro): 0.5043, 0.4995, 0.4560, 0.5068,  => 0.4916
recalls (micro): 0.5043, 0.4995, 0.4560, 0.5068,  => 0.4916
f1s (micro): 0.5043, 0.4995, 0.4560, 0.5068,  => 0.4916

precs (weighed): 0.6200, 0.6002, 0.5576, 0.7216,  => 0.6249
recalls (weighed): 0.5043, 0.4995, 0.4560, 0.5068,  => 0.4916
f1s (weighed): 0.5552, 0.5451, 0.4963, 0.5953,  => 0.5480


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 127.7171, 

accs: 0.7544, 0.6697, 0.7112, 0.8515,  => 0.7467

precs (macro): 0.5033, 0.7022, 0.5533, 0.2181,  => 0.4942
recalls (macro): 0.3933, 0.5880, 0.5187, 0.2488,  => 0.4372
f1s (macro): 0.3911, 0.5623, 0.5272, 0.2324,  => 0.4283

precs (micro): 0.7544, 0.6697, 0.7112, 0.8515,  => 0.7467
recalls (micro): 0.7544, 0.6697, 0.7112, 0.8515,  => 0.7467
f1s (micro): 0.7544, 0.6697, 0.7112, 0.8515,  => 0.7467

precs (weighed): 0.7477, 0.6927, 0.6819, 0.7465,  => 0.7172
recalls (weighed): 0.7544, 0.6697, 0.7112, 0.8515,  => 0.7467
f1s (weighed): 0.6977, 0.6110, 0.6867, 0.7955,  => 0.6977



Epoch 10/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 10/20, Loss: 1071.1719, 

accs: 0.5056, 0.5011, 0.4471, 0.5050,  => 0.4897

precs (macro): 0.2699, 0.2878, 0.4457, 0.2139,  => 0.3043
recalls (macro): 0.2506, 0.3243, 0.3717, 0.2736,  => 0.3051
f1s (macro): 0.2456, 0.2626, 0.3824, 0.1756,  => 0.2666

precs (micro): 0.5056, 0.5011, 0.4471, 0.5050,  => 0.4897
recalls (micro): 0.5056, 0.5011, 0.4471, 0.5050,  => 0.4897
f1s (micro): 0.5056, 0.5011, 0.4471, 0.5050,  => 0.4897

precs (weighed): 0.6248, 0.5995, 0.5581, 0.7259,  => 0.6271
recalls (weighed): 0.5056, 0.5011, 0.4471, 0.5050,  => 0.4897
f1s (weighed): 0.5579, 0.5458, 0.4906, 0.5955,  => 0.5475


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 122.8458, 

accs: 0.7535, 0.6631, 0.7253, 0.8531,  => 0.7488

precs (macro): 0.5350, 0.7207, 0.5603, 0.2158,  => 0.5079
recalls (macro): 0.3842, 0.5740, 0.5368, 0.2493,  => 0.4361
f1s (macro): 0.3753, 0.5347, 0.5417, 0.2313,  => 0.4208

precs (micro): 0.7535, 0.6631, 0.7253, 0.8531,  => 0.7488
recalls (micro): 0.7535, 0.6631, 0.7253, 0.8531,  => 0.7488
f1s (micro): 0.7535, 0.6631, 0.7253, 0.8531,  => 0.7488

precs (weighed): 0.7710, 0.7050, 0.6947, 0.7385,  => 0.7273
recalls (weighed): 0.7535, 0.6631, 0.7253, 0.8531,  => 0.7488
f1s (weighed): 0.6854, 0.5896, 0.7007, 0.7917,  => 0.6919

Saved the best model to path: ./models/task_2/lstm_phobert-base_9.pth


Epoch 11/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 11/20, Loss: 1074.3502, 

accs: 0.5039, 0.4875, 0.4459, 0.5119,  => 0.4873

precs (macro): 0.2680, 0.2820, 0.4462, 0.2138,  => 0.3025
recalls (macro): 0.2544, 0.3188, 0.3812, 0.3381,  => 0.3231
f1s (macro): 0.2442, 0.2568, 0.3853, 0.1771,  => 0.2659

precs (micro): 0.5039, 0.4875, 0.4459, 0.5119,  => 0.4873
recalls (micro): 0.5039, 0.4875, 0.4459, 0.5119,  => 0.4873
f1s (micro): 0.5039, 0.4875, 0.4459, 0.5119,  => 0.4873

precs (weighed): 0.6205, 0.5884, 0.5590, 0.7256,  => 0.6234
recalls (weighed): 0.5039, 0.4875, 0.4459, 0.5119,  => 0.4873
f1s (weighed): 0.5551, 0.5330, 0.4900, 0.6000,  => 0.5445


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 124.6236, 

accs: 0.7527, 0.6838, 0.6830, 0.8548,  => 0.7436

precs (macro): 0.5109, 0.6850, 0.5754, 0.2153,  => 0.4967
recalls (macro): 0.3882, 0.6204, 0.4473, 0.2498,  => 0.4264
f1s (macro): 0.3828, 0.6155, 0.4569, 0.2313,  => 0.4216

precs (micro): 0.7527, 0.6838, 0.6830, 0.8548,  => 0.7436
recalls (micro): 0.7527, 0.6838, 0.6830, 0.8548,  => 0.7436
f1s (micro): 0.7527, 0.6838, 0.6830, 0.8548,  => 0.7436

precs (weighed): 0.7524, 0.6846, 0.6786, 0.7368,  => 0.7131
recalls (weighed): 0.7527, 0.6838, 0.6830, 0.8548,  => 0.7436
f1s (weighed): 0.6909, 0.6519, 0.6436, 0.7914,  => 0.6945



Epoch 12/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 12/20, Loss: 1071.0808, 

accs: 0.5041, 0.4959, 0.4557, 0.5131,  => 0.4922

precs (macro): 0.2735, 0.2848, 0.4542, 0.2125,  => 0.3062
recalls (macro): 0.2633, 0.2387, 0.3880, 0.2760,  => 0.2915
f1s (macro): 0.2472, 0.2597, 0.3934, 0.1768,  => 0.2693

precs (micro): 0.5041, 0.4959, 0.4557, 0.5131,  => 0.4922
recalls (micro): 0.5041, 0.4959, 0.4557, 0.5131,  => 0.4922
f1s (micro): 0.5041, 0.4959, 0.4557, 0.5131,  => 0.4922

precs (weighed): 0.6280, 0.5939, 0.5596, 0.7213,  => 0.6257
recalls (weighed): 0.5041, 0.4959, 0.4557, 0.5131,  => 0.4922
f1s (weighed): 0.5582, 0.5405, 0.4958, 0.5995,  => 0.5485


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 128.0182, 

accs: 0.7568, 0.6855, 0.7212, 0.8382,  => 0.7504

precs (macro): 0.5187, 0.6682, 0.5642, 0.2203,  => 0.4929
recalls (macro): 0.3926, 0.6445, 0.5234, 0.2449,  => 0.4514
f1s (macro): 0.3894, 0.6478, 0.5339, 0.2320,  => 0.4508

precs (micro): 0.7568, 0.6855, 0.7212, 0.8382,  => 0.7504
recalls (micro): 0.7568, 0.6855, 0.7212, 0.8382,  => 0.7504
f1s (micro): 0.7568, 0.6855, 0.7212, 0.8382,  => 0.7504

precs (weighed): 0.7601, 0.6773, 0.6924, 0.7541,  => 0.7210
recalls (weighed): 0.7568, 0.6855, 0.7212, 0.8382,  => 0.7504
f1s (weighed): 0.6972, 0.6737, 0.6982, 0.7939,  => 0.7158



Epoch 13/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 13/20, Loss: 1063.6509, 

accs: 0.5066, 0.4915, 0.4562, 0.5095,  => 0.4910

precs (macro): 0.2743, 0.2854, 0.4583, 0.2137,  => 0.3079
recalls (macro): 0.2543, 0.2367, 0.3827, 0.2125,  => 0.2716
f1s (macro): 0.2467, 0.2588, 0.3964, 0.1764,  => 0.2696

precs (micro): 0.5066, 0.4915, 0.4562, 0.5095,  => 0.4910
recalls (micro): 0.5066, 0.4915, 0.4562, 0.5095,  => 0.4910
f1s (micro): 0.5066, 0.4915, 0.4562, 0.5095,  => 0.4910

precs (weighed): 0.6299, 0.5948, 0.5696, 0.7255,  => 0.6299
recalls (weighed): 0.5066, 0.4915, 0.4562, 0.5095,  => 0.4910
f1s (weighed): 0.5607, 0.5382, 0.5016, 0.5986,  => 0.5498


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 121.1282, 

accs: 0.7519, 0.6755, 0.7303, 0.8556,  => 0.7533

precs (macro): 0.5330, 0.6570, 0.5844, 0.2141,  => 0.4971
recalls (macro): 0.3822, 0.6305, 0.5348, 0.2500,  => 0.4494
f1s (macro): 0.3721, 0.6326, 0.5468, 0.2306,  => 0.4455

precs (micro): 0.7519, 0.6755, 0.7303, 0.8556,  => 0.7533
recalls (micro): 0.7519, 0.6755, 0.7303, 0.8556,  => 0.7533
f1s (micro): 0.7519, 0.6755, 0.7303, 0.8556,  => 0.7533

precs (weighed): 0.7688, 0.6661, 0.7083, 0.7327,  => 0.7189
recalls (weighed): 0.7519, 0.6755, 0.7303, 0.8556,  => 0.7533
f1s (weighed): 0.6824, 0.6608, 0.7040, 0.7894,  => 0.7091

Saved the best model to path: ./models/task_2/lstm_phobert-base_12.pth


Epoch 14/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 14/20, Loss: 1069.4811, 

accs: 0.4954, 0.4924, 0.4730, 0.5049,  => 0.4914

precs (macro): 0.2719, 0.2880, 0.4584, 0.4624,  => 0.3702
recalls (macro): 0.2668, 0.3206, 0.3921, 0.3363,  => 0.3290
f1s (macro): 0.2455, 0.2604, 0.4034, 0.1757,  => 0.2712

precs (micro): 0.4954, 0.4924, 0.4730, 0.5049,  => 0.4914
recalls (micro): 0.4954, 0.4924, 0.4730, 0.5049,  => 0.4914
f1s (micro): 0.4954, 0.4924, 0.4730, 0.5049,  => 0.4914

precs (weighed): 0.6220, 0.5995, 0.5723, 0.8345,  => 0.6571
recalls (weighed): 0.4954, 0.4924, 0.4730, 0.5049,  => 0.4914
f1s (weighed): 0.5502, 0.5405, 0.5128, 0.5937,  => 0.5493


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 126.0875, 

accs: 0.7494, 0.6680, 0.7029, 0.8639,  => 0.7461

precs (macro): 0.4979, 0.6484, 0.5675, 0.5222,  => 0.5590
recalls (macro): 0.3867, 0.6197, 0.4967, 0.3661,  => 0.4673
f1s (macro): 0.3811, 0.6204, 0.5117, 0.3684,  => 0.4704

precs (micro): 0.7494, 0.6680, 0.7029, 0.8639,  => 0.7461
recalls (micro): 0.7494, 0.6680, 0.7029, 0.8639,  => 0.7461
f1s (micro): 0.7494, 0.6680, 0.7029, 0.8639,  => 0.7461

precs (weighed): 0.7413, 0.6575, 0.6833, 0.8211,  => 0.7258
recalls (weighed): 0.7494, 0.6680, 0.7029, 0.8639,  => 0.7461
f1s (weighed): 0.6884, 0.6506, 0.6784, 0.8133,  => 0.7077



Epoch 15/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 15/20, Loss: 1067.1095, 

accs: 0.5009, 0.4935, 0.4622, 0.5177,  => 0.4936

precs (macro): 0.2726, 0.2907, 0.4465, 0.3170,  => 0.3317
recalls (macro): 0.2644, 0.2404, 0.3890, 0.1819,  => 0.2689
f1s (macro): 0.2474, 0.2629, 0.3985, 0.2261,  => 0.2837

precs (micro): 0.5009, 0.4935, 0.4622, 0.5177,  => 0.4936
recalls (micro): 0.5009, 0.4935, 0.4622, 0.5177,  => 0.4936
f1s (micro): 0.5009, 0.4935, 0.4622, 0.5177,  => 0.4936

precs (weighed): 0.6250, 0.6048, 0.5595, 0.7739,  => 0.6408
recalls (weighed): 0.5009, 0.4935, 0.4622, 0.5177,  => 0.4936
f1s (weighed): 0.5548, 0.5431, 0.5022, 0.6174,  => 0.5544


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 123.8865, 

accs: 0.7519, 0.6888, 0.7162, 0.8548,  => 0.7529

precs (macro): 0.5097, 0.6952, 0.5676, 0.4512,  => 0.5559
recalls (macro): 0.3872, 0.6244, 0.5266, 0.4343,  => 0.4931
f1s (macro): 0.3813, 0.6196, 0.5358, 0.4398,  => 0.4941

precs (micro): 0.7519, 0.6888, 0.7162, 0.8548,  => 0.7529
recalls (micro): 0.7519, 0.6888, 0.7162, 0.8548,  => 0.7529
f1s (micro): 0.7519, 0.6888, 0.7162, 0.8548,  => 0.7529

precs (weighed): 0.7512, 0.6929, 0.6973, 0.8156,  => 0.7392
recalls (weighed): 0.7519, 0.6888, 0.7162, 0.8548,  => 0.7529
f1s (weighed): 0.6894, 0.6561, 0.6991, 0.8335,  => 0.7195



Epoch 16/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 16/20, Loss: 1056.6519, 

accs: 0.5065, 0.5033, 0.4640, 0.5244,  => 0.4995

precs (macro): 0.2717, 0.2930, 0.4598, 0.3118,  => 0.3341
recalls (macro): 0.2593, 0.3277, 0.3917, 0.1843,  => 0.2907
f1s (macro): 0.2472, 0.2667, 0.4016, 0.2278,  => 0.2858

precs (micro): 0.5065, 0.5033, 0.4640, 0.5244,  => 0.4995
recalls (micro): 0.5065, 0.5033, 0.4640, 0.5244,  => 0.4995
f1s (micro): 0.5065, 0.5033, 0.4640, 0.5244,  => 0.4995

precs (weighed): 0.6248, 0.6081, 0.5760, 0.7795,  => 0.6471
recalls (weighed): 0.5065, 0.5033, 0.4640, 0.5244,  => 0.4995
f1s (weighed): 0.5584, 0.5505, 0.5088, 0.6247,  => 0.5606


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 123.8651, 

accs: 0.7502, 0.6813, 0.7378, 0.8573,  => 0.7566

precs (macro): 0.5176, 0.6690, 0.5812, 0.4550,  => 0.5557
recalls (macro): 0.3827, 0.6286, 0.5382, 0.4310,  => 0.4951
f1s (macro): 0.3736, 0.6289, 0.5493, 0.4387,  => 0.4976

precs (micro): 0.7502, 0.6813, 0.7378, 0.8573,  => 0.7566
recalls (micro): 0.7502, 0.6813, 0.7378, 0.8573,  => 0.7566
f1s (micro): 0.7502, 0.6813, 0.7378, 0.8573,  => 0.7566

precs (weighed): 0.7563, 0.6743, 0.7101, 0.8161,  => 0.7392
recalls (weighed): 0.7502, 0.6813, 0.7378, 0.8573,  => 0.7566
f1s (weighed): 0.6830, 0.6603, 0.7132, 0.8343,  => 0.7227



Epoch 17/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 17/20, Loss: 1061.3082, 

accs: 0.5099, 0.4887, 0.4633, 0.5148,  => 0.4942

precs (macro): 0.2744, 0.2885, 0.4564, 0.3217,  => 0.3353
recalls (macro): 0.2776, 0.2361, 0.3908, 0.3044,  => 0.3022
f1s (macro): 0.2503, 0.2596, 0.4008, 0.2246,  => 0.2838

precs (micro): 0.5099, 0.4887, 0.4633, 0.5148,  => 0.4942
recalls (micro): 0.5099, 0.4887, 0.4633, 0.5148,  => 0.4942
f1s (micro): 0.5099, 0.4887, 0.4633, 0.5148,  => 0.4942

precs (weighed): 0.6283, 0.6016, 0.5743, 0.7788,  => 0.6458
recalls (weighed): 0.5099, 0.4887, 0.4633, 0.5148,  => 0.4942
f1s (weighed): 0.5616, 0.5392, 0.5082, 0.6160,  => 0.5563


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 123.0525, 

accs: 0.7519, 0.6871, 0.7311, 0.8705,  => 0.7602

precs (macro): 0.4935, 0.6830, 0.5777, 0.5068,  => 0.5652
recalls (macro): 0.3928, 0.6290, 0.5404, 0.4067,  => 0.4922
f1s (macro): 0.3908, 0.6275, 0.5499, 0.4242,  => 0.4981

precs (micro): 0.7519, 0.6871, 0.7311, 0.8705,  => 0.7602
recalls (micro): 0.7519, 0.6871, 0.7311, 0.8705,  => 0.7602
f1s (micro): 0.7519, 0.6871, 0.7311, 0.8705,  => 0.7602

precs (weighed): 0.7396, 0.6846, 0.7045, 0.8261,  => 0.7387
recalls (weighed): 0.7519, 0.6871, 0.7311, 0.8705,  => 0.7602
f1s (weighed): 0.6966, 0.6610, 0.7069, 0.8350,  => 0.7249



Epoch 18/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 18/20, Loss: 1056.9223, 

accs: 0.5080, 0.5014, 0.4632, 0.5183,  => 0.4977

precs (macro): 0.2722, 0.2916, 0.4625, 0.3024,  => 0.3322
recalls (macro): 0.2783, 0.3251, 0.3950, 0.2484,  => 0.3117
f1s (macro): 0.2498, 0.2646, 0.4039, 0.2284,  => 0.2867

precs (micro): 0.5080, 0.5014, 0.4632, 0.5183,  => 0.4977
recalls (micro): 0.5080, 0.5014, 0.4632, 0.5183,  => 0.4977
f1s (micro): 0.5080, 0.5014, 0.4632, 0.5183,  => 0.4977

precs (weighed): 0.6257, 0.6067, 0.5746, 0.7672,  => 0.6435
recalls (weighed): 0.5080, 0.5014, 0.4632, 0.5183,  => 0.4977
f1s (weighed): 0.5591, 0.5489, 0.5077, 0.6174,  => 0.5583


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 122.5799, 

accs: 0.7502, 0.6788, 0.7328, 0.8672,  => 0.7573

precs (macro): 0.4746, 0.6719, 0.5731, 0.4803,  => 0.5500
recalls (macro): 0.3995, 0.6191, 0.5359, 0.4265,  => 0.4952
f1s (macro): 0.4012, 0.6158, 0.5458, 0.4410,  => 0.5010

precs (micro): 0.7502, 0.6788, 0.7328, 0.8672,  => 0.7573
recalls (micro): 0.7502, 0.6788, 0.7328, 0.8672,  => 0.7573
f1s (micro): 0.7502, 0.6788, 0.7328, 0.8672,  => 0.7573

precs (weighed): 0.7266, 0.6746, 0.7037, 0.8229,  => 0.7320
recalls (weighed): 0.7502, 0.6788, 0.7328, 0.8672,  => 0.7573
f1s (weighed): 0.7040, 0.6508, 0.7109, 0.8394,  => 0.7263

Early stopping triggered
lstm + uitnlp/visobert

[16:14:14] task: task-2                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: uitnlp/visobert                                                                 my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/lstm_visobert                                                  my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/visobert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Training ...

Epoch 1/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 1/20, Loss: 1165.9647, 

accs: 0.4250, 0.4534, 0.3911, 0.5103,  => 0.4449

precs (macro): 0.2213, 0.2508, 0.2575, 0.2445,  => 0.2435
recalls (macro): 0.2215, 0.2097, 0.2673, 0.2763,  => 0.2437
f1s (macro): 0.1659, 0.2278, 0.2431, 0.1786,  => 0.2038

precs (micro): 0.4250, 0.4534, 0.3911, 0.5103,  => 0.4449
recalls (micro): 0.4250, 0.4534, 0.3911, 0.5103,  => 0.4449
f1s (micro): 0.4250, 0.4534, 0.3911, 0.5103,  => 0.4449

precs (weighed): 0.5592, 0.5301, 0.3947, 0.7357,  => 0.5549
recalls (weighed): 0.4250, 0.4534, 0.3911, 0.5103,  => 0.4449
f1s (weighed): 0.4653, 0.4876, 0.3855, 0.5982,  => 0.4841


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 142.1665, 

accs: 0.7162, 0.6324, 0.5494, 0.8556,  => 0.6884

precs (macro): 0.2387, 0.6237, 0.2995, 0.2852,  => 0.3618
recalls (macro): 0.3333, 0.5418, 0.3025, 0.3333,  => 0.3778
f1s (macro): 0.2782, 0.4929, 0.2612, 0.3074,  => 0.3349

precs (micro): 0.7162, 0.6324, 0.5494, 0.8556,  => 0.6884
recalls (micro): 0.7162, 0.6324, 0.5494, 0.8556,  => 0.6884
f1s (micro): 0.7162, 0.6324, 0.5494, 0.8556,  => 0.6884

precs (weighed): 0.5129, 0.6261, 0.4779, 0.7321,  => 0.5872
recalls (weighed): 0.7162, 0.6324, 0.5494, 0.8556,  => 0.6884
f1s (weighed): 0.5977, 0.5527, 0.4514, 0.7890,  => 0.5977

Saved the best model to path: ./models/task_2/lstm_visobert_0.pth


Epoch 2/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 2/20, Loss: 1121.0520, 

accs: 0.4385, 0.4799, 0.4138, 0.5087,  => 0.4602

precs (macro): 0.1816, 0.2746, 0.2728, 0.2121,  => 0.2353
recalls (macro): 0.2371, 0.3139, 0.3086, 0.2747,  => 0.2836
f1s (macro): 0.1683, 0.2509, 0.2721, 0.1758,  => 0.2168

precs (micro): 0.4385, 0.4799, 0.4138, 0.5087,  => 0.4602
recalls (micro): 0.4385, 0.4799, 0.4138, 0.5087,  => 0.4602
f1s (micro): 0.4385, 0.4799, 0.4138, 0.5087,  => 0.4602

precs (weighed): 0.5170, 0.5746, 0.4384, 0.7199,  => 0.5625
recalls (weighed): 0.4385, 0.4799, 0.4138, 0.5087,  => 0.4602
f1s (weighed): 0.4726, 0.5228, 0.4216, 0.5960,  => 0.5033


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 136.7546, 

accs: 0.7162, 0.6523, 0.6207, 0.8556,  => 0.7112

precs (macro): 0.2387, 0.6585, 0.3488, 0.2852,  => 0.3828
recalls (macro): 0.3333, 0.5703, 0.3971, 0.3333,  => 0.4085
f1s (macro): 0.2782, 0.5407, 0.3703, 0.3074,  => 0.3741

precs (micro): 0.7162, 0.6523, 0.6207, 0.8556,  => 0.7112
recalls (micro): 0.7162, 0.6523, 0.6207, 0.8556,  => 0.7112
f1s (micro): 0.7162, 0.6523, 0.6207, 0.8556,  => 0.7112

precs (weighed): 0.5129, 0.6567, 0.5261, 0.7321,  => 0.6069
recalls (weighed): 0.7162, 0.6523, 0.6207, 0.8556,  => 0.7112
f1s (weighed): 0.5977, 0.5916, 0.5685, 0.7890,  => 0.6367

Saved the best model to path: ./models/task_2/lstm_visobert_1.pth


Epoch 3/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 3/20, Loss: 1110.4292, 

accs: 0.4383, 0.4708, 0.4070, 0.5061,  => 0.4556

precs (macro): 0.1827, 0.2726, 0.2860, 0.2115,  => 0.2382
recalls (macro): 0.2512, 0.2262, 0.3198, 0.2739,  => 0.2678
f1s (macro): 0.1691, 0.2472, 0.2759, 0.1750,  => 0.2168

precs (micro): 0.4383, 0.4708, 0.4070, 0.5061,  => 0.4556
recalls (micro): 0.4383, 0.4708, 0.4070, 0.5061,  => 0.4556
f1s (micro): 0.4383, 0.4708, 0.4070, 0.5061,  => 0.4556

precs (weighed): 0.5193, 0.5712, 0.4594, 0.7178,  => 0.5669
recalls (weighed): 0.4383, 0.4708, 0.4070, 0.5061,  => 0.4556
f1s (weighed): 0.4731, 0.5160, 0.4254, 0.5935,  => 0.5020


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 133.3178, 

accs: 0.7162, 0.6763, 0.6241, 0.8556,  => 0.7180

precs (macro): 0.2387, 0.6571, 0.3608, 0.2852,  => 0.3855
recalls (macro): 0.3333, 0.6347, 0.3998, 0.3333,  => 0.4253
f1s (macro): 0.2782, 0.6374, 0.3756, 0.3074,  => 0.3997

precs (micro): 0.7162, 0.6763, 0.6241, 0.8556,  => 0.7180
recalls (micro): 0.7162, 0.6763, 0.6241, 0.8556,  => 0.7180
f1s (micro): 0.7162, 0.6763, 0.6241, 0.8556,  => 0.7180

precs (weighed): 0.5129, 0.6671, 0.5256, 0.7321,  => 0.6094
recalls (weighed): 0.7162, 0.6763, 0.6241, 0.8556,  => 0.7180
f1s (weighed): 0.5977, 0.6641, 0.5692, 0.7890,  => 0.6550

Saved the best model to path: ./models/task_2/lstm_visobert_2.pth


Epoch 4/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 4/20, Loss: 1100.0135, 

accs: 0.4785, 0.4845, 0.4050, 0.5033,  => 0.4678

precs (macro): 0.2656, 0.2832, 0.2907, 0.2126,  => 0.2630
recalls (macro): 0.2744, 0.2323, 0.3185, 0.1482,  => 0.2434
f1s (macro): 0.2326, 0.2553, 0.2758, 0.1746,  => 0.2346

precs (micro): 0.4785, 0.4845, 0.4050, 0.5033,  => 0.4678
recalls (micro): 0.4785, 0.4845, 0.4050, 0.5033,  => 0.4678
f1s (micro): 0.4785, 0.4845, 0.4050, 0.5033,  => 0.4678

precs (weighed): 0.6095, 0.5909, 0.4686, 0.7222,  => 0.5978
recalls (weighed): 0.4785, 0.4845, 0.4050, 0.5033,  => 0.4678
f1s (weighed): 0.5340, 0.5324, 0.4279, 0.5932,  => 0.5219


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 130.3570, 

accs: 0.7369, 0.6846, 0.5809, 0.8556,  => 0.7145

precs (macro): 0.4443, 0.6678, 0.3422, 0.2852,  => 0.4349
recalls (macro): 0.4020, 0.6419, 0.3976, 0.3333,  => 0.4437
f1s (macro): 0.4053, 0.6449, 0.3594, 0.3074,  => 0.4292

precs (micro): 0.7369, 0.6846, 0.5809, 0.8556,  => 0.7145
recalls (micro): 0.7369, 0.6846, 0.5809, 0.8556,  => 0.7145
f1s (micro): 0.7369, 0.6846, 0.5809, 0.8556,  => 0.7145

precs (weighed): 0.7034, 0.6763, 0.5438, 0.7321,  => 0.6639
recalls (weighed): 0.7369, 0.6846, 0.5809, 0.8556,  => 0.7145
f1s (weighed): 0.7023, 0.6716, 0.5499, 0.7890,  => 0.6782

Saved the best model to path: ./models/task_2/lstm_visobert_3.pth


Epoch 5/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 5/20, Loss: 1085.1115, 

accs: 0.5052, 0.4829, 0.4110, 0.5175,  => 0.4791

precs (macro): 0.2757, 0.2819, 0.2951, 0.3147,  => 0.2919
recalls (macro): 0.2545, 0.2328, 0.3235, 0.2398,  => 0.2627
f1s (macro): 0.2473, 0.2550, 0.2795, 0.2202,  => 0.2505

precs (micro): 0.5052, 0.4829, 0.4110, 0.5175,  => 0.4791
recalls (micro): 0.5052, 0.4829, 0.4110, 0.5175,  => 0.4791
f1s (micro): 0.5052, 0.4829, 0.4110, 0.5175,  => 0.4791

precs (weighed): 0.6310, 0.5876, 0.4757, 0.7709,  => 0.6163
recalls (weighed): 0.5052, 0.4829, 0.4110, 0.5175,  => 0.4791
f1s (weighed): 0.5603, 0.5301, 0.4343, 0.6152,  => 0.5350


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 129.4172, 

accs: 0.7303, 0.6846, 0.6349, 0.8589,  => 0.7272

precs (macro): 0.4957, 0.6713, 0.3542, 0.6195,  => 0.5352
recalls (macro): 0.3554, 0.6352, 0.4086, 0.3431,  => 0.4356
f1s (macro): 0.3259, 0.6369, 0.3793, 0.3269,  => 0.4172

precs (micro): 0.7303, 0.6846, 0.6349, 0.8589,  => 0.7272
recalls (micro): 0.7303, 0.6846, 0.6349, 0.8589,  => 0.7272
f1s (micro): 0.7303, 0.6846, 0.6349, 0.8589,  => 0.7272

precs (weighed): 0.7306, 0.6773, 0.5430, 0.8482,  => 0.6998
recalls (weighed): 0.7303, 0.6846, 0.6349, 0.8589,  => 0.7272
f1s (weighed): 0.6395, 0.6665, 0.5852, 0.7969,  => 0.6720

Saved the best model to path: ./models/task_2/lstm_visobert_4.pth


Epoch 6/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 6/20, Loss: 1077.8649, 

accs: 0.5049, 0.4831, 0.4148, 0.5219,  => 0.4812

precs (macro): 0.2703, 0.2852, 0.2982, 0.3134,  => 0.2918
recalls (macro): 0.2484, 0.3996, 0.3238, 0.3715,  => 0.3358
f1s (macro): 0.2446, 0.2569, 0.2819, 0.2282,  => 0.2529

precs (micro): 0.5049, 0.4831, 0.4148, 0.5219,  => 0.4812
recalls (micro): 0.5049, 0.4831, 0.4148, 0.5219,  => 0.4812
f1s (micro): 0.5049, 0.4831, 0.4148, 0.5219,  => 0.4812

precs (weighed): 0.6224, 0.5931, 0.4814, 0.7760,  => 0.6182
recalls (weighed): 0.5049, 0.4831, 0.4148, 0.5219,  => 0.4812
f1s (weighed): 0.5569, 0.5323, 0.4391, 0.6214,  => 0.5374


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 125.7178, 

accs: 0.7361, 0.6996, 0.5959, 0.8564,  => 0.7220

precs (macro): 0.5113, 0.6980, 0.3478, 0.6188,  => 0.5440
recalls (macro): 0.3625, 0.6447, 0.4039, 0.3358,  => 0.4367
f1s (macro): 0.3386, 0.6460, 0.3665, 0.3124,  => 0.4159

precs (micro): 0.7361, 0.6996, 0.5959, 0.8564,  => 0.7220
recalls (micro): 0.7361, 0.6996, 0.5959, 0.8564,  => 0.7220
f1s (micro): 0.7361, 0.6996, 0.5959, 0.8564,  => 0.7220

precs (weighed): 0.7454, 0.6987, 0.5593, 0.8464,  => 0.7124
recalls (weighed): 0.7361, 0.6996, 0.5959, 0.8564,  => 0.7220
f1s (weighed): 0.6513, 0.6769, 0.5691, 0.7910,  => 0.6721

Saved the best model to path: ./models/task_2/lstm_visobert_5.pth


Epoch 7/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 7/20, Loss: 1075.9870, 

accs: 0.5117, 0.4881, 0.4165, 0.5124,  => 0.4822

precs (macro): 0.2722, 0.2896, 0.3029, 0.3227,  => 0.2968
recalls (macro): 0.2767, 0.2375, 0.3287, 0.3121,  => 0.2888
f1s (macro): 0.2491, 0.2608, 0.2852, 0.2339,  => 0.2573

precs (micro): 0.5117, 0.4881, 0.4165, 0.5124,  => 0.4822
recalls (micro): 0.5117, 0.4881, 0.4165, 0.5124,  => 0.4822
f1s (micro): 0.5117, 0.4881, 0.4165, 0.5124,  => 0.4822

precs (weighed): 0.6230, 0.6024, 0.4890, 0.7790,  => 0.6233
recalls (weighed): 0.5117, 0.4881, 0.4165, 0.5124,  => 0.4822
f1s (weighed): 0.5606, 0.5389, 0.4428, 0.6160,  => 0.5396


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 125.7816, 

accs: 0.7369, 0.6896, 0.6282, 0.8780,  => 0.7332

precs (macro): 0.4792, 0.6727, 0.3528, 0.5461,  => 0.5127
recalls (macro): 0.3703, 0.6719, 0.4129, 0.4138,  => 0.4672
f1s (macro): 0.3551, 0.6723, 0.3794, 0.4361,  => 0.4607

precs (micro): 0.7369, 0.6896, 0.6282, 0.8780,  => 0.7332
recalls (micro): 0.7369, 0.6896, 0.6282, 0.8780,  => 0.7332
f1s (micro): 0.7369, 0.6896, 0.6282, 0.8780,  => 0.7332

precs (weighed): 0.7211, 0.6889, 0.5555, 0.8412,  => 0.7017
recalls (weighed): 0.7369, 0.6896, 0.6282, 0.8780,  => 0.7332
f1s (weighed): 0.6643, 0.6893, 0.5887, 0.8422,  => 0.6961



Epoch 8/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 8/20, Loss: 1066.6887, 

accs: 0.5108, 0.4915, 0.4132, 0.5298,  => 0.4863

precs (macro): 0.2794, 0.2938, 0.3027, 0.3111,  => 0.2968
recalls (macro): 0.2718, 0.2377, 0.3253, 0.2579,  => 0.2732
f1s (macro): 0.2532, 0.2628, 0.2836, 0.2386,  => 0.2595

precs (micro): 0.5108, 0.4915, 0.4132, 0.5298,  => 0.4863
recalls (micro): 0.5108, 0.4915, 0.4132, 0.5298,  => 0.4863
f1s (micro): 0.5108, 0.4915, 0.4132, 0.5298,  => 0.4863

precs (weighed): 0.6359, 0.6096, 0.4910, 0.7742,  => 0.6277
recalls (weighed): 0.5108, 0.4915, 0.4132, 0.5298,  => 0.4863
f1s (weighed): 0.5653, 0.5442, 0.4414, 0.6281,  => 0.5448


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 123.7716, 

accs: 0.7353, 0.6971, 0.6174, 0.8805,  => 0.7326

precs (macro): 0.5426, 0.6973, 0.3534, 0.5393,  => 0.5332
recalls (macro): 0.3584, 0.6399, 0.4113, 0.4232,  => 0.4582
f1s (macro): 0.3293, 0.6400, 0.3761, 0.4463,  => 0.4479

precs (micro): 0.7353, 0.6971, 0.6174, 0.8805,  => 0.7326
recalls (micro): 0.7353, 0.6971, 0.6174, 0.8805,  => 0.7326
f1s (micro): 0.7353, 0.6971, 0.6174, 0.8805,  => 0.7326

precs (weighed): 0.7700, 0.6972, 0.5551, 0.8422,  => 0.7161
recalls (weighed): 0.7353, 0.6971, 0.6174, 0.8805,  => 0.7326
f1s (weighed): 0.6438, 0.6722, 0.5834, 0.8473,  => 0.6867

Saved the best model to path: ./models/task_2/lstm_visobert_7.pth


Epoch 9/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 9/20, Loss: 1062.1570, 

accs: 0.5080, 0.5023, 0.4286, 0.5228,  => 0.4904

precs (macro): 0.2739, 0.2962, 0.3068, 0.3233,  => 0.3000
recalls (macro): 0.2685, 0.2446, 0.3328, 0.3866,  => 0.3081
f1s (macro): 0.2476, 0.2678, 0.2908, 0.2455,  => 0.2629

precs (micro): 0.5080, 0.5023, 0.4286, 0.5228,  => 0.4904
recalls (micro): 0.5080, 0.5023, 0.4286, 0.5228,  => 0.4904
f1s (micro): 0.5080, 0.5023, 0.4286, 0.5228,  => 0.4904

precs (weighed): 0.6264, 0.6148, 0.4956, 0.7815,  => 0.6296
recalls (weighed): 0.5080, 0.5023, 0.4286, 0.5228,  => 0.4904
f1s (weighed): 0.5599, 0.5526, 0.4531, 0.6254,  => 0.5478


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 120.8133, 

accs: 0.7444, 0.6979, 0.6266, 0.8822,  => 0.7378

precs (macro): 0.4881, 0.6808, 0.3513, 0.5389,  => 0.5148
recalls (macro): 0.3812, 0.6767, 0.4096, 0.4281,  => 0.4739
f1s (macro): 0.3730, 0.6784, 0.3772, 0.4516,  => 0.4701

precs (micro): 0.7444, 0.6979, 0.6266, 0.8822,  => 0.7378
recalls (micro): 0.7444, 0.6979, 0.6266, 0.8822,  => 0.7378
f1s (micro): 0.7444, 0.6979, 0.6266, 0.8822,  => 0.7378

precs (weighed): 0.7316, 0.6952, 0.5468, 0.8438,  => 0.7044
recalls (weighed): 0.7444, 0.6979, 0.6266, 0.8822,  => 0.7378
f1s (weighed): 0.6806, 0.6962, 0.5824, 0.8500,  => 0.7023

Saved the best model to path: ./models/task_2/lstm_visobert_8.pth


Epoch 10/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 10/20, Loss: 1068.5722, 

accs: 0.5020, 0.4913, 0.4138, 0.5221,  => 0.4823

precs (macro): 0.2749, 0.2925, 0.3076, 0.3180,  => 0.2983
recalls (macro): 0.2614, 0.3218, 0.3268, 0.2556,  => 0.2914
f1s (macro): 0.2466, 0.2629, 0.2847, 0.2385,  => 0.2582

precs (micro): 0.5020, 0.4913, 0.4138, 0.5221,  => 0.4823
recalls (micro): 0.5020, 0.4913, 0.4138, 0.5221,  => 0.4823
f1s (micro): 0.5020, 0.4913, 0.4138, 0.5221,  => 0.4823

precs (weighed): 0.6283, 0.6062, 0.4970, 0.7813,  => 0.6282
recalls (weighed): 0.5020, 0.4913, 0.4138, 0.5221,  => 0.4823
f1s (weighed): 0.5571, 0.5425, 0.4441, 0.6247,  => 0.5421


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 122.2989, 

accs: 0.7494, 0.6954, 0.6241, 0.8680,  => 0.7342

precs (macro): 0.4969, 0.6777, 0.3484, 0.6221,  => 0.5363
recalls (macro): 0.3860, 0.6660, 0.4026, 0.3698,  => 0.4561
f1s (macro): 0.3799, 0.6694, 0.3719, 0.3753,  => 0.4491

precs (micro): 0.7494, 0.6954, 0.6241, 0.8680,  => 0.7342
recalls (micro): 0.7494, 0.6954, 0.6241, 0.8680,  => 0.7342
f1s (micro): 0.7494, 0.6954, 0.6241, 0.8680,  => 0.7342

precs (weighed): 0.7406, 0.6896, 0.5465, 0.8550,  => 0.7079
recalls (weighed): 0.7494, 0.6954, 0.6241, 0.8680,  => 0.7342
f1s (weighed): 0.6877, 0.6903, 0.5816, 0.8168,  => 0.6941



Epoch 11/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 11/20, Loss: 1060.3051, 

accs: 0.4991, 0.5028, 0.4196, 0.5297,  => 0.4878

precs (macro): 0.2708, 0.2984, 0.3075, 0.3239,  => 0.3002
recalls (macro): 0.2665, 0.3292, 0.3269, 0.3194,  => 0.3105
f1s (macro): 0.2450, 0.2696, 0.2874, 0.2402,  => 0.2605

precs (micro): 0.4991, 0.5028, 0.4196, 0.5297,  => 0.4878
recalls (micro): 0.4991, 0.5028, 0.4196, 0.5297,  => 0.4878
f1s (micro): 0.4991, 0.5028, 0.4196, 0.5297,  => 0.4878

precs (weighed): 0.6229, 0.6183, 0.4989, 0.7812,  => 0.6303
recalls (weighed): 0.4991, 0.5028, 0.4196, 0.5297,  => 0.4878
f1s (weighed): 0.5529, 0.5541, 0.4488, 0.6293,  => 0.5463


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 127.5878, 

accs: 0.7361, 0.6896, 0.5909, 0.8913,  => 0.7270

precs (macro): 0.5285, 0.6886, 0.3660, 0.5416,  => 0.5312
recalls (macro): 0.3606, 0.6302, 0.4316, 0.4675,  => 0.4725
f1s (macro): 0.3342, 0.6284, 0.3805, 0.4903,  => 0.4583

precs (micro): 0.7361, 0.6896, 0.5909, 0.8913,  => 0.7270
recalls (micro): 0.7361, 0.6896, 0.5909, 0.8913,  => 0.7270
f1s (micro): 0.7361, 0.6896, 0.5909, 0.8913,  => 0.7270

precs (weighed): 0.7590, 0.6890, 0.5822, 0.8548,  => 0.7213
recalls (weighed): 0.7361, 0.6896, 0.5909, 0.8913,  => 0.7270
f1s (weighed): 0.6479, 0.6623, 0.5813, 0.8668,  => 0.6896



Epoch 12/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 12/20, Loss: 1066.6454, 

accs: 0.5008, 0.4870, 0.4210, 0.5271,  => 0.4840

precs (macro): 0.2714, 0.2896, 0.3103, 0.3264,  => 0.2995
recalls (macro): 0.2794, 0.4018, 0.3378, 0.2672,  => 0.3216
f1s (macro): 0.2455, 0.2600, 0.2904, 0.2508,  => 0.2617

precs (micro): 0.5008, 0.4870, 0.4210, 0.5271,  => 0.4840
recalls (micro): 0.5008, 0.4870, 0.4210, 0.5271,  => 0.4840
f1s (micro): 0.5008, 0.4870, 0.4210, 0.5271,  => 0.4840

precs (weighed): 0.6223, 0.6014, 0.4999, 0.7843,  => 0.6270
recalls (weighed): 0.5008, 0.4870, 0.4210, 0.5271,  => 0.4840
f1s (weighed): 0.5535, 0.5380, 0.4492, 0.6298,  => 0.5426


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 122.9911, 

accs: 0.7485, 0.6880, 0.6141, 0.8697,  => 0.7301

precs (macro): 0.4966, 0.6729, 0.3535, 0.5679,  => 0.5227
recalls (macro): 0.3856, 0.6757, 0.4133, 0.3810,  => 0.4639
f1s (macro): 0.3796, 0.6740, 0.3768, 0.3926,  => 0.4558

precs (micro): 0.7485, 0.6880, 0.6141, 0.8697,  => 0.7301
recalls (micro): 0.7485, 0.6880, 0.6141, 0.8697,  => 0.7301
f1s (micro): 0.7485, 0.6880, 0.6141, 0.8697,  => 0.7301

precs (weighed): 0.7400, 0.6909, 0.5635, 0.8395,  => 0.7085
recalls (weighed): 0.7485, 0.6880, 0.6141, 0.8697,  => 0.7301
f1s (weighed): 0.6870, 0.6892, 0.5851, 0.8235,  => 0.6962



Epoch 13/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 13/20, Loss: 1054.0645, 

accs: 0.5088, 0.4978, 0.4165, 0.5338,  => 0.4892

precs (macro): 0.2731, 0.2953, 0.3136, 0.3353,  => 0.3043
recalls (macro): 0.2565, 0.3255, 0.3364, 0.2728,  => 0.2978
f1s (macro): 0.2460, 0.2663, 0.2893, 0.2578,  => 0.2648

precs (micro): 0.5088, 0.4978, 0.4165, 0.5338,  => 0.4892
recalls (micro): 0.5088, 0.4978, 0.4165, 0.5338,  => 0.4892
f1s (micro): 0.5088, 0.4978, 0.4165, 0.5338,  => 0.4892

precs (weighed): 0.6279, 0.6118, 0.5044, 0.7969,  => 0.6353
recalls (weighed): 0.5088, 0.4978, 0.4165, 0.5338,  => 0.4892
f1s (weighed): 0.5613, 0.5487, 0.4480, 0.6388,  => 0.5492


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 125.9459, 

accs: 0.7386, 0.6896, 0.6183, 0.8830,  => 0.7324

precs (macro): 0.5121, 0.6718, 0.3548, 0.5042,  => 0.5107
recalls (macro): 0.3661, 0.6676, 0.4162, 0.4727,  => 0.4807
f1s (macro): 0.3453, 0.6693, 0.3794, 0.4841,  => 0.4695

precs (micro): 0.7386, 0.6896, 0.6183, 0.8830,  => 0.7324
recalls (micro): 0.7386, 0.6896, 0.6183, 0.8830,  => 0.7324
f1s (micro): 0.7386, 0.6896, 0.6183, 0.8830,  => 0.7324

precs (weighed): 0.7470, 0.6866, 0.5623, 0.8460,  => 0.7105
recalls (weighed): 0.7386, 0.6896, 0.6183, 0.8830,  => 0.7324
f1s (weighed): 0.6573, 0.6877, 0.5865, 0.8622,  => 0.6984



Epoch 14/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 14/20, Loss: 1055.4220, 

accs: 0.5015, 0.4943, 0.4216, 0.5330,  => 0.4876

precs (macro): 0.2741, 0.2975, 0.3094, 0.3282,  => 0.3023
recalls (macro): 0.2790, 0.2396, 0.3358, 0.3321,  => 0.2966
f1s (macro): 0.2477, 0.2654, 0.2894, 0.2534,  => 0.2640

precs (micro): 0.5015, 0.4943, 0.4216, 0.5330,  => 0.4876
recalls (micro): 0.5015, 0.4943, 0.4216, 0.5330,  => 0.4876
f1s (micro): 0.5015, 0.4943, 0.4216, 0.5330,  => 0.4876

precs (weighed): 0.6270, 0.6157, 0.4991, 0.7907,  => 0.6331
recalls (weighed): 0.5015, 0.4943, 0.4216, 0.5330,  => 0.4876
f1s (weighed): 0.5558, 0.5483, 0.4495, 0.6361,  => 0.5474


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 124.2519, 

accs: 0.7394, 0.7054, 0.6174, 0.8838,  => 0.7365

precs (macro): 0.5231, 0.6895, 0.3505, 0.5417,  => 0.5262
recalls (macro): 0.3659, 0.6734, 0.3980, 0.4372,  => 0.4686
f1s (macro): 0.3442, 0.6776, 0.3673, 0.4617,  => 0.4627

precs (micro): 0.7394, 0.7054, 0.6174, 0.8838,  => 0.7365
recalls (micro): 0.7394, 0.7054, 0.6174, 0.8838,  => 0.7365
f1s (micro): 0.7394, 0.7054, 0.6174, 0.8838,  => 0.7365

precs (weighed): 0.7560, 0.6994, 0.5453, 0.8463,  => 0.7118
recalls (weighed): 0.7394, 0.7054, 0.6174, 0.8838,  => 0.7365
f1s (weighed): 0.6567, 0.6989, 0.5735, 0.8537,  => 0.6957

Early stopping triggered
lstm + uitnlp/CafeBERT

[16:35:52] task: task-2                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: uitnlp/CafeBERT                                                                 my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/lstm_CafeBERT                                                  my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/CafeBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Training ...

Epoch 1/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 1/20, Loss: 1167.1855, 

accs: 0.4911, 0.4654, 0.3852, 0.5097,  => 0.4628

precs (macro): 0.2468, 0.2549, 0.2214, 0.2296,  => 0.2382
recalls (macro): 0.2449, 0.2946, 0.2578, 0.2754,  => 0.2682
f1s (macro): 0.2288, 0.2287, 0.2283, 0.1772,  => 0.2158

precs (micro): 0.4911, 0.4654, 0.3852, 0.5097,  => 0.4628
recalls (micro): 0.4911, 0.4654, 0.3852, 0.5097,  => 0.4628
f1s (micro): 0.4911, 0.4654, 0.3852, 0.5097,  => 0.4628

precs (weighed): 0.5857, 0.5363, 0.3558, 0.7302,  => 0.5520
recalls (weighed): 0.4911, 0.4654, 0.3852, 0.5097,  => 0.4628
f1s (weighed): 0.5333, 0.4937, 0.3641, 0.5979,  => 0.4972


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 146.6889, 

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423

Saved the best model to path: ./models/task_2/lstm_CafeBERT_0.pth


Epoch 2/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 2/20, Loss: 1152.1222, 

accs: 0.4963, 0.4714, 0.3862, 0.5079,  => 0.4654

precs (macro): 0.2470, 0.2586, 0.2291, 0.2113,  => 0.2365
recalls (macro): 0.2560, 0.2166, 0.2674, 0.3369,  => 0.2692
f1s (macro): 0.2299, 0.2341, 0.2384, 0.1754,  => 0.2195

precs (micro): 0.4963, 0.4714, 0.3862, 0.5079,  => 0.4654
recalls (micro): 0.4963, 0.4714, 0.3862, 0.5079,  => 0.4654
f1s (micro): 0.4963, 0.4714, 0.3862, 0.5079,  => 0.4654

precs (weighed): 0.5861, 0.5432, 0.3681, 0.7169,  => 0.5536
recalls (weighed): 0.4963, 0.4714, 0.3862, 0.5079,  => 0.4654
f1s (weighed): 0.5364, 0.5019, 0.3746, 0.5943,  => 0.5018


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 142.0170, 

accs: 0.7162, 0.6108, 0.5494, 0.8556,  => 0.6830

precs (macro): 0.2387, 0.4728, 0.2655, 0.2852,  => 0.3155
recalls (macro): 0.3333, 0.4994, 0.3258, 0.3333,  => 0.3730
f1s (macro): 0.2782, 0.3832, 0.2926, 0.3074,  => 0.3153

precs (micro): 0.7162, 0.6108, 0.5494, 0.8556,  => 0.6830
recalls (micro): 0.7162, 0.6108, 0.5494, 0.8556,  => 0.6830
f1s (micro): 0.7162, 0.6108, 0.5494, 0.8556,  => 0.6830

precs (weighed): 0.5129, 0.5041, 0.4487, 0.7321,  => 0.5494
recalls (weighed): 0.7162, 0.6108, 0.5494, 0.8556,  => 0.6830
f1s (weighed): 0.5977, 0.4674, 0.4939, 0.7890,  => 0.5870

Saved the best model to path: ./models/task_2/lstm_CafeBERT_1.pth


Epoch 3/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 3/20, Loss: 1135.5735, 

accs: 0.5001, 0.4568, 0.4013, 0.5164,  => 0.4686

precs (macro): 0.2535, 0.2567, 0.2459, 0.2123,  => 0.2421
recalls (macro): 0.2527, 0.2153, 0.2826, 0.2145,  => 0.2413
f1s (macro): 0.2348, 0.2342, 0.2539, 0.1773,  => 0.2250

precs (micro): 0.5001, 0.4568, 0.4013, 0.5164,  => 0.4686
recalls (micro): 0.5001, 0.4568, 0.4013, 0.5164,  => 0.4686
f1s (micro): 0.5001, 0.4568, 0.4013, 0.5164,  => 0.4686

precs (weighed): 0.5964, 0.5412, 0.3980, 0.7207,  => 0.5641
recalls (weighed): 0.5001, 0.4568, 0.4013, 0.5164,  => 0.4686
f1s (weighed): 0.5431, 0.4954, 0.3972, 0.6016,  => 0.5093


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 136.3408, 

accs: 0.7162, 0.6282, 0.5668, 0.8556,  => 0.6917

precs (macro): 0.2387, 0.5981, 0.2722, 0.2852,  => 0.3486
recalls (macro): 0.3333, 0.5498, 0.3176, 0.3333,  => 0.3835
f1s (macro): 0.2782, 0.5215, 0.2786, 0.3074,  => 0.3464

precs (micro): 0.7162, 0.6282, 0.5668, 0.8556,  => 0.6917
recalls (micro): 0.7162, 0.6282, 0.5668, 0.8556,  => 0.6917
f1s (micro): 0.7162, 0.6282, 0.5668, 0.8556,  => 0.6917

precs (weighed): 0.5129, 0.6075, 0.4496, 0.7321,  => 0.5755
recalls (weighed): 0.7162, 0.6282, 0.5668, 0.8556,  => 0.6917
f1s (weighed): 0.5977, 0.5723, 0.4799, 0.7890,  => 0.6098

Saved the best model to path: ./models/task_2/lstm_CafeBERT_2.pth


Epoch 4/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 4/20, Loss: 1126.2632, 

accs: 0.4966, 0.4670, 0.3916, 0.5132,  => 0.4671

precs (macro): 0.2650, 0.2663, 0.2477, 0.2133,  => 0.2481
recalls (macro): 0.2695, 0.2224, 0.2821, 0.2136,  => 0.2469
f1s (macro): 0.2416, 0.2424, 0.2515, 0.1770,  => 0.2281

precs (micro): 0.4966, 0.4670, 0.3916, 0.5132,  => 0.4671
recalls (micro): 0.4966, 0.4670, 0.3916, 0.5132,  => 0.4671
f1s (micro): 0.4966, 0.4670, 0.3916, 0.5132,  => 0.4671

precs (weighed): 0.6128, 0.5593, 0.4033, 0.7242,  => 0.5749
recalls (weighed): 0.4966, 0.4670, 0.3916, 0.5132,  => 0.4671
f1s (weighed): 0.5473, 0.5090, 0.3937, 0.6006,  => 0.5126


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 137.0301, 

accs: 0.7311, 0.6340, 0.5427, 0.8556,  => 0.6909

precs (macro): 0.4577, 0.6057, 0.2637, 0.2852,  => 0.4031
recalls (macro): 0.3664, 0.5636, 0.2967, 0.3333,  => 0.3900
f1s (macro): 0.3499, 0.5462, 0.2506, 0.3074,  => 0.3635

precs (micro): 0.7311, 0.6340, 0.5427, 0.8556,  => 0.6909
recalls (micro): 0.7311, 0.6340, 0.5427, 0.8556,  => 0.6909
f1s (micro): 0.7311, 0.6340, 0.5427, 0.8556,  => 0.6909

precs (weighed): 0.7023, 0.6154, 0.4331, 0.7321,  => 0.6207
recalls (weighed): 0.7311, 0.6340, 0.5427, 0.8556,  => 0.6909
f1s (weighed): 0.6584, 0.5911, 0.4397, 0.7890,  => 0.6196



Epoch 5/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 5/20, Loss: 1121.4698, 

accs: 0.4988, 0.4717, 0.4086, 0.5010,  => 0.4700

precs (macro): 0.2665, 0.2690, 0.2548, 0.2127,  => 0.2508
recalls (macro): 0.2626, 0.2265, 0.2880, 0.2100,  => 0.2468
f1s (macro): 0.2425, 0.2458, 0.2599, 0.1743,  => 0.2307

precs (micro): 0.4988, 0.4717, 0.4086, 0.5010,  => 0.4700
recalls (micro): 0.4988, 0.4717, 0.4086, 0.5010,  => 0.4700
f1s (micro): 0.4988, 0.4717, 0.4086, 0.5010,  => 0.4700

precs (weighed): 0.6180, 0.5644, 0.4152, 0.7223,  => 0.5799
recalls (weighed): 0.4988, 0.4717, 0.4086, 0.5010,  => 0.4700
f1s (weighed): 0.5508, 0.5138, 0.4086, 0.5916,  => 0.5162


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 137.1149, 

accs: 0.7427, 0.6332, 0.5959, 0.8556,  => 0.7068

precs (macro): 0.4523, 0.6108, 0.2927, 0.2852,  => 0.4103
recalls (macro): 0.4103, 0.5523, 0.3582, 0.3333,  => 0.4135
f1s (macro): 0.4153, 0.5211, 0.3212, 0.3074,  => 0.3913

precs (micro): 0.7427, 0.6332, 0.5959, 0.8556,  => 0.7068
recalls (micro): 0.7427, 0.6332, 0.5959, 0.8556,  => 0.7068
f1s (micro): 0.7427, 0.6332, 0.5959, 0.8556,  => 0.7068

precs (weighed): 0.7125, 0.6176, 0.4966, 0.7321,  => 0.6397
recalls (weighed): 0.7427, 0.6332, 0.5959, 0.8556,  => 0.7068
f1s (weighed): 0.7116, 0.5732, 0.5403, 0.7890,  => 0.6535



Epoch 6/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 6/20, Loss: 1113.2898, 

accs: 0.5007, 0.4754, 0.4169, 0.5097,  => 0.4756

precs (macro): 0.2762, 0.2721, 0.2631, 0.2129,  => 0.2561
recalls (macro): 0.2598, 0.3949, 0.2978, 0.3375,  => 0.3225
f1s (macro): 0.2490, 0.2488, 0.2672, 0.1763,  => 0.2353

precs (micro): 0.5007, 0.4754, 0.4169, 0.5097,  => 0.4756
recalls (micro): 0.5007, 0.4754, 0.4169, 0.5097,  => 0.4756
f1s (micro): 0.5007, 0.4754, 0.4169, 0.5097,  => 0.4756

precs (weighed): 0.6303, 0.5694, 0.4277, 0.7224,  => 0.5874
recalls (weighed): 0.5007, 0.4754, 0.4169, 0.5097,  => 0.4756
f1s (weighed): 0.5569, 0.5179, 0.4187, 0.5974,  => 0.5227


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 132.9890, 

accs: 0.7261, 0.6398, 0.5959, 0.8556,  => 0.7044

precs (macro): 0.4856, 0.6185, 0.2842, 0.2852,  => 0.4184
recalls (macro): 0.3498, 0.5660, 0.3471, 0.3333,  => 0.3991
f1s (macro): 0.3149, 0.5455, 0.3113, 0.3074,  => 0.3698

precs (micro): 0.7261, 0.6398, 0.5959, 0.8556,  => 0.7044
recalls (micro): 0.7261, 0.6398, 0.5959, 0.8556,  => 0.7044
f1s (micro): 0.7261, 0.6398, 0.5959, 0.8556,  => 0.7044

precs (weighed): 0.7207, 0.6255, 0.4769, 0.7321,  => 0.6388
recalls (weighed): 0.7261, 0.6398, 0.5959, 0.8556,  => 0.7044
f1s (weighed): 0.6297, 0.5921, 0.5279, 0.7890,  => 0.6347

Saved the best model to path: ./models/task_2/lstm_CafeBERT_5.pth


Epoch 7/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 7/20, Loss: 1105.4148, 

accs: 0.5007, 0.4762, 0.4134, 0.5199,  => 0.4775

precs (macro): 0.2738, 0.2699, 0.2655, 0.2127,  => 0.2555
recalls (macro): 0.3067, 0.2273, 0.2965, 0.1531,  => 0.2459
f1s (macro): 0.2508, 0.2468, 0.2665, 0.1780,  => 0.2355

precs (micro): 0.5007, 0.4762, 0.4134, 0.5199,  => 0.4775
recalls (micro): 0.5007, 0.4762, 0.4134, 0.5199,  => 0.4775
f1s (micro): 0.5007, 0.4762, 0.4134, 0.5199,  => 0.4775

precs (weighed): 0.6231, 0.5663, 0.4333, 0.7223,  => 0.5862
recalls (weighed): 0.5007, 0.4762, 0.4134, 0.5199,  => 0.4775
f1s (weighed): 0.5529, 0.5174, 0.4193, 0.6046,  => 0.5236


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 133.6900, 

accs: 0.7469, 0.6407, 0.5734, 0.8556,  => 0.7041

precs (macro): 0.5000, 0.6195, 0.2754, 0.2852,  => 0.4200
recalls (macro): 0.3818, 0.5675, 0.3236, 0.3333,  => 0.4015
f1s (macro): 0.3731, 0.5478, 0.2860, 0.3074,  => 0.3786

precs (micro): 0.7469, 0.6407, 0.5734, 0.8556,  => 0.7041
recalls (micro): 0.7469, 0.6407, 0.5734, 0.8556,  => 0.7041
f1s (micro): 0.7469, 0.6407, 0.5734, 0.8556,  => 0.7041

precs (weighed): 0.7416, 0.6265, 0.4554, 0.7321,  => 0.6389
recalls (weighed): 0.7469, 0.6407, 0.5734, 0.8556,  => 0.7041
f1s (weighed): 0.6815, 0.5939, 0.4904, 0.7890,  => 0.6387



Epoch 8/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 8/20, Loss: 1114.5879, 

accs: 0.4932, 0.4658, 0.4089, 0.5140,  => 0.4705

precs (macro): 0.2738, 0.2676, 0.2647, 0.2128,  => 0.2547
recalls (macro): 0.2699, 0.3079, 0.2915, 0.2138,  => 0.2708
f1s (macro): 0.2464, 0.2444, 0.2639, 0.1770,  => 0.2329

precs (micro): 0.4932, 0.4658, 0.4089, 0.5140,  => 0.4705
recalls (micro): 0.4932, 0.4658, 0.4089, 0.5140,  => 0.4705
f1s (micro): 0.4932, 0.4658, 0.4089, 0.5140,  => 0.4705

precs (weighed): 0.6256, 0.5602, 0.4325, 0.7225,  => 0.5852
recalls (weighed): 0.4932, 0.4658, 0.4089, 0.5140,  => 0.4705
f1s (weighed): 0.5500, 0.5084, 0.4167, 0.6006,  => 0.5189


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 134.4371, 

accs: 0.7394, 0.6199, 0.5942, 0.8556,  => 0.7023

precs (macro): 0.4758, 0.5960, 0.2858, 0.2852,  => 0.4107
recalls (macro): 0.3764, 0.5206, 0.3487, 0.3333,  => 0.3948
f1s (macro): 0.3660, 0.4476, 0.3133, 0.3074,  => 0.3586

precs (micro): 0.7394, 0.6199, 0.5942, 0.8556,  => 0.7023
recalls (micro): 0.7394, 0.6199, 0.5942, 0.8556,  => 0.7023
f1s (micro): 0.7394, 0.6199, 0.5942, 0.8556,  => 0.7023

precs (weighed): 0.7202, 0.6020, 0.4779, 0.7321,  => 0.6330
recalls (weighed): 0.7394, 0.6199, 0.5942, 0.8556,  => 0.7023
f1s (weighed): 0.6735, 0.5170, 0.5285, 0.7890,  => 0.6270



Epoch 9/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 9/20, Loss: 1102.4300, 

accs: 0.5089, 0.4828, 0.4153, 0.5069,  => 0.4785

precs (macro): 0.2804, 0.2739, 0.2708, 0.2109,  => 0.2590
recalls (macro): 0.2783, 0.2324, 0.3064, 0.2117,  => 0.2572
f1s (macro): 0.2529, 0.2514, 0.2711, 0.1749,  => 0.2376

precs (micro): 0.5089, 0.4828, 0.4153, 0.5069,  => 0.4785
recalls (micro): 0.5089, 0.4828, 0.4153, 0.5069,  => 0.4785
f1s (micro): 0.5089, 0.4828, 0.4153, 0.5069,  => 0.4785

precs (weighed): 0.6370, 0.5733, 0.4386, 0.7160,  => 0.5912
recalls (weighed): 0.5089, 0.4828, 0.4153, 0.5069,  => 0.4785
f1s (weighed): 0.5645, 0.5240, 0.4224, 0.5935,  => 0.5261


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 130.5353, 

accs: 0.7245, 0.6398, 0.5884, 0.8556,  => 0.7021

precs (macro): 0.5464, 0.6373, 0.2816, 0.2852,  => 0.4376
recalls (macro): 0.3440, 0.5530, 0.3374, 0.3333,  => 0.3919
f1s (macro): 0.3009, 0.5130, 0.3013, 0.3074,  => 0.3556

precs (micro): 0.7245, 0.6398, 0.5884, 0.8556,  => 0.7021
recalls (micro): 0.7245, 0.6398, 0.5884, 0.8556,  => 0.7021
f1s (micro): 0.7245, 0.6398, 0.5884, 0.8556,  => 0.7021

precs (weighed): 0.7693, 0.6380, 0.4678, 0.7321,  => 0.6518
recalls (weighed): 0.7245, 0.6398, 0.5884, 0.8556,  => 0.7021
f1s (weighed): 0.6182, 0.5689, 0.5128, 0.7890,  => 0.6222

Saved the best model to path: ./models/task_2/lstm_CafeBERT_8.pth


Epoch 10/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 10/20, Loss: 1100.9899, 

accs: 0.4978, 0.4733, 0.4135, 0.5094,  => 0.4735

precs (macro): 0.2857, 0.2726, 0.2711, 0.2132,  => 0.2606
recalls (macro): 0.2914, 0.3936, 0.3053, 0.2125,  => 0.3007
f1s (macro): 0.2514, 0.2482, 0.2703, 0.1762,  => 0.2365

precs (micro): 0.4978, 0.4733, 0.4135, 0.5094,  => 0.4735
recalls (micro): 0.4978, 0.4733, 0.4135, 0.5094,  => 0.4735
f1s (micro): 0.4978, 0.4733, 0.4135, 0.5094,  => 0.4735

precs (weighed): 0.6407, 0.5701, 0.4405, 0.7238,  => 0.5938
recalls (weighed): 0.4978, 0.4733, 0.4135, 0.5094,  => 0.4735
f1s (weighed): 0.5585, 0.5170, 0.4221, 0.5979,  => 0.5239


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 133.3390, 

accs: 0.7386, 0.6423, 0.6091, 0.8556,  => 0.7114

precs (macro): 0.5166, 0.6334, 0.3258, 0.2852,  => 0.4403
recalls (macro): 0.3655, 0.5606, 0.3798, 0.3333,  => 0.4098
f1s (macro): 0.3439, 0.5295, 0.3474, 0.3074,  => 0.3820

precs (micro): 0.7386, 0.6423, 0.6091, 0.8556,  => 0.7114
recalls (micro): 0.7386, 0.6423, 0.6091, 0.8556,  => 0.7114
f1s (micro): 0.7386, 0.6423, 0.6091, 0.8556,  => 0.7114

precs (weighed): 0.7506, 0.6361, 0.5324, 0.7321,  => 0.6628
recalls (weighed): 0.7386, 0.6423, 0.6091, 0.8556,  => 0.7114
f1s (weighed): 0.6562, 0.5813, 0.5639, 0.7890,  => 0.6476



Epoch 11/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 11/20, Loss: 1102.8533, 

accs: 0.5040, 0.4803, 0.4113, 0.5059,  => 0.4753

precs (macro): 0.2855, 0.2739, 0.2728, 0.2131,  => 0.2613
recalls (macro): 0.2776, 0.2310, 0.3046, 0.2739,  => 0.2718
f1s (macro): 0.2542, 0.2506, 0.2700, 0.1755,  => 0.2376

precs (micro): 0.5040, 0.4803, 0.4113, 0.5059,  => 0.4753
recalls (micro): 0.5040, 0.4803, 0.4113, 0.5059,  => 0.4753
f1s (micro): 0.5040, 0.4803, 0.4113, 0.5059,  => 0.4753

precs (weighed): 0.6424, 0.5740, 0.4433, 0.7233,  => 0.5958
recalls (weighed): 0.5040, 0.4803, 0.4113, 0.5059,  => 0.4753
f1s (weighed): 0.5635, 0.5228, 0.4221, 0.5952,  => 0.5259


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 130.8708, 

accs: 0.7527, 0.6415, 0.6066, 0.8556,  => 0.7141

precs (macro): 0.4696, 0.6166, 0.3094, 0.2852,  => 0.4202
recalls (macro): 0.4099, 0.5756, 0.3620, 0.3333,  => 0.4202
f1s (macro): 0.4149, 0.5636, 0.3313, 0.3074,  => 0.4043

precs (micro): 0.7527, 0.6415, 0.6066, 0.8556,  => 0.7141
recalls (micro): 0.7527, 0.6415, 0.6066, 0.8556,  => 0.7141
f1s (micro): 0.7527, 0.6415, 0.6066, 0.8556,  => 0.7141

precs (weighed): 0.7263, 0.6256, 0.5007, 0.7321,  => 0.6462
recalls (weighed): 0.7527, 0.6415, 0.6066, 0.8556,  => 0.7141
f1s (weighed): 0.7153, 0.6051, 0.5474, 0.7890,  => 0.6642



Epoch 12/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 12/20, Loss: 1098.4034, 

accs: 0.5055, 0.4698, 0.4099, 0.5169,  => 0.4755

precs (macro): 0.2823, 0.2692, 0.2735, 0.2116,  => 0.2592
recalls (macro): 0.2711, 0.3096, 0.2976, 0.2146,  => 0.2732
f1s (macro): 0.2538, 0.2460, 0.2683, 0.1772,  => 0.2363

precs (micro): 0.5055, 0.4698, 0.4099, 0.5169,  => 0.4755
recalls (micro): 0.5055, 0.4698, 0.4099, 0.5169,  => 0.4755
f1s (micro): 0.5055, 0.4698, 0.4099, 0.5169,  => 0.4755

precs (weighed): 0.6380, 0.5648, 0.4461, 0.7186,  => 0.5919
recalls (weighed): 0.5055, 0.4698, 0.4099, 0.5169,  => 0.4755
f1s (weighed): 0.5628, 0.5126, 0.4231, 0.6012,  => 0.5250


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 131.1940, 

accs: 0.7469, 0.6523, 0.6091, 0.8556,  => 0.7160

precs (macro): 0.4766, 0.6343, 0.3177, 0.2852,  => 0.4285
recalls (macro): 0.3892, 0.5868, 0.3697, 0.3333,  => 0.4198
f1s (macro): 0.3859, 0.5764, 0.3411, 0.3074,  => 0.4027

precs (micro): 0.7469, 0.6523, 0.6091, 0.8556,  => 0.7160
recalls (micro): 0.7469, 0.6523, 0.6091, 0.8556,  => 0.7160
f1s (micro): 0.7469, 0.6523, 0.6091, 0.8556,  => 0.7160

precs (weighed): 0.7253, 0.6408, 0.5128, 0.7321,  => 0.6527
recalls (weighed): 0.7469, 0.6523, 0.6091, 0.8556,  => 0.7160
f1s (weighed): 0.6916, 0.6167, 0.5564, 0.7890,  => 0.6634



Epoch 13/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 13/20, Loss: 1094.5035, 

accs: 0.5048, 0.4822, 0.4027, 0.5130,  => 0.4757

precs (macro): 0.2894, 0.2752, 0.2807, 0.2116,  => 0.2642
recalls (macro): 0.2752, 0.3150, 0.3078, 0.2135,  => 0.2779
f1s (macro): 0.2574, 0.2518, 0.2701, 0.1764,  => 0.2389

precs (micro): 0.5048, 0.4822, 0.4027, 0.5130,  => 0.4757
recalls (micro): 0.5048, 0.4822, 0.4027, 0.5130,  => 0.4757
f1s (micro): 0.5048, 0.4822, 0.4027, 0.5130,  => 0.4757

precs (weighed): 0.6470, 0.5758, 0.4553, 0.7184,  => 0.5992
recalls (weighed): 0.5048, 0.4822, 0.4027, 0.5130,  => 0.4757
f1s (weighed): 0.5659, 0.5247, 0.4219, 0.5985,  => 0.5277


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 129.3505, 

accs: 0.7577, 0.6440, 0.6133, 0.8556,  => 0.7176

precs (macro): 0.5043, 0.6300, 0.3292, 0.2852,  => 0.4372
recalls (macro): 0.3986, 0.5670, 0.3819, 0.3333,  => 0.4202
f1s (macro): 0.3989, 0.5432, 0.3535, 0.3074,  => 0.4008

precs (micro): 0.7577, 0.6440, 0.6133, 0.8556,  => 0.7176
recalls (micro): 0.7577, 0.6440, 0.6133, 0.8556,  => 0.7176
f1s (micro): 0.7577, 0.6440, 0.6133, 0.8556,  => 0.7176

precs (weighed): 0.7502, 0.6344, 0.5246, 0.7321,  => 0.6603
recalls (weighed): 0.7577, 0.6440, 0.6133, 0.8556,  => 0.7176
f1s (weighed): 0.7048, 0.5915, 0.5655, 0.7890,  => 0.6627

Saved the best model to path: ./models/task_2/lstm_CafeBERT_12.pth


Epoch 14/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 14/20, Loss: 1102.4027, 

accs: 0.4958, 0.4873, 0.4011, 0.5035,  => 0.4719

precs (macro): 0.2836, 0.2752, 0.2779, 0.2106,  => 0.2618
recalls (macro): 0.2868, 0.3160, 0.3071, 0.2107,  => 0.2801
f1s (macro): 0.2527, 0.2524, 0.2685, 0.1741,  => 0.2369

precs (micro): 0.4958, 0.4873, 0.4011, 0.5035,  => 0.4719
recalls (micro): 0.4958, 0.4873, 0.4011, 0.5035,  => 0.4719
f1s (micro): 0.4958, 0.4873, 0.4011, 0.5035,  => 0.4719

precs (weighed): 0.6371, 0.5767, 0.4504, 0.7149,  => 0.5948
recalls (weighed): 0.4958, 0.4873, 0.4011, 0.5035,  => 0.4719
f1s (weighed): 0.5559, 0.5281, 0.4188, 0.5908,  => 0.5234


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 129.9904, 

accs: 0.7485, 0.6523, 0.6058, 0.8556,  => 0.7156

precs (macro): 0.5097, 0.6385, 0.3247, 0.2852,  => 0.4395
recalls (macro): 0.3819, 0.5821, 0.3635, 0.3333,  => 0.4152
f1s (macro): 0.3728, 0.5673, 0.3365, 0.3074,  => 0.3960

precs (micro): 0.7485, 0.6523, 0.6058, 0.8556,  => 0.7156
recalls (micro): 0.7485, 0.6523, 0.6058, 0.8556,  => 0.7156
f1s (micro): 0.7485, 0.6523, 0.6058, 0.8556,  => 0.7156

precs (weighed): 0.7496, 0.6432, 0.5007, 0.7321,  => 0.6564
recalls (weighed): 0.7485, 0.6523, 0.6058, 0.8556,  => 0.7156
f1s (weighed): 0.6818, 0.6104, 0.5417, 0.7890,  => 0.6557



Epoch 15/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 15/20, Loss: 1091.3351, 

accs: 0.5063, 0.4724, 0.4126, 0.5108,  => 0.4756

precs (macro): 0.2859, 0.2731, 0.2833, 0.2128,  => 0.2637
recalls (macro): 0.2622, 0.2273, 0.3093, 0.3378,  => 0.2842
f1s (macro): 0.2543, 0.2480, 0.2747, 0.1765,  => 0.2384

precs (micro): 0.5063, 0.4724, 0.4126, 0.5108,  => 0.4756
recalls (micro): 0.5063, 0.4724, 0.4126, 0.5108,  => 0.4756
f1s (micro): 0.5063, 0.4724, 0.4126, 0.5108,  => 0.4756

precs (weighed): 0.6447, 0.5715, 0.4598, 0.7219,  => 0.5995
recalls (weighed): 0.5063, 0.4724, 0.4126, 0.5108,  => 0.4756
f1s (weighed): 0.5663, 0.5172, 0.4299, 0.5981,  => 0.5278


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 128.9773, 

accs: 0.7593, 0.6415, 0.6166, 0.8556,  => 0.7183

precs (macro): 0.5097, 0.6418, 0.3160, 0.2852,  => 0.4382
recalls (macro): 0.3994, 0.5548, 0.3709, 0.3333,  => 0.4146
f1s (macro): 0.3998, 0.5152, 0.3395, 0.3074,  => 0.3905

precs (micro): 0.7593, 0.6415, 0.6166, 0.8556,  => 0.7183
recalls (micro): 0.7593, 0.6415, 0.6166, 0.8556,  => 0.7183
f1s (micro): 0.7593, 0.6415, 0.6166, 0.8556,  => 0.7183

precs (weighed): 0.7548, 0.6417, 0.5118, 0.7321,  => 0.6601
recalls (weighed): 0.7593, 0.6415, 0.6166, 0.8556,  => 0.7183
f1s (weighed): 0.7061, 0.5709, 0.5588, 0.7890,  => 0.6562

Saved the best model to path: ./models/task_2/lstm_CafeBERT_14.pth


Epoch 16/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 16/20, Loss: 1091.8283, 

accs: 0.5028, 0.4805, 0.4101, 0.5176,  => 0.4777

precs (macro): 0.2830, 0.2756, 0.2772, 0.2140,  => 0.2625
recalls (macro): 0.2757, 0.2314, 0.3061, 0.3398,  => 0.2883
f1s (macro): 0.2537, 0.2515, 0.2711, 0.1783,  => 0.2386

precs (micro): 0.5028, 0.4805, 0.4101, 0.5176,  => 0.4777
recalls (micro): 0.5028, 0.4805, 0.4101, 0.5176,  => 0.4777
f1s (micro): 0.5028, 0.4805, 0.4101, 0.5176,  => 0.4777

precs (weighed): 0.6392, 0.5762, 0.4490, 0.7262,  => 0.5976
recalls (weighed): 0.5028, 0.4805, 0.4101, 0.5176,  => 0.4777
f1s (weighed): 0.5615, 0.5239, 0.4241, 0.6041,  => 0.5284


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 127.0332, 

accs: 0.7411, 0.6382, 0.6166, 0.8556,  => 0.7129

precs (macro): 0.5091, 0.6319, 0.3442, 0.2852,  => 0.4426
recalls (macro): 0.3704, 0.5517, 0.4008, 0.3333,  => 0.4140
f1s (macro): 0.3531, 0.5118, 0.3695, 0.3074,  => 0.3855

precs (micro): 0.7411, 0.6382, 0.6166, 0.8556,  => 0.7129
recalls (micro): 0.7411, 0.6382, 0.6166, 0.8556,  => 0.7129
f1s (micro): 0.7411, 0.6382, 0.6166, 0.8556,  => 0.7129

precs (weighed): 0.7458, 0.6337, 0.5404, 0.7321,  => 0.6630
recalls (weighed): 0.7411, 0.6382, 0.6166, 0.8556,  => 0.7129
f1s (weighed): 0.6642, 0.5677, 0.5758, 0.7890,  => 0.6492

Saved the best model to path: ./models/task_2/lstm_CafeBERT_15.pth


Epoch 17/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 17/20, Loss: 1087.4773, 

accs: 0.5093, 0.4912, 0.4115, 0.5124,  => 0.4811

precs (macro): 0.2857, 0.2780, 0.2793, 0.2126,  => 0.2639
recalls (macro): 0.2666, 0.2356, 0.3018, 0.1509,  => 0.2387
f1s (macro): 0.2555, 0.2550, 0.2716, 0.1765,  => 0.2396

precs (micro): 0.5093, 0.4912, 0.4115, 0.5124,  => 0.4811
recalls (micro): 0.5093, 0.4912, 0.4115, 0.5124,  => 0.4811
f1s (micro): 0.5093, 0.4912, 0.4115, 0.5124,  => 0.4811

precs (weighed): 0.6434, 0.5817, 0.4546, 0.7220,  => 0.6004
recalls (weighed): 0.5093, 0.4912, 0.4115, 0.5124,  => 0.4811
f1s (weighed): 0.5676, 0.5326, 0.4276, 0.5994,  => 0.5318


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 128.2232, 

accs: 0.7593, 0.6531, 0.6141, 0.8556,  => 0.7205

precs (macro): 0.4773, 0.6420, 0.3317, 0.2852,  => 0.4341
recalls (macro): 0.4198, 0.5812, 0.3733, 0.3333,  => 0.4269
f1s (macro): 0.4272, 0.5647, 0.3465, 0.3074,  => 0.4114

precs (micro): 0.7593, 0.6531, 0.6141, 0.8556,  => 0.7205
recalls (micro): 0.7593, 0.6531, 0.6141, 0.8556,  => 0.7205
f1s (micro): 0.7593, 0.6531, 0.6141, 0.8556,  => 0.7205

precs (weighed): 0.7357, 0.6457, 0.5136, 0.7321,  => 0.6568
recalls (weighed): 0.7593, 0.6531, 0.6141, 0.8556,  => 0.7205
f1s (weighed): 0.7265, 0.6088, 0.5531, 0.7890,  => 0.6694



Epoch 18/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 18/20, Loss: 1089.9683, 

accs: 0.5085, 0.4820, 0.4014, 0.5111,  => 0.4758

precs (macro): 0.2849, 0.2755, 0.2793, 0.2124,  => 0.2630
recalls (macro): 0.2758, 0.3142, 0.3048, 0.3379,  => 0.3082
f1s (macro): 0.2561, 0.2515, 0.2685, 0.1765,  => 0.2382

precs (micro): 0.5085, 0.4820, 0.4014, 0.5111,  => 0.4758
recalls (micro): 0.5085, 0.4820, 0.4014, 0.5111,  => 0.4758
f1s (micro): 0.5085, 0.4820, 0.4014, 0.5111,  => 0.4758

precs (weighed): 0.6426, 0.5755, 0.4547, 0.7207,  => 0.5984
recalls (weighed): 0.5085, 0.4820, 0.4014, 0.5111,  => 0.4758
f1s (weighed): 0.5664, 0.5246, 0.4209, 0.5978,  => 0.5274


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 128.4963, 

accs: 0.7618, 0.6290, 0.5992, 0.8556,  => 0.7114

precs (macro): 0.5104, 0.6593, 0.3247, 0.2852,  => 0.4449
recalls (macro): 0.4024, 0.5277, 0.3739, 0.3333,  => 0.4093
f1s (macro): 0.4040, 0.4512, 0.3372, 0.3074,  => 0.3749

precs (micro): 0.7618, 0.6290, 0.5992, 0.8556,  => 0.7114
recalls (micro): 0.7618, 0.6290, 0.5992, 0.8556,  => 0.7114
f1s (micro): 0.7618, 0.6290, 0.5992, 0.8556,  => 0.7114

precs (weighed): 0.7566, 0.6518, 0.5272, 0.7321,  => 0.6669
recalls (weighed): 0.7618, 0.6290, 0.5992, 0.8556,  => 0.7114
f1s (weighed): 0.7103, 0.5214, 0.5512, 0.7890,  => 0.6430



Epoch 19/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 19/20, Loss: 1087.3663, 

accs: 0.5069, 0.4798, 0.4108, 0.5156,  => 0.4783

precs (macro): 0.2946, 0.2746, 0.2840, 0.2122,  => 0.2664
recalls (macro): 0.2948, 0.2314, 0.3153, 0.1518,  => 0.2483
f1s (macro): 0.2603, 0.2511, 0.2750, 0.1770,  => 0.2409

precs (micro): 0.5069, 0.4798, 0.4108, 0.5156,  => 0.4783
recalls (micro): 0.5069, 0.4798, 0.4108, 0.5156,  => 0.4783
f1s (micro): 0.5069, 0.4798, 0.4108, 0.5156,  => 0.4783

precs (weighed): 0.6554, 0.5747, 0.4595, 0.7206,  => 0.6025
recalls (weighed): 0.5069, 0.4798, 0.4108, 0.5156,  => 0.4783
f1s (weighed): 0.5700, 0.5228, 0.4282, 0.6011,  => 0.5305


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 131.0659, 

accs: 0.7627, 0.6556, 0.6041, 0.8556,  => 0.7195

precs (macro): 0.4881, 0.6675, 0.3219, 0.2852,  => 0.4407
recalls (macro): 0.4164, 0.5734, 0.3615, 0.3333,  => 0.4211
f1s (macro): 0.4232, 0.5441, 0.3343, 0.3074,  => 0.4022

precs (micro): 0.7627, 0.6556, 0.6041, 0.8556,  => 0.7195
recalls (micro): 0.7627, 0.6556, 0.6041, 0.8556,  => 0.7195
f1s (micro): 0.7627, 0.6556, 0.6041, 0.8556,  => 0.7195

precs (weighed): 0.7432, 0.6640, 0.5044, 0.7321,  => 0.6609
recalls (weighed): 0.7627, 0.6556, 0.6041, 0.8556,  => 0.7195
f1s (weighed): 0.7249, 0.5948, 0.5412, 0.7890,  => 0.6625



Epoch 20/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 20/20, Loss: 1087.7796, 

accs: 0.5076, 0.4815, 0.4106, 0.5129,  => 0.4781

precs (macro): 0.2882, 0.2723, 0.2880, 0.2119,  => 0.2651
recalls (macro): 0.2686, 0.2312, 0.3123, 0.2759,  => 0.2720
f1s (macro): 0.2560, 0.2500, 0.2761, 0.1765,  => 0.2397

precs (micro): 0.5076, 0.4815, 0.4106, 0.5129,  => 0.4781
recalls (micro): 0.5076, 0.4815, 0.4106, 0.5129,  => 0.4781
f1s (micro): 0.5076, 0.4815, 0.4106, 0.5129,  => 0.4781

precs (weighed): 0.6485, 0.5697, 0.4667, 0.7191,  => 0.6010
recalls (weighed): 0.5076, 0.4815, 0.4106, 0.5129,  => 0.4781
f1s (weighed): 0.5685, 0.5218, 0.4313, 0.5986,  => 0.5300


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 127.0939, 

accs: 0.7602, 0.6481, 0.6207, 0.8556,  => 0.7212

precs (macro): 0.4893, 0.6361, 0.3290, 0.2852,  => 0.4349
recalls (macro): 0.4091, 0.5732, 0.3771, 0.3333,  => 0.4232
f1s (macro): 0.4136, 0.5525, 0.3497, 0.3074,  => 0.4058

precs (micro): 0.7602, 0.6481, 0.6207, 0.8556,  => 0.7212
recalls (micro): 0.7602, 0.6481, 0.6207, 0.8556,  => 0.7212
f1s (micro): 0.7602, 0.6481, 0.6207, 0.8556,  => 0.7212

precs (weighed): 0.7419, 0.6399, 0.5259, 0.7321,  => 0.6600
recalls (weighed): 0.7602, 0.6481, 0.6207, 0.8556,  => 0.7212
f1s (weighed): 0.7170, 0.5990, 0.5668, 0.7890,  => 0.6680


lstm + xlm-roberta-base

[18:04:42] task: task-2                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: xlm-roberta-base                                                                my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/lstm_xlm-roberta-base                                          my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

Training ...

Epoch 1/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 1/20, Loss: 1184.7221, 

accs: 0.4304, 0.4518, 0.3158, 0.5050,  => 0.4257

precs (macro): 0.2358, 0.2432, 0.2402, 0.2137,  => 0.2332
recalls (macro): 0.2600, 0.2039, 0.2520, 0.3986,  => 0.2786
f1s (macro): 0.1681, 0.2194, 0.1587, 0.1757,  => 0.1805

precs (micro): 0.4304, 0.4518, 0.3158, 0.5050,  => 0.4257
recalls (micro): 0.4304, 0.4518, 0.3158, 0.5050,  => 0.4257
f1s (micro): 0.4304, 0.4518, 0.3158, 0.5050,  => 0.4257

precs (weighed): 0.5748, 0.5159, 0.3489, 0.7247,  => 0.5411
recalls (weighed): 0.4304, 0.4518, 0.3158, 0.5050,  => 0.4257
f1s (weighed): 0.4678, 0.4775, 0.2708, 0.5949,  => 0.4527


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 152.2162, 

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423

Saved the best model to path: ./models/task_2/lstm_xlm-roberta-base_0.pth


Epoch 2/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 2/20, Loss: 1171.3801, 

accs: 0.4440, 0.4652, 0.3144, 0.5021,  => 0.4314

precs (macro): 0.1854, 0.2529, 0.1361, 0.2122,  => 0.1967
recalls (macro): 0.2843, 0.2108, 0.2538, 0.2103,  => 0.2398
f1s (macro): 0.1724, 0.2274, 0.1581, 0.1744,  => 0.1831

precs (micro): 0.4440, 0.4652, 0.3144, 0.5021,  => 0.4314
recalls (micro): 0.4440, 0.4652, 0.3144, 0.5021,  => 0.4314
f1s (micro): 0.4440, 0.4652, 0.3144, 0.5021,  => 0.4314

precs (weighed): 0.5248, 0.5339, 0.2438, 0.7206,  => 0.5058
recalls (weighed): 0.4440, 0.4652, 0.3144, 0.5021,  => 0.4314
f1s (weighed): 0.4780, 0.4928, 0.2695, 0.5917,  => 0.4580


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 147.7343, 

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423

Saved the best model to path: ./models/task_2/lstm_xlm-roberta-base_1.pth


Epoch 3/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 3/20, Loss: 1177.4061, 

accs: 0.4362, 0.4624, 0.3109, 0.5087,  => 0.4295

precs (macro): 0.1817, 0.2511, 0.1339, 0.2127,  => 0.1948
recalls (macro): 0.2334, 0.2107, 0.2498, 0.2747,  => 0.2422
f1s (macro): 0.1677, 0.2274, 0.1559, 0.1760,  => 0.1817

precs (micro): 0.4362, 0.4624, 0.3109, 0.5087,  => 0.4295
recalls (micro): 0.4362, 0.4624, 0.3109, 0.5087,  => 0.4295
f1s (micro): 0.4362, 0.4624, 0.3109, 0.5087,  => 0.4295

precs (weighed): 0.5177, 0.5308, 0.2399, 0.7219,  => 0.5026
recalls (weighed): 0.4362, 0.4624, 0.3109, 0.5087,  => 0.4295
f1s (weighed): 0.4716, 0.4912, 0.2659, 0.5967,  => 0.4563


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 150.4501, 

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423



Epoch 4/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 4/20, Loss: 1172.7586, 

accs: 0.4374, 0.4700, 0.3095, 0.5104,  => 0.4318

precs (macro): 0.1823, 0.2549, 0.1340, 0.2120,  => 0.1958
recalls (macro): 0.2423, 0.2965, 0.2523, 0.2127,  => 0.2510
f1s (macro): 0.1685, 0.2299, 0.1559, 0.1760,  => 0.1826

precs (micro): 0.4374, 0.4700, 0.3095, 0.5104,  => 0.4318
recalls (micro): 0.4374, 0.4700, 0.3095, 0.5104,  => 0.4318
f1s (micro): 0.4374, 0.4700, 0.3095, 0.5104,  => 0.4318

precs (weighed): 0.5189, 0.5370, 0.2396, 0.7199,  => 0.5038
recalls (weighed): 0.4374, 0.4700, 0.3095, 0.5104,  => 0.4318
f1s (weighed): 0.4726, 0.4967, 0.2649, 0.5972,  => 0.4578


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 147.7015, 

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423

Saved the best model to path: ./models/task_2/lstm_xlm-roberta-base_3.pth


Epoch 5/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 5/20, Loss: 1175.4139, 

accs: 0.4455, 0.4574, 0.3058, 0.5084,  => 0.4293

precs (macro): 0.1844, 0.2463, 0.1328, 0.2120,  => 0.1939
recalls (macro): 0.2593, 0.2072, 0.2424, 0.3371,  => 0.2615
f1s (macro): 0.1715, 0.2229, 0.1534, 0.1758,  => 0.1809

precs (micro): 0.4455, 0.4574, 0.3058, 0.5084,  => 0.4293
recalls (micro): 0.4455, 0.4574, 0.3058, 0.5084,  => 0.4293
f1s (micro): 0.4455, 0.4574, 0.3058, 0.5084,  => 0.4293

precs (weighed): 0.5237, 0.5214, 0.2393, 0.7192,  => 0.5009
recalls (weighed): 0.4455, 0.4574, 0.3058, 0.5084,  => 0.4293
f1s (weighed): 0.4791, 0.4834, 0.2637, 0.5954,  => 0.4554


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 149.7281, 

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423



Epoch 6/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 6/20, Loss: 1173.1447, 

accs: 0.4376, 0.4608, 0.3095, 0.5110,  => 0.4297

precs (macro): 0.1833, 0.2470, 0.1341, 0.2130,  => 0.1943
recalls (macro): 0.2481, 0.2922, 0.2475, 0.2129,  => 0.2502
f1s (macro): 0.1691, 0.2245, 0.1554, 0.1764,  => 0.1814

precs (micro): 0.4376, 0.4608, 0.3095, 0.5110,  => 0.4297
recalls (micro): 0.4376, 0.4608, 0.3095, 0.5110,  => 0.4297
f1s (micro): 0.4376, 0.4608, 0.3095, 0.5110,  => 0.4297

precs (weighed): 0.5213, 0.5224, 0.2409, 0.7233,  => 0.5020
recalls (weighed): 0.4376, 0.4608, 0.3095, 0.5110,  => 0.4297
f1s (weighed): 0.4735, 0.4859, 0.2660, 0.5988,  => 0.4560


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 149.8598, 

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423



Epoch 7/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 7/20, Loss: 1168.8855, 

accs: 0.4358, 0.4641, 0.3149, 0.5129,  => 0.4319

precs (macro): 0.1824, 0.2534, 0.1366, 0.2125,  => 0.1962
recalls (macro): 0.2645, 0.2113, 0.2530, 0.2759,  => 0.2512
f1s (macro): 0.1689, 0.2285, 0.1583, 0.1767,  => 0.1831

precs (micro): 0.4358, 0.4641, 0.3149, 0.5129,  => 0.4319
recalls (micro): 0.4358, 0.4641, 0.3149, 0.5129,  => 0.4319
f1s (micro): 0.4358, 0.4641, 0.3149, 0.5129,  => 0.4319

precs (weighed): 0.5175, 0.5352, 0.2452, 0.7212,  => 0.5048
recalls (weighed): 0.4358, 0.4641, 0.3149, 0.5129,  => 0.4319
f1s (weighed): 0.4705, 0.4937, 0.2706, 0.5993,  => 0.4585


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 150.6584, 

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1214, 0.2852,  => 0.2379
recalls (macro): 0.3333, 0.5000, 0.2500, 0.3333,  => 0.3542
f1s (macro): 0.2782, 0.3798, 0.1634, 0.3074,  => 0.2822

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2357, 0.7321,  => 0.4639
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3173, 0.7890,  => 0.5423



Epoch 8/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 8/20, Loss: 1162.5065, 

accs: 0.4385, 0.4684, 0.3082, 0.5074,  => 0.4306

precs (macro): 0.1834, 0.2532, 0.1391, 0.2132,  => 0.1972
recalls (macro): 0.2711, 0.2126, 0.2560, 0.2743,  => 0.2535
f1s (macro): 0.1700, 0.2287, 0.1582, 0.1759,  => 0.1832

precs (micro): 0.4385, 0.4684, 0.3082, 0.5074,  => 0.4306
recalls (micro): 0.4385, 0.4684, 0.3082, 0.5074,  => 0.4306
f1s (micro): 0.4385, 0.4684, 0.3082, 0.5074,  => 0.4306

precs (weighed): 0.5197, 0.5340, 0.2500, 0.7238,  => 0.5069
recalls (weighed): 0.4385, 0.4684, 0.3082, 0.5074,  => 0.4306
f1s (weighed): 0.4730, 0.4948, 0.2701, 0.5964,  => 0.4585


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 148.5237, 

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1469, 0.2852,  => 0.2443
recalls (macro): 0.3333, 0.5000, 0.2581, 0.3333,  => 0.3562
f1s (macro): 0.2782, 0.3798, 0.1778, 0.3074,  => 0.2858

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2435, 0.7321,  => 0.4659
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3224, 0.7890,  => 0.5436



Epoch 9/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 9/20, Loss: 1164.9468, 

accs: 0.4353, 0.4567, 0.3121, 0.5046,  => 0.4272

precs (macro): 0.1830, 0.2488, 0.1440, 0.2121,  => 0.1970
recalls (macro): 0.2416, 0.2075, 0.2677, 0.2735,  => 0.2476
f1s (macro): 0.1683, 0.2240, 0.1625, 0.1749,  => 0.1824

precs (micro): 0.4353, 0.4567, 0.3121, 0.5046,  => 0.4272
recalls (micro): 0.4353, 0.4567, 0.3121, 0.5046,  => 0.4272
f1s (micro): 0.4353, 0.4567, 0.3121, 0.5046,  => 0.4272

precs (weighed): 0.5209, 0.5250, 0.2583, 0.7200,  => 0.5060
recalls (weighed): 0.4353, 0.4567, 0.3121, 0.5046,  => 0.4272
f1s (weighed): 0.4722, 0.4845, 0.2757, 0.5932,  => 0.4564


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 144.9663, 

accs: 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (macro): 0.2387, 0.3062, 0.1376, 0.2852,  => 0.2419
recalls (macro): 0.3333, 0.5000, 0.2581, 0.3333,  => 0.3562
f1s (macro): 0.2782, 0.3798, 0.1770, 0.3074,  => 0.2856

precs (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
recalls (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (micro): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674

precs (weighed): 0.5129, 0.3751, 0.2453, 0.7321,  => 0.4663
recalls (weighed): 0.7162, 0.6124, 0.4855, 0.8556,  => 0.6674
f1s (weighed): 0.5977, 0.4652, 0.3254, 0.7890,  => 0.5444

Saved the best model to path: ./models/task_2/lstm_xlm-roberta-base_8.pth


Epoch 10/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 10/20, Loss: 1157.6848, 

accs: 0.4434, 0.4613, 0.2958, 0.5118,  => 0.4281

precs (macro): 0.1851, 0.2495, 0.1475, 0.2126,  => 0.1987
recalls (macro): 0.2643, 0.2926, 0.2702, 0.2756,  => 0.2757
f1s (macro): 0.1715, 0.2259, 0.1600, 0.1765,  => 0.1835

precs (micro): 0.4434, 0.4613, 0.2958, 0.5118,  => 0.4281
recalls (micro): 0.4434, 0.4613, 0.2958, 0.5118,  => 0.4281
f1s (micro): 0.4434, 0.4613, 0.2958, 0.5118,  => 0.4281

precs (weighed): 0.5254, 0.5277, 0.2654, 0.7214,  => 0.5100
recalls (weighed): 0.4434, 0.4613, 0.2958, 0.5118,  => 0.4281
f1s (weighed): 0.4784, 0.4886, 0.2709, 0.5986,  => 0.4591


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 146.3707, 

accs: 0.7162, 0.6124, 0.4788, 0.8556,  => 0.6658

precs (macro): 0.2387, 0.3062, 0.1463, 0.2852,  => 0.2441
recalls (macro): 0.3333, 0.5000, 0.2748, 0.3333,  => 0.3604
f1s (macro): 0.2782, 0.3798, 0.1906, 0.3074,  => 0.2890

precs (micro): 0.7162, 0.6124, 0.4788, 0.8556,  => 0.6658
recalls (micro): 0.7162, 0.6124, 0.4788, 0.8556,  => 0.6658
f1s (micro): 0.7162, 0.6124, 0.4788, 0.8556,  => 0.6658

precs (weighed): 0.5129, 0.3751, 0.2622, 0.7321,  => 0.4706
recalls (weighed): 0.7162, 0.6124, 0.4788, 0.8556,  => 0.6658
f1s (weighed): 0.5977, 0.4652, 0.3387, 0.7890,  => 0.5477



Epoch 11/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 11/20, Loss: 1159.0498, 

accs: 0.4345, 0.4696, 0.3005, 0.5137,  => 0.4296

precs (macro): 0.1815, 0.2532, 0.1478, 0.2136,  => 0.1990
recalls (macro): 0.2414, 0.2136, 0.2720, 0.2762,  => 0.2508
f1s (macro): 0.1675, 0.2294, 0.1615, 0.1773,  => 0.1839

precs (micro): 0.4345, 0.4696, 0.3005, 0.5137,  => 0.4296
recalls (micro): 0.4345, 0.4696, 0.3005, 0.5137,  => 0.4296
f1s (micro): 0.4345, 0.4696, 0.3005, 0.5137,  => 0.4296

precs (weighed): 0.5164, 0.5336, 0.2657, 0.7251,  => 0.5102
recalls (weighed): 0.4345, 0.4696, 0.3005, 0.5137,  => 0.4296
f1s (weighed): 0.4698, 0.4955, 0.2734, 0.6012,  => 0.4600


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 146.3659, 

accs: 0.7162, 0.6124, 0.4830, 0.8556,  => 0.6668

precs (macro): 0.2387, 0.3062, 0.1570, 0.2852,  => 0.2468
recalls (macro): 0.3333, 0.5000, 0.2972, 0.3333,  => 0.3660
f1s (macro): 0.2782, 0.3798, 0.2028, 0.3074,  => 0.2920

precs (micro): 0.7162, 0.6124, 0.4830, 0.8556,  => 0.6668
recalls (micro): 0.7162, 0.6124, 0.4830, 0.8556,  => 0.6668
f1s (micro): 0.7162, 0.6124, 0.4830, 0.8556,  => 0.6668

precs (weighed): 0.5129, 0.3751, 0.2789, 0.7321,  => 0.4747
recalls (weighed): 0.7162, 0.6124, 0.4830, 0.8556,  => 0.6668
f1s (weighed): 0.5977, 0.4652, 0.3530, 0.7890,  => 0.5513



Epoch 12/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 12/20, Loss: 1155.6098, 

accs: 0.4406, 0.4641, 0.3080, 0.5087,  => 0.4304

precs (macro): 0.1840, 0.2514, 0.1525, 0.2128,  => 0.2002
recalls (macro): 0.2689, 0.2100, 0.2835, 0.3372,  => 0.2749
f1s (macro): 0.1706, 0.2260, 0.1664, 0.1761,  => 0.1848

precs (micro): 0.4406, 0.4641, 0.3080, 0.5087,  => 0.4304
recalls (micro): 0.4406, 0.4641, 0.3080, 0.5087,  => 0.4304
f1s (micro): 0.4406, 0.4641, 0.3080, 0.5087,  => 0.4304

precs (weighed): 0.5217, 0.5298, 0.2736, 0.7220,  => 0.5118
recalls (weighed): 0.4406, 0.4641, 0.3080, 0.5087,  => 0.4304
f1s (weighed): 0.4750, 0.4897, 0.2806, 0.5966,  => 0.4605


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 144.4351, 

accs: 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672

precs (macro): 0.2387, 0.3062, 0.1224, 0.2852,  => 0.2381
recalls (macro): 0.3333, 0.5000, 0.2496, 0.3333,  => 0.3541
f1s (macro): 0.2782, 0.3798, 0.1642, 0.3074,  => 0.2824

precs (micro): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672
recalls (micro): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672
f1s (micro): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672

precs (weighed): 0.5129, 0.3751, 0.2377, 0.7321,  => 0.4644
recalls (weighed): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672
f1s (weighed): 0.5977, 0.4652, 0.3189, 0.7890,  => 0.5427

Saved the best model to path: ./models/task_2/lstm_xlm-roberta-base_11.pth


Epoch 13/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 13/20, Loss: 1160.5207, 

accs: 0.4340, 0.4611, 0.2904, 0.5106,  => 0.4240

precs (macro): 0.1824, 0.2521, 0.1464, 0.2117,  => 0.1981
recalls (macro): 0.2327, 0.2102, 0.2577, 0.2753,  => 0.2440
f1s (macro): 0.1675, 0.2273, 0.1570, 0.1760,  => 0.1820

precs (micro): 0.4340, 0.4611, 0.2904, 0.5106,  => 0.4240
recalls (micro): 0.4340, 0.4611, 0.2904, 0.5106,  => 0.4240
f1s (micro): 0.4340, 0.4611, 0.2904, 0.5106,  => 0.4240

precs (weighed): 0.5197, 0.5317, 0.2653, 0.7185,  => 0.5088
recalls (weighed): 0.4340, 0.4611, 0.2904, 0.5106,  => 0.4240
f1s (weighed): 0.4711, 0.4905, 0.2689, 0.5968,  => 0.4568


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 146.1971, 

accs: 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672

precs (macro): 0.2387, 0.3062, 0.1226, 0.2852,  => 0.2382
recalls (macro): 0.3333, 0.5000, 0.2496, 0.3333,  => 0.3541
f1s (macro): 0.2782, 0.3798, 0.1644, 0.3074,  => 0.2825

precs (micro): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672
recalls (micro): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672
f1s (micro): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672

precs (weighed): 0.5129, 0.3751, 0.2381, 0.7321,  => 0.4645
recalls (weighed): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672
f1s (weighed): 0.5977, 0.4652, 0.3193, 0.7890,  => 0.5428



Epoch 14/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 14/20, Loss: 1160.4761, 

accs: 0.4325, 0.4637, 0.3018, 0.5100,  => 0.4270

precs (macro): 0.1820, 0.2491, 0.1460, 0.2128,  => 0.1975
recalls (macro): 0.2576, 0.2109, 0.2700, 0.1502,  => 0.2222
f1s (macro): 0.1678, 0.2264, 0.1609, 0.1761,  => 0.1828

precs (micro): 0.4325, 0.4637, 0.3018, 0.5100,  => 0.4270
recalls (micro): 0.4325, 0.4637, 0.3018, 0.5100,  => 0.4270
f1s (micro): 0.4325, 0.4637, 0.3018, 0.5100,  => 0.4270

precs (weighed): 0.5168, 0.5266, 0.2621, 0.7227,  => 0.5070
recalls (weighed): 0.4325, 0.4637, 0.3018, 0.5100,  => 0.4270
f1s (weighed): 0.4684, 0.4896, 0.2724, 0.5980,  => 0.4571


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 147.2577, 

accs: 0.7162, 0.6124, 0.4863, 0.8556,  => 0.6676

precs (macro): 0.2387, 0.3062, 0.1480, 0.2852,  => 0.2445
recalls (macro): 0.3333, 0.5000, 0.2787, 0.3333,  => 0.3613
f1s (macro): 0.2782, 0.3798, 0.1933, 0.3074,  => 0.2897

precs (micro): 0.7162, 0.6124, 0.4863, 0.8556,  => 0.6676
recalls (micro): 0.7162, 0.6124, 0.4863, 0.8556,  => 0.6676
f1s (micro): 0.7162, 0.6124, 0.4863, 0.8556,  => 0.6676

precs (weighed): 0.5129, 0.3751, 0.2611, 0.7321,  => 0.4703
recalls (weighed): 0.7162, 0.6124, 0.4863, 0.8556,  => 0.6676
f1s (weighed): 0.5977, 0.4652, 0.3398, 0.7890,  => 0.5480



Epoch 15/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 15/20, Loss: 1154.4370, 

accs: 0.4282, 0.4665, 0.3154, 0.5116,  => 0.4304

precs (macro): 0.1832, 0.2539, 0.1476, 0.2130,  => 0.1994
recalls (macro): 0.2618, 0.2952, 0.2781, 0.2755,  => 0.2777
f1s (macro): 0.1675, 0.2290, 0.1658, 0.1766,  => 0.1847

precs (micro): 0.4282, 0.4665, 0.3154, 0.5116,  => 0.4304
recalls (micro): 0.4282, 0.4665, 0.3154, 0.5116,  => 0.4304
f1s (micro): 0.4282, 0.4665, 0.3154, 0.5116,  => 0.4304

precs (weighed): 0.5201, 0.5358, 0.2640, 0.7228,  => 0.5107
recalls (weighed): 0.4282, 0.4665, 0.3154, 0.5116,  => 0.4304
f1s (weighed): 0.4670, 0.4948, 0.2798, 0.5989,  => 0.4601


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 145.4577, 

accs: 0.7162, 0.6124, 0.4788, 0.8556,  => 0.6658

precs (macro): 0.2387, 0.3062, 0.1612, 0.2852,  => 0.2478
recalls (macro): 0.3333, 0.5000, 0.3071, 0.3333,  => 0.3685
f1s (macro): 0.2782, 0.3798, 0.2066, 0.3074,  => 0.2930

precs (micro): 0.7162, 0.6124, 0.4788, 0.8556,  => 0.6658
recalls (micro): 0.7162, 0.6124, 0.4788, 0.8556,  => 0.6658
f1s (micro): 0.7162, 0.6124, 0.4788, 0.8556,  => 0.6658

precs (weighed): 0.5129, 0.3751, 0.2856, 0.7321,  => 0.4764
recalls (weighed): 0.7162, 0.6124, 0.4788, 0.8556,  => 0.6658
f1s (weighed): 0.5977, 0.4652, 0.3568, 0.7890,  => 0.5522



Epoch 16/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 16/20, Loss: 1156.0154, 

accs: 0.4439, 0.4583, 0.2955, 0.5079,  => 0.4264

precs (macro): 0.1847, 0.2495, 0.1474, 0.2128,  => 0.1986
recalls (macro): 0.2644, 0.2078, 0.2700, 0.1495,  => 0.2230
f1s (macro): 0.1715, 0.2245, 0.1598, 0.1757,  => 0.1829

precs (micro): 0.4439, 0.4583, 0.2955, 0.5079,  => 0.4264
recalls (micro): 0.4439, 0.4583, 0.2955, 0.5079,  => 0.4264
f1s (micro): 0.4439, 0.4583, 0.2955, 0.5079,  => 0.4264

precs (weighed): 0.5243, 0.5273, 0.2651, 0.7229,  => 0.5099
recalls (weighed): 0.4439, 0.4583, 0.2955, 0.5079,  => 0.4264
f1s (weighed): 0.4782, 0.4864, 0.2706, 0.5966,  => 0.4580


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 141.8422, 

accs: 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672

precs (macro): 0.2387, 0.3062, 0.1220, 0.2852,  => 0.2380
recalls (macro): 0.3333, 0.5000, 0.2496, 0.3333,  => 0.3541
f1s (macro): 0.2782, 0.3798, 0.1639, 0.3074,  => 0.2823

precs (micro): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672
recalls (micro): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672
f1s (micro): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672

precs (weighed): 0.5129, 0.3751, 0.2369, 0.7321,  => 0.4642
recalls (weighed): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672
f1s (weighed): 0.5977, 0.4652, 0.3182, 0.7890,  => 0.5426

Saved the best model to path: ./models/task_2/lstm_xlm-roberta-base_15.pth


Epoch 17/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 17/20, Loss: 1161.1739, 

accs: 0.4371, 0.4586, 0.3015, 0.5009,  => 0.4245

precs (macro): 0.1825, 0.2472, 0.1503, 0.2116,  => 0.1979
recalls (macro): 0.2593, 0.2083, 0.2807, 0.2099,  => 0.2395
f1s (macro): 0.1690, 0.2240, 0.1635, 0.1739,  => 0.1826

precs (micro): 0.4371, 0.4586, 0.3015, 0.5009,  => 0.4245
recalls (micro): 0.4371, 0.4586, 0.3015, 0.5009,  => 0.4245
f1s (micro): 0.4371, 0.4586, 0.3015, 0.5009,  => 0.4245

precs (weighed): 0.5181, 0.5230, 0.2694, 0.7185,  => 0.5072
recalls (weighed): 0.4371, 0.4586, 0.3015, 0.5009,  => 0.4245
f1s (weighed): 0.4717, 0.4851, 0.2752, 0.5902,  => 0.4555


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 145.9682, 

accs: 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672

precs (macro): 0.2387, 0.3062, 0.1378, 0.2852,  => 0.2420
recalls (macro): 0.3333, 0.5000, 0.2576, 0.3333,  => 0.3561
f1s (macro): 0.2782, 0.3798, 0.1767, 0.3074,  => 0.2855

precs (micro): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672
recalls (micro): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672
f1s (micro): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672

precs (weighed): 0.5129, 0.3751, 0.2446, 0.7321,  => 0.4662
recalls (weighed): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672
f1s (weighed): 0.5977, 0.4652, 0.3245, 0.7890,  => 0.5441



Epoch 18/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 18/20, Loss: 1160.7510, 

accs: 0.4342, 0.4649, 0.2947, 0.5022,  => 0.4240

precs (macro): 0.1826, 0.2514, 0.1501, 0.2115,  => 0.1989
recalls (macro): 0.2384, 0.2117, 0.2756, 0.1479,  => 0.2184
f1s (macro): 0.1678, 0.2279, 0.1611, 0.1740,  => 0.1827

precs (micro): 0.4342, 0.4649, 0.2947, 0.5022,  => 0.4240
recalls (micro): 0.4342, 0.4649, 0.2947, 0.5022,  => 0.4240
f1s (micro): 0.4342, 0.4649, 0.2947, 0.5022,  => 0.4240

precs (weighed): 0.5200, 0.5310, 0.2698, 0.7184,  => 0.5098
recalls (weighed): 0.4342, 0.4649, 0.2947, 0.5022,  => 0.4240
f1s (weighed): 0.4712, 0.4923, 0.2720, 0.5911,  => 0.4567


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 144.2830, 

accs: 0.7162, 0.6124, 0.4722, 0.8556,  => 0.6641

precs (macro): 0.2387, 0.3062, 0.1459, 0.2852,  => 0.2440
recalls (macro): 0.3333, 0.5000, 0.2755, 0.3333,  => 0.3605
f1s (macro): 0.2782, 0.3798, 0.1904, 0.3074,  => 0.2890

precs (micro): 0.7162, 0.6124, 0.4722, 0.8556,  => 0.6641
recalls (micro): 0.7162, 0.6124, 0.4722, 0.8556,  => 0.6641
f1s (micro): 0.7162, 0.6124, 0.4722, 0.8556,  => 0.6641

precs (weighed): 0.5129, 0.3751, 0.2569, 0.7321,  => 0.4692
recalls (weighed): 0.7162, 0.6124, 0.4722, 0.8556,  => 0.6641
f1s (weighed): 0.5977, 0.4652, 0.3327, 0.7890,  => 0.5462



Epoch 19/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 19/20, Loss: 1163.2157, 

accs: 0.4331, 0.4491, 0.2981, 0.5059,  => 0.4215

precs (macro): 0.1807, 0.2431, 0.1515, 0.2116,  => 0.1967
recalls (macro): 0.2465, 0.2870, 0.2773, 0.2739,  => 0.2712
f1s (macro): 0.1671, 0.2201, 0.1628, 0.1750,  => 0.1812

precs (micro): 0.4331, 0.4491, 0.2981, 0.5059,  => 0.4215
recalls (micro): 0.4331, 0.4491, 0.2981, 0.5059,  => 0.4215
f1s (micro): 0.4331, 0.4491, 0.2981, 0.5059,  => 0.4215

precs (weighed): 0.5137, 0.5153, 0.2725, 0.7183,  => 0.5049
recalls (weighed): 0.4331, 0.4491, 0.2981, 0.5059,  => 0.4215
f1s (weighed): 0.4677, 0.4766, 0.2751, 0.5935,  => 0.4532


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 143.8138, 

accs: 0.7162, 0.6124, 0.4805, 0.8556,  => 0.6662

precs (macro): 0.2387, 0.3062, 0.1492, 0.2852,  => 0.2448
recalls (macro): 0.3333, 0.5000, 0.2757, 0.3333,  => 0.3606
f1s (macro): 0.2782, 0.3798, 0.1924, 0.3074,  => 0.2895

precs (micro): 0.7162, 0.6124, 0.4805, 0.8556,  => 0.6662
recalls (micro): 0.7162, 0.6124, 0.4805, 0.8556,  => 0.6662
f1s (micro): 0.7162, 0.6124, 0.4805, 0.8556,  => 0.6662

precs (weighed): 0.5129, 0.3751, 0.2725, 0.7321,  => 0.4731
recalls (weighed): 0.7162, 0.6124, 0.4805, 0.8556,  => 0.6662
f1s (weighed): 0.5977, 0.4652, 0.3475, 0.7890,  => 0.5499



Epoch 20/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 20/20, Loss: 1149.8099, 

accs: 0.4427, 0.4684, 0.3008, 0.5103,  => 0.4305

precs (macro): 0.1843, 0.2513, 0.1511, 0.2143,  => 0.2002
recalls (macro): 0.2470, 0.2955, 0.2733, 0.2752,  => 0.2728
f1s (macro): 0.1705, 0.2279, 0.1630, 0.1768,  => 0.1846

precs (micro): 0.4427, 0.4684, 0.3008, 0.5103,  => 0.4305
recalls (micro): 0.4427, 0.4684, 0.3008, 0.5103,  => 0.4305
f1s (micro): 0.4427, 0.4684, 0.3008, 0.5103,  => 0.4305

precs (weighed): 0.5243, 0.5304, 0.2725, 0.7273,  => 0.5136
recalls (weighed): 0.4427, 0.4684, 0.3008, 0.5103,  => 0.4305
f1s (weighed): 0.4779, 0.4931, 0.2769, 0.5996,  => 0.4619


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 145.8380, 

accs: 0.7162, 0.6124, 0.4822, 0.8556,  => 0.6666

precs (macro): 0.2387, 0.3062, 0.1440, 0.2852,  => 0.2435
recalls (macro): 0.3333, 0.5000, 0.2685, 0.3333,  => 0.3588
f1s (macro): 0.2782, 0.3798, 0.1872, 0.3074,  => 0.2882

precs (micro): 0.7162, 0.6124, 0.4822, 0.8556,  => 0.6666
recalls (micro): 0.7162, 0.6124, 0.4822, 0.8556,  => 0.6666
f1s (micro): 0.7162, 0.6124, 0.4822, 0.8556,  => 0.6666

precs (weighed): 0.5129, 0.3751, 0.2635, 0.7321,  => 0.4709
recalls (weighed): 0.7162, 0.6124, 0.4822, 0.8556,  => 0.6666
f1s (weighed): 0.5977, 0.4652, 0.3407, 0.7890,  => 0.5482


lstm + bert-base-multilingual-cased

[18:35:19] task: task-2                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: bert-base-multilingual-cased                                                    my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/lstm_bert-base-multilingual-cased                              my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

Training ...

Epoch 1/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 1/20, Loss: 1189.3150, 

accs: 0.4052, 0.4605, 0.3154, 0.5039,  => 0.4212

precs (macro): 0.1829, 0.2524, 0.1711, 0.2444,  => 0.2127
recalls (macro): 0.2567, 0.2936, 0.2532, 0.2115,  => 0.2537
f1s (macro): 0.1622, 0.2280, 0.1592, 0.1760,  => 0.1814

precs (micro): 0.4052, 0.4605, 0.3154, 0.5039,  => 0.4212
recalls (micro): 0.4052, 0.4605, 0.3154, 0.5039,  => 0.4212
f1s (micro): 0.4052, 0.4605, 0.3154, 0.5039,  => 0.4212

precs (weighed): 0.5195, 0.5327, 0.2608, 0.7338,  => 0.5117
recalls (weighed): 0.4052, 0.4605, 0.3154, 0.5039,  => 0.4212
f1s (weighed): 0.4524, 0.4910, 0.2698, 0.5928,  => 0.4515


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 149.7620, 

accs: 0.7162, 0.5776, 0.4855, 0.8556,  => 0.6587

precs (macro): 0.2387, 0.5605, 0.1214, 0.2852,  => 0.3015
recalls (macro): 0.3333, 0.5620, 0.2500, 0.3333,  => 0.3697
f1s (macro): 0.2782, 0.5607, 0.1634, 0.3074,  => 0.3274

precs (micro): 0.7162, 0.5776, 0.4855, 0.8556,  => 0.6587
recalls (micro): 0.7162, 0.5776, 0.4855, 0.8556,  => 0.6587
f1s (micro): 0.7162, 0.5776, 0.4855, 0.8556,  => 0.6587

precs (weighed): 0.5129, 0.5835, 0.2357, 0.7321,  => 0.5160
recalls (weighed): 0.7162, 0.5776, 0.4855, 0.8556,  => 0.6587
f1s (weighed): 0.5977, 0.5801, 0.3173, 0.7890,  => 0.5710

Saved the best model to path: ./models/task_2/lstm_bert-base-multilingual-cased_0.pth


Epoch 2/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 2/20, Loss: 1176.6560, 

accs: 0.4397, 0.4521, 0.3110, 0.5110,  => 0.4284

precs (macro): 0.1810, 0.2462, 0.1346, 0.2121,  => 0.1935
recalls (macro): 0.2290, 0.2914, 0.2445, 0.2754,  => 0.2601
f1s (macro): 0.1680, 0.2250, 0.1556, 0.1762,  => 0.1812

precs (micro): 0.4397, 0.4521, 0.3110, 0.5110,  => 0.4284
recalls (micro): 0.4397, 0.4521, 0.3110, 0.5110,  => 0.4284
f1s (micro): 0.4397, 0.4521, 0.3110, 0.5110,  => 0.4284

precs (weighed): 0.5160, 0.5201, 0.2429, 0.7198,  => 0.4997
recalls (weighed): 0.4397, 0.4521, 0.3110, 0.5110,  => 0.4284
f1s (weighed): 0.4731, 0.4821, 0.2680, 0.5975,  => 0.4552


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 148.1490, 

accs: 0.7162, 0.6324, 0.4855, 0.8556,  => 0.6724

precs (macro): 0.2387, 0.6557, 0.1214, 0.2852,  => 0.3252
recalls (macro): 0.3333, 0.5340, 0.2500, 0.3333,  => 0.3627
f1s (macro): 0.2782, 0.4664, 0.1634, 0.3074,  => 0.3039

precs (micro): 0.7162, 0.6324, 0.4855, 0.8556,  => 0.6724
recalls (micro): 0.7162, 0.6324, 0.4855, 0.8556,  => 0.6724
f1s (micro): 0.7162, 0.6324, 0.4855, 0.8556,  => 0.6724

precs (weighed): 0.5129, 0.6498, 0.2357, 0.7321,  => 0.5326
recalls (weighed): 0.7162, 0.6324, 0.4855, 0.8556,  => 0.6724
f1s (weighed): 0.5977, 0.5333, 0.3173, 0.7890,  => 0.5594

Saved the best model to path: ./models/task_2/lstm_bert-base-multilingual-cased_1.pth


Epoch 3/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 3/20, Loss: 1169.6653, 

accs: 0.4363, 0.4666, 0.3158, 0.5105,  => 0.4323

precs (macro): 0.1821, 0.2613, 0.1364, 0.2127,  => 0.1981
recalls (macro): 0.2448, 0.3016, 0.2572, 0.2128,  => 0.2541
f1s (macro): 0.1683, 0.2379, 0.1589, 0.1762,  => 0.1853

precs (micro): 0.4363, 0.4666, 0.3158, 0.5105,  => 0.4323
recalls (micro): 0.4363, 0.4666, 0.3158, 0.5105,  => 0.4323
f1s (micro): 0.4363, 0.4666, 0.3158, 0.5105,  => 0.4323

precs (weighed): 0.5180, 0.5501, 0.2437, 0.7222,  => 0.5085
recalls (weighed): 0.4363, 0.4666, 0.3158, 0.5105,  => 0.4323
f1s (weighed): 0.4715, 0.5044, 0.2698, 0.5981,  => 0.4610


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 146.9911, 

accs: 0.7162, 0.6249, 0.4855, 0.8556,  => 0.6705

precs (macro): 0.2387, 0.5902, 0.1214, 0.2852,  => 0.3089
recalls (macro): 0.3333, 0.5503, 0.2500, 0.3333,  => 0.3667
f1s (macro): 0.2782, 0.5266, 0.1634, 0.3074,  => 0.3189

precs (micro): 0.7162, 0.6249, 0.4855, 0.8556,  => 0.6705
recalls (micro): 0.7162, 0.6249, 0.4855, 0.8556,  => 0.6705
f1s (micro): 0.7162, 0.6249, 0.4855, 0.8556,  => 0.6705

precs (weighed): 0.5129, 0.6016, 0.2357, 0.7321,  => 0.5206
recalls (weighed): 0.7162, 0.6249, 0.4855, 0.8556,  => 0.6705
f1s (weighed): 0.5977, 0.5751, 0.3173, 0.7890,  => 0.5698

Saved the best model to path: ./models/task_2/lstm_bert-base-multilingual-cased_2.pth


Epoch 4/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 4/20, Loss: 1165.7489, 

accs: 0.4307, 0.4786, 0.3153, 0.5101,  => 0.4337

precs (macro): 0.1831, 0.2658, 0.1358, 0.2129,  => 0.1994
recalls (macro): 0.2570, 0.3069, 0.2542, 0.2127,  => 0.2577
f1s (macro): 0.1679, 0.2426, 0.1582, 0.1762,  => 0.1862

precs (micro): 0.4307, 0.4786, 0.3153, 0.5101,  => 0.4337
recalls (micro): 0.4307, 0.4786, 0.3153, 0.5101,  => 0.4337
f1s (micro): 0.4307, 0.4786, 0.3153, 0.5101,  => 0.4337

precs (weighed): 0.5201, 0.5569, 0.2430, 0.7229,  => 0.5107
recalls (weighed): 0.4307, 0.4786, 0.3153, 0.5101,  => 0.4337
f1s (weighed): 0.4687, 0.5137, 0.2694, 0.5981,  => 0.4625


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 149.3166, 

accs: 0.7162, 0.6257, 0.4855, 0.8556,  => 0.6707

precs (macro): 0.2387, 0.6043, 0.1214, 0.2852,  => 0.3124
recalls (macro): 0.3333, 0.5340, 0.2500, 0.3333,  => 0.3627
f1s (macro): 0.2782, 0.4812, 0.1634, 0.3074,  => 0.3076

precs (micro): 0.7162, 0.6257, 0.4855, 0.8556,  => 0.6707
recalls (micro): 0.7162, 0.6257, 0.4855, 0.8556,  => 0.6707
f1s (micro): 0.7162, 0.6257, 0.4855, 0.8556,  => 0.6707

precs (weighed): 0.5129, 0.6101, 0.2357, 0.7321,  => 0.5227
recalls (weighed): 0.7162, 0.6257, 0.4855, 0.8556,  => 0.6707
f1s (weighed): 0.5977, 0.5428, 0.3173, 0.7890,  => 0.5617



Epoch 5/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 5/20, Loss: 1168.3247, 

accs: 0.4281, 0.4721, 0.3207, 0.5140,  => 0.4337

precs (macro): 0.1821, 0.2614, 0.1363, 0.2121,  => 0.1980
recalls (macro): 0.2533, 0.3041, 0.2608, 0.3387,  => 0.2892
f1s (macro): 0.1668, 0.2393, 0.1604, 0.1769,  => 0.1859

precs (micro): 0.4281, 0.4721, 0.3207, 0.5140,  => 0.4337
recalls (micro): 0.4281, 0.4721, 0.3207, 0.5140,  => 0.4337
f1s (micro): 0.4281, 0.4721, 0.3207, 0.5140,  => 0.4337

precs (weighed): 0.5173, 0.5488, 0.2425, 0.7196,  => 0.5071
recalls (weighed): 0.4281, 0.4721, 0.3207, 0.5140,  => 0.4337
f1s (weighed): 0.4660, 0.5069, 0.2711, 0.5995,  => 0.4609


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 148.3770, 

accs: 0.7162, 0.6133, 0.4855, 0.8556,  => 0.6676

precs (macro): 0.2387, 0.5784, 0.1214, 0.2852,  => 0.3059
recalls (macro): 0.3333, 0.5655, 0.2500, 0.3333,  => 0.3706
f1s (macro): 0.2782, 0.5629, 0.1634, 0.3074,  => 0.3280

precs (micro): 0.7162, 0.6133, 0.4855, 0.8556,  => 0.6676
recalls (micro): 0.7162, 0.6133, 0.4855, 0.8556,  => 0.6676
f1s (micro): 0.7162, 0.6133, 0.4855, 0.8556,  => 0.6676

precs (weighed): 0.5129, 0.5957, 0.2357, 0.7321,  => 0.5191
recalls (weighed): 0.7162, 0.6133, 0.4855, 0.8556,  => 0.6676
f1s (weighed): 0.5977, 0.5963, 0.3173, 0.7890,  => 0.5751



Epoch 6/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 6/20, Loss: 1166.8489, 

accs: 0.4343, 0.4694, 0.3126, 0.5065,  => 0.4307

precs (macro): 0.1834, 0.2634, 0.1354, 0.2129,  => 0.1988
recalls (macro): 0.2611, 0.3874, 0.2615, 0.3365,  => 0.3116
f1s (macro): 0.1688, 0.2406, 0.1583, 0.1757,  => 0.1859

precs (micro): 0.4343, 0.4694, 0.3126, 0.5065,  => 0.4307
recalls (micro): 0.4343, 0.4694, 0.3126, 0.5065,  => 0.4307
f1s (micro): 0.4343, 0.4694, 0.3126, 0.5065,  => 0.4307

precs (weighed): 0.5206, 0.5524, 0.2404, 0.7224,  => 0.5089
recalls (weighed): 0.4343, 0.4694, 0.3126, 0.5065,  => 0.4307
f1s (weighed): 0.4709, 0.5070, 0.2662, 0.5952,  => 0.4599


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 146.8656, 

accs: 0.7162, 0.6315, 0.4855, 0.8556,  => 0.6722

precs (macro): 0.2387, 0.6047, 0.1214, 0.2852,  => 0.3125
recalls (macro): 0.3333, 0.5537, 0.2500, 0.3333,  => 0.3676
f1s (macro): 0.2782, 0.5268, 0.1634, 0.3074,  => 0.3189

precs (micro): 0.7162, 0.6315, 0.4855, 0.8556,  => 0.6722
recalls (micro): 0.7162, 0.6315, 0.4855, 0.8556,  => 0.6722
f1s (micro): 0.7162, 0.6315, 0.4855, 0.8556,  => 0.6722

precs (weighed): 0.5129, 0.6131, 0.2357, 0.7321,  => 0.5234
recalls (weighed): 0.7162, 0.6315, 0.4855, 0.8556,  => 0.6722
f1s (weighed): 0.5977, 0.5768, 0.3173, 0.7890,  => 0.5702

Saved the best model to path: ./models/task_2/lstm_bert-base-multilingual-cased_5.pth


Epoch 7/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 7/20, Loss: 1157.9281, 

accs: 0.4406, 0.4732, 0.3207, 0.5100,  => 0.4361

precs (macro): 0.1826, 0.2651, 0.1356, 0.2130,  => 0.1991
recalls (macro): 0.2463, 0.2219, 0.2505, 0.2751,  => 0.2485
f1s (macro): 0.1694, 0.2413, 0.1588, 0.1763,  => 0.1865

precs (micro): 0.4406, 0.4732, 0.3207, 0.5100,  => 0.4361
recalls (micro): 0.4406, 0.4732, 0.3207, 0.5100,  => 0.4361
f1s (micro): 0.4406, 0.4732, 0.3207, 0.5100,  => 0.4361

precs (weighed): 0.5195, 0.5564, 0.2436, 0.7229,  => 0.5106
recalls (weighed): 0.4406, 0.4732, 0.3207, 0.5100,  => 0.4361
f1s (weighed): 0.4746, 0.5109, 0.2724, 0.5979,  => 0.4640


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 145.1314, 

accs: 0.7162, 0.6307, 0.4855, 0.8556,  => 0.6720

precs (macro): 0.2387, 0.6081, 0.1214, 0.2852,  => 0.3133
recalls (macro): 0.3333, 0.5468, 0.2500, 0.3333,  => 0.3659
f1s (macro): 0.2782, 0.5100, 0.1634, 0.3074,  => 0.3148

precs (micro): 0.7162, 0.6307, 0.4855, 0.8556,  => 0.6720
recalls (micro): 0.7162, 0.6307, 0.4855, 0.8556,  => 0.6720
f1s (micro): 0.7162, 0.6307, 0.4855, 0.8556,  => 0.6720

precs (weighed): 0.5129, 0.6147, 0.2357, 0.7321,  => 0.5238
recalls (weighed): 0.7162, 0.6307, 0.4855, 0.8556,  => 0.6720
f1s (weighed): 0.5977, 0.5647, 0.3173, 0.7890,  => 0.5672

Saved the best model to path: ./models/task_2/lstm_bert-base-multilingual-cased_6.pth


Epoch 8/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 8/20, Loss: 1160.6492, 

accs: 0.4394, 0.4756, 0.3124, 0.5072,  => 0.4337

precs (macro): 0.1833, 0.2658, 0.1338, 0.2122,  => 0.1988
recalls (macro): 0.2430, 0.2231, 0.2490, 0.3367,  => 0.2630
f1s (macro): 0.1693, 0.2422, 0.1561, 0.1756,  => 0.1858

precs (micro): 0.4394, 0.4756, 0.3124, 0.5072,  => 0.4337
recalls (micro): 0.4394, 0.4756, 0.3124, 0.5072,  => 0.4337
f1s (micro): 0.4394, 0.4756, 0.3124, 0.5072,  => 0.4337

precs (weighed): 0.5218, 0.5562, 0.2398, 0.7201,  => 0.5095
recalls (weighed): 0.4394, 0.4756, 0.3124, 0.5072,  => 0.4337
f1s (weighed): 0.4750, 0.5120, 0.2665, 0.5949,  => 0.4621


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 147.7888, 

accs: 0.7162, 0.6216, 0.4855, 0.8556,  => 0.6697

precs (macro): 0.2387, 0.5862, 0.1214, 0.2852,  => 0.3079
recalls (macro): 0.3333, 0.5652, 0.2500, 0.3333,  => 0.3705
f1s (macro): 0.2782, 0.5587, 0.1634, 0.3074,  => 0.3269

precs (micro): 0.7162, 0.6216, 0.4855, 0.8556,  => 0.6697
recalls (micro): 0.7162, 0.6216, 0.4855, 0.8556,  => 0.6697
f1s (micro): 0.7162, 0.6216, 0.4855, 0.8556,  => 0.6697

precs (weighed): 0.5129, 0.6012, 0.2357, 0.7321,  => 0.5205
recalls (weighed): 0.7162, 0.6216, 0.4855, 0.8556,  => 0.6697
f1s (weighed): 0.5977, 0.5961, 0.3173, 0.7890,  => 0.5751



Epoch 9/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 9/20, Loss: 1148.2865, 

accs: 0.4425, 0.4688, 0.3172, 0.5158,  => 0.4361

precs (macro): 0.1843, 0.2636, 0.1383, 0.2148,  => 0.2002
recalls (macro): 0.2639, 0.2203, 0.2563, 0.3393,  => 0.2700
f1s (macro): 0.1710, 0.2398, 0.1599, 0.1782,  => 0.1872

precs (micro): 0.4425, 0.4688, 0.3172, 0.5158,  => 0.4361
recalls (micro): 0.4425, 0.4688, 0.3172, 0.5158,  => 0.4361
f1s (micro): 0.4425, 0.4688, 0.3172, 0.5158,  => 0.4361

precs (weighed): 0.5231, 0.5538, 0.2482, 0.7287,  => 0.5134
recalls (weighed): 0.4425, 0.4688, 0.3172, 0.5158,  => 0.4361
f1s (weighed): 0.4769, 0.5074, 0.2732, 0.6038,  => 0.4653


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 143.1346, 

accs: 0.7162, 0.6332, 0.4855, 0.8556,  => 0.6726

precs (macro): 0.2387, 0.6112, 0.1931, 0.2852,  => 0.3321
recalls (macro): 0.3333, 0.5519, 0.2581, 0.3333,  => 0.3692
f1s (macro): 0.2782, 0.5201, 0.1794, 0.3074,  => 0.3213

precs (micro): 0.7162, 0.6332, 0.4855, 0.8556,  => 0.6726
recalls (micro): 0.7162, 0.6332, 0.4855, 0.8556,  => 0.6726
f1s (micro): 0.7162, 0.6332, 0.4855, 0.8556,  => 0.6726

precs (weighed): 0.5129, 0.6178, 0.2495, 0.7321,  => 0.5281
recalls (weighed): 0.7162, 0.6332, 0.4855, 0.8556,  => 0.6726
f1s (weighed): 0.5977, 0.5725, 0.3204, 0.7890,  => 0.5699

Saved the best model to path: ./models/task_2/lstm_bert-base-multilingual-cased_8.pth


Epoch 10/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 10/20, Loss: 1163.0241, 

accs: 0.4330, 0.4602, 0.3123, 0.4992,  => 0.4262

precs (macro): 0.1828, 0.2574, 0.1393, 0.2117,  => 0.1978
recalls (macro): 0.2550, 0.2161, 0.2711, 0.2719,  => 0.2535
f1s (macro): 0.1681, 0.2348, 0.1610, 0.1737,  => 0.1844

precs (micro): 0.4330, 0.4602, 0.3123, 0.4992,  => 0.4262
recalls (micro): 0.4330, 0.4602, 0.3123, 0.4992,  => 0.4262
f1s (micro): 0.4330, 0.4602, 0.3123, 0.4992,  => 0.4262

precs (weighed): 0.5192, 0.5425, 0.2469, 0.7184,  => 0.5067
recalls (weighed): 0.4330, 0.4602, 0.3123, 0.4992,  => 0.4262
f1s (weighed): 0.4697, 0.4977, 0.2693, 0.5889,  => 0.4564


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 143.1573, 

accs: 0.7162, 0.6357, 0.4846, 0.8556,  => 0.6730

precs (macro): 0.2387, 0.6196, 0.1538, 0.2852,  => 0.3243
recalls (macro): 0.3333, 0.5524, 0.2778, 0.3333,  => 0.3742
f1s (macro): 0.2782, 0.5177, 0.1963, 0.3074,  => 0.3249

precs (micro): 0.7162, 0.6357, 0.4846, 0.8556,  => 0.6730
recalls (micro): 0.7162, 0.6357, 0.4846, 0.8556,  => 0.6730
f1s (micro): 0.7162, 0.6357, 0.4846, 0.8556,  => 0.6730

precs (weighed): 0.5129, 0.6243, 0.2506, 0.7321,  => 0.5300
recalls (weighed): 0.7162, 0.6357, 0.4846, 0.8556,  => 0.6730
f1s (weighed): 0.5977, 0.5713, 0.3300, 0.7890,  => 0.5720



Epoch 11/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 11/20, Loss: 1151.2242, 

accs: 0.4388, 0.4713, 0.3712, 0.5003,  => 0.4454

precs (macro): 0.1828, 0.2662, 0.2333, 0.2123,  => 0.2237
recalls (macro): 0.2400, 0.3052, 0.2711, 0.1473,  => 0.2409
f1s (macro): 0.1689, 0.2421, 0.2334, 0.1739,  => 0.2046

precs (micro): 0.4388, 0.4713, 0.3712, 0.5003,  => 0.4454
recalls (micro): 0.4388, 0.4713, 0.3712, 0.5003,  => 0.4454
f1s (micro): 0.4388, 0.4713, 0.3712, 0.5003,  => 0.4454

precs (weighed): 0.5205, 0.5580, 0.3761, 0.7212,  => 0.5439
recalls (weighed): 0.4388, 0.4713, 0.3712, 0.5003,  => 0.4454
f1s (weighed): 0.4741, 0.5106, 0.3680, 0.5908,  => 0.4859


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 142.7479, 

accs: 0.7162, 0.6340, 0.5137, 0.8556,  => 0.6799

precs (macro): 0.2387, 0.6040, 0.2304, 0.2852,  => 0.3396
recalls (macro): 0.3333, 0.5703, 0.2746, 0.3333,  => 0.3779
f1s (macro): 0.2782, 0.5594, 0.2188, 0.3074,  => 0.3409

precs (micro): 0.7162, 0.6340, 0.5137, 0.8556,  => 0.6799
recalls (micro): 0.7162, 0.6340, 0.5137, 0.8556,  => 0.6799
f1s (micro): 0.7162, 0.6340, 0.5137, 0.8556,  => 0.6799

precs (weighed): 0.5129, 0.6153, 0.3860, 0.7321,  => 0.5615
recalls (weighed): 0.7162, 0.6340, 0.5137, 0.8556,  => 0.6799
f1s (weighed): 0.5977, 0.6002, 0.3951, 0.7890,  => 0.5955

Saved the best model to path: ./models/task_2/lstm_bert-base-multilingual-cased_10.pth


Epoch 12/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 12/20, Loss: 1144.8439, 

accs: 0.4374, 0.4726, 0.3838, 0.5088,  => 0.4506

precs (macro): 0.1829, 0.2665, 0.2324, 0.2111,  => 0.2232
recalls (macro): 0.2508, 0.3894, 0.2600, 0.3372,  => 0.3094
f1s (macro): 0.1690, 0.2431, 0.2378, 0.1756,  => 0.2064

precs (micro): 0.4374, 0.4726, 0.3838, 0.5088,  => 0.4506
recalls (micro): 0.4374, 0.4726, 0.3838, 0.5088,  => 0.4506
f1s (micro): 0.4374, 0.4726, 0.3838, 0.5088,  => 0.4506

precs (weighed): 0.5199, 0.5592, 0.3794, 0.7164,  => 0.5437
recalls (weighed): 0.4374, 0.4726, 0.3838, 0.5088,  => 0.4506
f1s (weighed): 0.4728, 0.5119, 0.3799, 0.5948,  => 0.4898


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 140.7961, 

accs: 0.7162, 0.6332, 0.5245, 0.8556,  => 0.6824

precs (macro): 0.2387, 0.6101, 0.2530, 0.2852,  => 0.3467
recalls (macro): 0.3333, 0.5531, 0.3102, 0.3333,  => 0.3825
f1s (macro): 0.2782, 0.5231, 0.2786, 0.3074,  => 0.3468

precs (micro): 0.7162, 0.6332, 0.5245, 0.8556,  => 0.6824
recalls (micro): 0.7162, 0.6332, 0.5245, 0.8556,  => 0.6824
f1s (micro): 0.7162, 0.6332, 0.5245, 0.8556,  => 0.6824

precs (weighed): 0.5129, 0.6171, 0.4298, 0.7321,  => 0.5730
recalls (weighed): 0.7162, 0.6332, 0.5245, 0.8556,  => 0.6824
f1s (weighed): 0.5977, 0.5746, 0.4724, 0.7890,  => 0.6084

Saved the best model to path: ./models/task_2/lstm_bert-base-multilingual-cased_11.pth


Epoch 13/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 13/20, Loss: 1142.0347, 

accs: 0.4363, 0.4751, 0.3918, 0.5126,  => 0.4540

precs (macro): 0.1849, 0.2668, 0.2379, 0.2112,  => 0.2252
recalls (macro): 0.2703, 0.2246, 0.2667, 0.2134,  => 0.2437
f1s (macro): 0.1701, 0.2437, 0.2440, 0.1761,  => 0.2085

precs (micro): 0.4363, 0.4751, 0.3918, 0.5126,  => 0.4540
recalls (micro): 0.4363, 0.4751, 0.3918, 0.5126,  => 0.4540
f1s (micro): 0.4363, 0.4751, 0.3918, 0.5126,  => 0.4540

precs (weighed): 0.5243, 0.5581, 0.3878, 0.7170,  => 0.5468
recalls (weighed): 0.4363, 0.4751, 0.3918, 0.5126,  => 0.4540
f1s (weighed): 0.4735, 0.5130, 0.3882, 0.5977,  => 0.4931


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 137.4750, 

accs: 0.7162, 0.6357, 0.5087, 0.8556,  => 0.6790

precs (macro): 0.2387, 0.6687, 0.2276, 0.2852,  => 0.3551
recalls (macro): 0.3333, 0.5378, 0.2709, 0.3333,  => 0.3688
f1s (macro): 0.2782, 0.4727, 0.2128, 0.3074,  => 0.3178

precs (micro): 0.7162, 0.6357, 0.5087, 0.8556,  => 0.6790
recalls (micro): 0.7162, 0.6357, 0.5087, 0.8556,  => 0.6790
f1s (micro): 0.7162, 0.6357, 0.5087, 0.8556,  => 0.6790

precs (weighed): 0.5129, 0.6603, 0.3812, 0.7321,  => 0.5716
recalls (weighed): 0.7162, 0.6357, 0.5087, 0.8556,  => 0.6790
f1s (weighed): 0.5977, 0.5386, 0.3863, 0.7890,  => 0.5779

Saved the best model to path: ./models/task_2/lstm_bert-base-multilingual-cased_12.pth


Epoch 14/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 14/20, Loss: 1134.0629, 

accs: 0.4442, 0.4787, 0.3909, 0.5187,  => 0.4581

precs (macro): 0.1824, 0.2707, 0.2404, 0.2785,  => 0.2430
recalls (macro): 0.2391, 0.3096, 0.2705, 0.2172,  => 0.2591
f1s (macro): 0.1698, 0.2467, 0.2454, 0.1821,  => 0.2110

precs (micro): 0.4442, 0.4787, 0.3909, 0.5187,  => 0.4581
recalls (micro): 0.4442, 0.4787, 0.3909, 0.5187,  => 0.4581
f1s (micro): 0.4442, 0.4787, 0.3909, 0.5187,  => 0.4581

precs (weighed): 0.5194, 0.5661, 0.3907, 0.7511,  => 0.5568
recalls (weighed): 0.4442, 0.4787, 0.3909, 0.5187,  => 0.4581
f1s (weighed): 0.4770, 0.5184, 0.3888, 0.6046,  => 0.4972


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 135.2770, 

accs: 0.7162, 0.6365, 0.5361, 0.8598,  => 0.6871

precs (macro): 0.2387, 0.6656, 0.2489, 0.4672,  => 0.4051
recalls (macro): 0.3333, 0.5397, 0.2945, 0.3497,  => 0.3793
f1s (macro): 0.2782, 0.4773, 0.2506, 0.3397,  => 0.3365

precs (micro): 0.7162, 0.6365, 0.5361, 0.8598,  => 0.6871
recalls (micro): 0.7162, 0.6365, 0.5361, 0.8598,  => 0.6871
f1s (micro): 0.7162, 0.6365, 0.5361, 0.8598,  => 0.6871

precs (weighed): 0.5129, 0.6581, 0.4148, 0.7998,  => 0.5964
recalls (weighed): 0.7162, 0.6365, 0.5361, 0.8598,  => 0.6871
f1s (weighed): 0.5977, 0.5422, 0.4397, 0.8027,  => 0.5956

Saved the best model to path: ./models/task_2/lstm_bert-base-multilingual-cased_13.pth


Epoch 15/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 15/20, Loss: 1143.5826, 

accs: 0.4407, 0.4682, 0.3890, 0.5099,  => 0.4519

precs (macro): 0.1838, 0.2650, 0.2384, 0.3185,  => 0.2514
recalls (macro): 0.2605, 0.2226, 0.2680, 0.2290,  => 0.2450
f1s (macro): 0.1703, 0.2419, 0.2439, 0.2065,  => 0.2157

precs (micro): 0.4407, 0.4682, 0.3890, 0.5099,  => 0.4519
recalls (micro): 0.4407, 0.4682, 0.3890, 0.5099,  => 0.4519
f1s (micro): 0.4407, 0.4682, 0.3890, 0.5099,  => 0.4519

precs (weighed): 0.5217, 0.5555, 0.3887, 0.7695,  => 0.5589
recalls (weighed): 0.4407, 0.4682, 0.3890, 0.5099,  => 0.4519
f1s (weighed): 0.4753, 0.5081, 0.3869, 0.6060,  => 0.4941


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 137.7372, 

accs: 0.7162, 0.6407, 0.5568, 0.8647,  => 0.6946

precs (macro): 0.2387, 0.6232, 0.2622, 0.4982,  => 0.4056
recalls (macro): 0.3333, 0.5635, 0.3183, 0.3685,  => 0.3959
f1s (macro): 0.2782, 0.5389, 0.2833, 0.3716,  => 0.3680

precs (micro): 0.7162, 0.6407, 0.5568, 0.8647,  => 0.6946
recalls (micro): 0.7162, 0.6407, 0.5568, 0.8647,  => 0.6946
f1s (micro): 0.7162, 0.6407, 0.5568, 0.8647,  => 0.6946

precs (weighed): 0.5129, 0.6287, 0.4393, 0.8151,  => 0.5990
recalls (weighed): 0.7162, 0.6407, 0.5568, 0.8647,  => 0.6946
f1s (weighed): 0.5977, 0.5876, 0.4848, 0.8157,  => 0.6215



Epoch 16/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 16/20, Loss: 1131.9243, 

accs: 0.4428, 0.4740, 0.4039, 0.5207,  => 0.4604

precs (macro): 0.1847, 0.2682, 0.2470, 0.3017,  => 0.2504
recalls (macro): 0.2641, 0.2241, 0.2825, 0.4214,  => 0.2980
f1s (macro): 0.1712, 0.2441, 0.2539, 0.2108,  => 0.2200

precs (micro): 0.4428, 0.4740, 0.4039, 0.5207,  => 0.4604
recalls (micro): 0.4428, 0.4740, 0.4039, 0.5207,  => 0.4604
f1s (micro): 0.4428, 0.4740, 0.4039, 0.5207,  => 0.4604

precs (weighed): 0.5241, 0.5621, 0.4003, 0.7625,  => 0.5623
recalls (weighed): 0.4428, 0.4740, 0.4039, 0.5207,  => 0.4604
f1s (weighed): 0.4775, 0.5141, 0.4000, 0.6137,  => 0.5013


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 135.6626, 

accs: 0.7162, 0.6382, 0.5062, 0.8647,  => 0.6813

precs (macro): 0.2387, 0.6118, 0.2246, 0.4870,  => 0.3905
recalls (macro): 0.3333, 0.5705, 0.2690, 0.3791,  => 0.3880
f1s (macro): 0.2782, 0.5565, 0.2097, 0.3874,  => 0.3580

precs (micro): 0.7162, 0.6382, 0.5062, 0.8647,  => 0.6813
recalls (micro): 0.7162, 0.6382, 0.5062, 0.8647,  => 0.6813
f1s (micro): 0.7162, 0.6382, 0.5062, 0.8647,  => 0.6813

precs (weighed): 0.5129, 0.6212, 0.3769, 0.8136,  => 0.5811
recalls (weighed): 0.7162, 0.6382, 0.5062, 0.8647,  => 0.6813
f1s (weighed): 0.5977, 0.5993, 0.3820, 0.8208,  => 0.6000



Epoch 17/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 17/20, Loss: 1137.4848, 

accs: 0.4321, 0.4717, 0.4004, 0.5221,  => 0.4566

precs (macro): 0.1833, 0.2639, 0.2447, 0.2813,  => 0.2433
recalls (macro): 0.2632, 0.3893, 0.2822, 0.1699,  => 0.2762
f1s (macro): 0.1684, 0.2421, 0.2522, 0.2060,  => 0.2172

precs (micro): 0.4321, 0.4717, 0.4004, 0.5221,  => 0.4566
recalls (micro): 0.4321, 0.4717, 0.4004, 0.5221,  => 0.4566
f1s (micro): 0.4321, 0.4717, 0.4004, 0.5221,  => 0.4566

precs (weighed): 0.5202, 0.5536, 0.3948, 0.7511,  => 0.5549
recalls (weighed): 0.4321, 0.4717, 0.4004, 0.5221,  => 0.4566
f1s (weighed): 0.4695, 0.5091, 0.3954, 0.6128,  => 0.4967


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 136.7594, 

accs: 0.7162, 0.6390, 0.5394, 0.8598,  => 0.6886

precs (macro): 0.2387, 0.6290, 0.2576, 0.4672,  => 0.3981
recalls (macro): 0.3333, 0.5551, 0.3162, 0.3497,  => 0.3886
f1s (macro): 0.2782, 0.5200, 0.2838, 0.3397,  => 0.3554

precs (micro): 0.7162, 0.6390, 0.5394, 0.8598,  => 0.6886
recalls (micro): 0.7162, 0.6390, 0.5394, 0.8598,  => 0.6886
f1s (micro): 0.7162, 0.6390, 0.5394, 0.8598,  => 0.6886

precs (weighed): 0.5129, 0.6319, 0.4357, 0.7998,  => 0.5951
recalls (weighed): 0.7162, 0.6390, 0.5394, 0.8598,  => 0.6886
f1s (weighed): 0.5977, 0.5737, 0.4818, 0.8027,  => 0.6140



Epoch 18/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 18/20, Loss: 1141.5327, 

accs: 0.4252, 0.4622, 0.3955, 0.5187,  => 0.4504

precs (macro): 0.1824, 0.2593, 0.2422, 0.3011,  => 0.2462
recalls (macro): 0.2551, 0.2172, 0.2733, 0.2927,  => 0.2596
f1s (macro): 0.1664, 0.2363, 0.2479, 0.2060,  => 0.2141

precs (micro): 0.4252, 0.4622, 0.3955, 0.5187,  => 0.4504
recalls (micro): 0.4252, 0.4622, 0.3955, 0.5187,  => 0.4504
f1s (micro): 0.4252, 0.4622, 0.3955, 0.5187,  => 0.4504

precs (weighed): 0.5182, 0.5457, 0.3941, 0.7669,  => 0.5562
recalls (weighed): 0.4252, 0.4622, 0.3955, 0.5187,  => 0.4504
f1s (weighed): 0.4645, 0.5003, 0.3928, 0.6132,  => 0.4927


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 139.7426, 

accs: 0.7162, 0.6432, 0.5693, 0.8672,  => 0.6990

precs (macro): 0.2387, 0.6405, 0.2690, 0.4844,  => 0.4081
recalls (macro): 0.3333, 0.5589, 0.3249, 0.4054,  => 0.4056
f1s (macro): 0.2782, 0.5239, 0.2890, 0.4203,  => 0.3779

precs (micro): 0.7162, 0.6432, 0.5693, 0.8672,  => 0.6990
recalls (micro): 0.7162, 0.6432, 0.5693, 0.8672,  => 0.6990
f1s (micro): 0.7162, 0.6432, 0.5693, 0.8672,  => 0.6990

precs (weighed): 0.5129, 0.6413, 0.4495, 0.8193,  => 0.6057
recalls (weighed): 0.7162, 0.6432, 0.5693, 0.8672,  => 0.6990
f1s (weighed): 0.5977, 0.5775, 0.4944, 0.8329,  => 0.6256



Epoch 19/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 19/20, Loss: 1139.2507, 

accs: 0.4345, 0.4766, 0.3878, 0.5206,  => 0.4549

precs (macro): 0.1815, 0.2663, 0.2408, 0.2849,  => 0.2434
recalls (macro): 0.2272, 0.2236, 0.2651, 0.3702,  => 0.2715
f1s (macro): 0.1671, 0.2428, 0.2433, 0.2217,  => 0.2187

precs (micro): 0.4345, 0.4766, 0.3878, 0.5206,  => 0.4549
recalls (micro): 0.4345, 0.4766, 0.3878, 0.5206,  => 0.4549
f1s (micro): 0.4345, 0.4766, 0.3878, 0.5206,  => 0.4549

precs (weighed): 0.5175, 0.5588, 0.3942, 0.7540,  => 0.5561
recalls (weighed): 0.4345, 0.4766, 0.3878, 0.5206,  => 0.4549
f1s (weighed): 0.4706, 0.5139, 0.3890, 0.6149,  => 0.4971


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 138.8262, 

accs: 0.7162, 0.6382, 0.5618, 0.8680,  => 0.6961

precs (macro): 0.2387, 0.6788, 0.2641, 0.4932,  => 0.4187
recalls (macro): 0.3333, 0.5407, 0.3201, 0.4015,  => 0.3989
f1s (macro): 0.2782, 0.4770, 0.2842, 0.4168,  => 0.3640

precs (micro): 0.7162, 0.6382, 0.5618, 0.8680,  => 0.6961
recalls (micro): 0.7162, 0.6382, 0.5618, 0.8680,  => 0.6961
f1s (micro): 0.7162, 0.6382, 0.5618, 0.8680,  => 0.6961

precs (weighed): 0.5129, 0.6685, 0.4423, 0.8208,  => 0.6111
recalls (weighed): 0.7162, 0.6382, 0.5618, 0.8680,  => 0.6961
f1s (weighed): 0.5977, 0.5423, 0.4872, 0.8318,  => 0.6148

Early stopping triggered
lstm + distilbert-base-multilingual-cased

[19:01:48] task: task-2                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: distilbert-base-multilingual-cased                                              my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 20                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/lstm_distilbert-base-multilingual-cased                        my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

Training ...

Epoch 1/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 1/20, Loss: 1169.4902, 

accs: 0.4986, 0.4581, 0.3770, 0.5056,  => 0.4598

precs (macro): 0.2504, 0.2553, 0.3070, 0.2119,  => 0.2561
recalls (macro): 0.2451, 0.2130, 0.2511, 0.1489,  => 0.2145
f1s (macro): 0.2320, 0.2318, 0.2238, 0.1749,  => 0.2156

precs (micro): 0.4986, 0.4581, 0.3770, 0.5056,  => 0.4598
recalls (micro): 0.4986, 0.4581, 0.3770, 0.5056,  => 0.4598
f1s (micro): 0.4986, 0.4581, 0.3770, 0.5056,  => 0.4598

precs (weighed): 0.5916, 0.5382, 0.3957, 0.7196,  => 0.5613
recalls (weighed): 0.4986, 0.4581, 0.3770, 0.5056,  => 0.4598
f1s (weighed): 0.5404, 0.4941, 0.3572, 0.5939,  => 0.4964


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 143.8715, 

accs: 0.7162, 0.6340, 0.4855, 0.8556,  => 0.6728

precs (macro): 0.2387, 0.6583, 0.1214, 0.2852,  => 0.3259
recalls (macro): 0.3333, 0.5365, 0.2500, 0.3333,  => 0.3633
f1s (macro): 0.2782, 0.4717, 0.1634, 0.3074,  => 0.3052

precs (micro): 0.7162, 0.6340, 0.4855, 0.8556,  => 0.6728
recalls (micro): 0.7162, 0.6340, 0.4855, 0.8556,  => 0.6728
f1s (micro): 0.7162, 0.6340, 0.4855, 0.8556,  => 0.6728

precs (weighed): 0.5129, 0.6521, 0.2357, 0.7321,  => 0.5332
recalls (weighed): 0.7162, 0.6340, 0.4855, 0.8556,  => 0.6728
f1s (weighed): 0.5977, 0.5375, 0.3173, 0.7890,  => 0.5604

Saved the best model to path: ./models/task_2/lstm_distilbert-base-multilingual-cased_0.pth


Epoch 2/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 2/20, Loss: 1146.4648, 

accs: 0.5031, 0.4704, 0.3899, 0.5033,  => 0.4667

precs (macro): 0.2506, 0.2643, 0.2304, 0.2110,  => 0.2391
recalls (macro): 0.2378, 0.2235, 0.2673, 0.2731,  => 0.2504
f1s (macro): 0.2321, 0.2422, 0.2390, 0.1743,  => 0.2219

precs (micro): 0.5031, 0.4704, 0.3899, 0.5033,  => 0.4667
recalls (micro): 0.5031, 0.4704, 0.3899, 0.5033,  => 0.4667
f1s (micro): 0.5031, 0.4704, 0.3899, 0.5033,  => 0.4667

precs (weighed): 0.5929, 0.5554, 0.3704, 0.7161,  => 0.5587
recalls (weighed): 0.5031, 0.4704, 0.3899, 0.5033,  => 0.4667
f1s (weighed): 0.5438, 0.5094, 0.3772, 0.5910,  => 0.5053


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 136.8360, 

accs: 0.7162, 0.6456, 0.5834, 0.8556,  => 0.7002

precs (macro): 0.2387, 0.6411, 0.2831, 0.2852,  => 0.3620
recalls (macro): 0.3333, 0.5641, 0.3470, 0.3333,  => 0.3944
f1s (macro): 0.2782, 0.5339, 0.3117, 0.3074,  => 0.3578

precs (micro): 0.7162, 0.6456, 0.5834, 0.8556,  => 0.7002
recalls (micro): 0.7162, 0.6456, 0.5834, 0.8556,  => 0.7002
f1s (micro): 0.7162, 0.6456, 0.5834, 0.8556,  => 0.7002

precs (weighed): 0.5129, 0.6425, 0.4740, 0.7321,  => 0.5904
recalls (weighed): 0.7162, 0.6456, 0.5834, 0.8556,  => 0.7002
f1s (weighed): 0.5977, 0.5852, 0.5230, 0.7890,  => 0.6237

Saved the best model to path: ./models/task_2/lstm_distilbert-base-multilingual-cased_1.pth


Epoch 3/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 3/20, Loss: 1120.7708, 

accs: 0.4971, 0.4759, 0.4110, 0.5052,  => 0.4723

precs (macro): 0.2487, 0.2692, 0.2679, 0.2129,  => 0.2497
recalls (macro): 0.2628, 0.2259, 0.3022, 0.2737,  => 0.2661
f1s (macro): 0.2317, 0.2456, 0.2681, 0.1753,  => 0.2302

precs (micro): 0.4971, 0.4759, 0.4110, 0.5052,  => 0.4723
recalls (micro): 0.4971, 0.4759, 0.4110, 0.5052,  => 0.4723
f1s (micro): 0.4971, 0.4759, 0.4110, 0.5052,  => 0.4723

precs (weighed): 0.5890, 0.5645, 0.4347, 0.7227,  => 0.5777
recalls (weighed): 0.4971, 0.4759, 0.4110, 0.5052,  => 0.4723
f1s (weighed): 0.5380, 0.5164, 0.4182, 0.5945,  => 0.5168


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 133.3144, 

accs: 0.7162, 0.6423, 0.6191, 0.8556,  => 0.7083

precs (macro): 0.2387, 0.6807, 0.3009, 0.2852,  => 0.3764
recalls (macro): 0.3333, 0.5472, 0.3686, 0.3333,  => 0.3956
f1s (macro): 0.2782, 0.4904, 0.3312, 0.3074,  => 0.3518

precs (micro): 0.7162, 0.6423, 0.6191, 0.8556,  => 0.7083
recalls (micro): 0.7162, 0.6423, 0.6191, 0.8556,  => 0.7083
f1s (micro): 0.7162, 0.6423, 0.6191, 0.8556,  => 0.7083

precs (weighed): 0.5129, 0.6708, 0.5028, 0.7321,  => 0.6046
recalls (weighed): 0.7162, 0.6423, 0.6191, 0.8556,  => 0.7083
f1s (weighed): 0.5977, 0.5529, 0.5548, 0.7890,  => 0.6236

Saved the best model to path: ./models/task_2/lstm_distilbert-base-multilingual-cased_2.pth


Epoch 4/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 4/20, Loss: 1105.3001, 

accs: 0.4971, 0.4769, 0.4125, 0.5110,  => 0.4744

precs (macro): 0.2490, 0.2697, 0.2801, 0.2988,  => 0.2744
recalls (macro): 0.2548, 0.3096, 0.3088, 0.2137,  => 0.2717
f1s (macro): 0.2314, 0.2464, 0.2740, 0.1788,  => 0.2326

precs (micro): 0.4971, 0.4769, 0.4125, 0.5110,  => 0.4744
recalls (micro): 0.4971, 0.4769, 0.4125, 0.5110,  => 0.4744
f1s (micro): 0.4971, 0.4769, 0.4125, 0.5110,  => 0.4744

precs (weighed): 0.5899, 0.5659, 0.4540, 0.7436,  => 0.5884
recalls (weighed): 0.4971, 0.4769, 0.4125, 0.5110,  => 0.4744
f1s (weighed): 0.5385, 0.5175, 0.4271, 0.6017,  => 0.5212


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 132.0027, 

accs: 0.7162, 0.6556, 0.6257, 0.8548,  => 0.7131

precs (macro): 0.2387, 0.6337, 0.3209, 0.2175,  => 0.3527
recalls (macro): 0.3333, 0.6005, 0.3779, 0.2498,  => 0.3904
f1s (macro): 0.2782, 0.5974, 0.3465, 0.2325,  => 0.3637

precs (micro): 0.7162, 0.6556, 0.6257, 0.8548,  => 0.7131
recalls (micro): 0.7162, 0.6556, 0.6257, 0.8548,  => 0.7131
f1s (micro): 0.7162, 0.6556, 0.6257, 0.8548,  => 0.7131

precs (weighed): 0.5129, 0.6429, 0.5253, 0.7443,  => 0.6064
recalls (weighed): 0.7162, 0.6556, 0.6257, 0.8548,  => 0.7131
f1s (weighed): 0.5977, 0.6319, 0.5710, 0.7957,  => 0.6491

Saved the best model to path: ./models/task_2/lstm_distilbert-base-multilingual-cased_3.pth


Epoch 5/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 5/20, Loss: 1090.9848, 

accs: 0.4935, 0.4781, 0.4186, 0.5042,  => 0.4736

precs (macro): 0.2549, 0.2692, 0.2951, 0.2656,  => 0.2712
recalls (macro): 0.2588, 0.3102, 0.3074, 0.2742,  => 0.2876
f1s (macro): 0.2358, 0.2465, 0.2802, 0.1776,  => 0.2350

precs (micro): 0.4935, 0.4781, 0.4186, 0.5042,  => 0.4736
recalls (micro): 0.4935, 0.4781, 0.4186, 0.5042,  => 0.4736
f1s (micro): 0.4935, 0.4781, 0.4186, 0.5042,  => 0.4736

precs (weighed): 0.5991, 0.5648, 0.4822, 0.7390,  => 0.5963
recalls (weighed): 0.4935, 0.4781, 0.4186, 0.5042,  => 0.4736
f1s (weighed): 0.5398, 0.5178, 0.4432, 0.5970,  => 0.5244


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 126.2796, 

accs: 0.7253, 0.6672, 0.6232, 0.8556,  => 0.7178

precs (macro): 0.5158, 0.6458, 0.3462, 0.2852,  => 0.4482
recalls (macro): 0.3463, 0.6257, 0.3893, 0.3333,  => 0.4237
f1s (macro): 0.3064, 0.6278, 0.3638, 0.3074,  => 0.4013

precs (micro): 0.7253, 0.6672, 0.6232, 0.8556,  => 0.7178
recalls (micro): 0.7253, 0.6672, 0.6232, 0.8556,  => 0.7178
f1s (micro): 0.7253, 0.6672, 0.6232, 0.8556,  => 0.7178

precs (weighed): 0.7447, 0.6571, 0.5540, 0.7321,  => 0.6720
recalls (weighed): 0.7253, 0.6672, 0.6232, 0.8556,  => 0.7178
f1s (weighed): 0.6227, 0.6551, 0.5842, 0.7890,  => 0.6628

Saved the best model to path: ./models/task_2/lstm_distilbert-base-multilingual-cased_4.pth


Epoch 6/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 6/20, Loss: 1078.5947, 

accs: 0.4984, 0.4835, 0.4300, 0.5071,  => 0.4797

precs (macro): 0.2594, 0.2764, 0.3024, 0.2165,  => 0.2637
recalls (macro): 0.2443, 0.2318, 0.3234, 0.2118,  => 0.2528
f1s (macro): 0.2390, 0.2521, 0.2888, 0.1768,  => 0.2392

precs (micro): 0.4984, 0.4835, 0.4300, 0.5071,  => 0.4797
recalls (micro): 0.4984, 0.4835, 0.4300, 0.5071,  => 0.4797
f1s (micro): 0.4984, 0.4835, 0.4300, 0.5071,  => 0.4797

precs (weighed): 0.6069, 0.5784, 0.4900, 0.7352,  => 0.6026
recalls (weighed): 0.4984, 0.4835, 0.4300, 0.5071,  => 0.4797
f1s (weighed): 0.5461, 0.5267, 0.4526, 0.6001,  => 0.5314


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 124.9732, 

accs: 0.7320, 0.6656, 0.6315, 0.8556,  => 0.7212

precs (macro): 0.5270, 0.6760, 0.3387, 0.2150,  => 0.4392
recalls (macro): 0.3550, 0.5890, 0.3798, 0.2500,  => 0.3934
f1s (macro): 0.3234, 0.5692, 0.3550, 0.2312,  => 0.3697

precs (micro): 0.7320, 0.6656, 0.6315, 0.8556,  => 0.7212
recalls (micro): 0.7320, 0.6656, 0.6315, 0.8556,  => 0.7212
f1s (micro): 0.7320, 0.6656, 0.6315, 0.8556,  => 0.7212

precs (weighed): 0.7562, 0.6727, 0.5460, 0.7357,  => 0.6777
recalls (weighed): 0.7320, 0.6656, 0.6315, 0.8556,  => 0.7212
f1s (weighed): 0.6381, 0.6150, 0.5809, 0.7911,  => 0.6563

Saved the best model to path: ./models/task_2/lstm_distilbert-base-multilingual-cased_5.pth


Epoch 7/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 7/20, Loss: 1062.4517, 

accs: 0.5156, 0.4943, 0.4238, 0.5129,  => 0.4866

precs (macro): 0.2695, 0.2850, 0.3058, 0.2189,  => 0.2698
recalls (macro): 0.2792, 0.3217, 0.3230, 0.1510,  => 0.2687
f1s (macro): 0.2509, 0.2598, 0.2883, 0.1787,  => 0.2444

precs (micro): 0.5156, 0.4943, 0.4238, 0.5129,  => 0.4866
recalls (micro): 0.5156, 0.4943, 0.4238, 0.5129,  => 0.4866
f1s (micro): 0.5156, 0.4943, 0.4238, 0.5129,  => 0.4866

precs (weighed): 0.6227, 0.5952, 0.4966, 0.7434,  => 0.6145
recalls (weighed): 0.5156, 0.4943, 0.4238, 0.5129,  => 0.4866
f1s (weighed): 0.5623, 0.5398, 0.4510, 0.6070,  => 0.5400


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 125.6108, 

accs: 0.7286, 0.6896, 0.5975, 0.8539,  => 0.7174

precs (macro): 0.4400, 0.6732, 0.3289, 0.2201,  => 0.4155
recalls (macro): 0.3708, 0.6491, 0.3815, 0.2495,  => 0.4127
f1s (macro): 0.3596, 0.6527, 0.3446, 0.2339,  => 0.3977

precs (micro): 0.7286, 0.6896, 0.5975, 0.8539,  => 0.7174
recalls (micro): 0.7286, 0.6896, 0.5975, 0.8539,  => 0.7174
f1s (micro): 0.7286, 0.6896, 0.5975, 0.8539,  => 0.7174

precs (weighed): 0.6894, 0.6818, 0.5414, 0.7531,  => 0.6665
recalls (weighed): 0.7286, 0.6896, 0.5975, 0.8539,  => 0.7174
f1s (weighed): 0.6651, 0.6782, 0.5550, 0.8004,  => 0.6747



Epoch 8/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 8/20, Loss: 1066.8853, 

accs: 0.4965, 0.4907, 0.4255, 0.5103,  => 0.4807

precs (macro): 0.2674, 0.2858, 0.3094, 0.2180,  => 0.2701
recalls (macro): 0.2657, 0.2374, 0.3171, 0.2127,  => 0.2582
f1s (macro): 0.2444, 0.2592, 0.2890, 0.1780,  => 0.2427

precs (micro): 0.4965, 0.4907, 0.4255, 0.5103,  => 0.4807
recalls (micro): 0.4965, 0.4907, 0.4255, 0.5103,  => 0.4807
f1s (micro): 0.4965, 0.4907, 0.4255, 0.5103,  => 0.4807

precs (weighed): 0.6192, 0.5965, 0.5037, 0.7401,  => 0.6149
recalls (weighed): 0.4965, 0.4907, 0.4255, 0.5103,  => 0.4807
f1s (weighed): 0.5493, 0.5383, 0.4557, 0.6040,  => 0.5368


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 124.4154, 

accs: 0.7411, 0.7037, 0.6033, 0.8556,  => 0.7259

precs (macro): 0.4517, 0.7003, 0.3255, 0.2164,  => 0.4235
recalls (macro): 0.4033, 0.6520, 0.3809, 0.2500,  => 0.4215
f1s (macro): 0.4069, 0.6547, 0.3423, 0.2320,  => 0.4090

precs (micro): 0.7411, 0.7037, 0.6033, 0.8556,  => 0.7259
recalls (micro): 0.7411, 0.7037, 0.6033, 0.8556,  => 0.7259
f1s (micro): 0.7411, 0.7037, 0.6033, 0.8556,  => 0.7259

precs (weighed): 0.7095, 0.7017, 0.5415, 0.7407,  => 0.6733
recalls (weighed): 0.7411, 0.7037, 0.6033, 0.8556,  => 0.7259
f1s (weighed): 0.7049, 0.6840, 0.5579, 0.7940,  => 0.6852

Saved the best model to path: ./models/task_2/lstm_distilbert-base-multilingual-cased_7.pth


Epoch 9/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 9/20, Loss: 1056.6375, 

accs: 0.5062, 0.4909, 0.4326, 0.5091,  => 0.4847

precs (macro): 0.2700, 0.2845, 0.3116, 0.2186,  => 0.2712
recalls (macro): 0.2620, 0.2374, 0.3235, 0.2748,  => 0.2744
f1s (macro): 0.2479, 0.2587, 0.2921, 0.1780,  => 0.2442

precs (micro): 0.5062, 0.4909, 0.4326, 0.5091,  => 0.4847
recalls (micro): 0.5062, 0.4909, 0.4326, 0.5091,  => 0.4847
f1s (micro): 0.5062, 0.4909, 0.4326, 0.5091,  => 0.4847

precs (weighed): 0.6242, 0.5940, 0.5071, 0.7421,  => 0.6168
recalls (weighed): 0.5062, 0.4909, 0.4326, 0.5091,  => 0.4847
f1s (weighed): 0.5574, 0.5374, 0.4612, 0.6037,  => 0.5399


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 123.4821, 

accs: 0.7452, 0.6722, 0.6058, 0.8556,  => 0.7197

precs (macro): 0.4684, 0.6532, 0.3349, 0.2164,  => 0.4182
recalls (macro): 0.3940, 0.6502, 0.3830, 0.2500,  => 0.4193
f1s (macro): 0.3940, 0.6515, 0.3522, 0.2320,  => 0.4074

precs (micro): 0.7452, 0.6722, 0.6058, 0.8556,  => 0.7197
recalls (micro): 0.7452, 0.6722, 0.6058, 0.8556,  => 0.7197
f1s (micro): 0.7452, 0.6722, 0.6058, 0.8556,  => 0.7197

precs (weighed): 0.7195, 0.6694, 0.5573, 0.7407,  => 0.6717
recalls (weighed): 0.7452, 0.6722, 0.6058, 0.8556,  => 0.7197
f1s (weighed): 0.6967, 0.6706, 0.5729, 0.7940,  => 0.6835

Saved the best model to path: ./models/task_2/lstm_distilbert-base-multilingual-cased_8.pth


Epoch 10/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 10/20, Loss: 1056.3328, 

accs: 0.5037, 0.4983, 0.4259, 0.5037,  => 0.4829

precs (macro): 0.2660, 0.2917, 0.3172, 0.2198,  => 0.2737
recalls (macro): 0.2543, 0.2413, 0.3152, 0.2732,  => 0.2710
f1s (macro): 0.2447, 0.2640, 0.2915, 0.1773,  => 0.2444

precs (micro): 0.5037, 0.4983, 0.4259, 0.5037,  => 0.4829
recalls (micro): 0.5037, 0.4983, 0.4259, 0.5037,  => 0.4829
f1s (micro): 0.5037, 0.4983, 0.4259, 0.5037,  => 0.4829

precs (weighed): 0.6178, 0.6069, 0.5188, 0.7462,  => 0.6224
recalls (weighed): 0.5037, 0.4983, 0.4259, 0.5037,  => 0.4829
f1s (weighed): 0.5535, 0.5471, 0.4619, 0.6013,  => 0.5409


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 122.2357, 

accs: 0.7270, 0.6971, 0.6465, 0.8548,  => 0.7313

precs (macro): 0.4393, 0.6801, 0.3521, 0.2193,  => 0.4227
recalls (macro): 0.3675, 0.6772, 0.3936, 0.2498,  => 0.4220
f1s (macro): 0.3542, 0.6785, 0.3705, 0.2336,  => 0.4092

precs (micro): 0.7270, 0.6971, 0.6465, 0.8548,  => 0.7313
recalls (micro): 0.7270, 0.6971, 0.6465, 0.8548,  => 0.7313
f1s (micro): 0.7270, 0.6971, 0.6465, 0.8548,  => 0.7313

precs (weighed): 0.6876, 0.6950, 0.5732, 0.7507,  => 0.6766
recalls (weighed): 0.7270, 0.6971, 0.6465, 0.8548,  => 0.7313
f1s (weighed): 0.6603, 0.6959, 0.6064, 0.7993,  => 0.6905

Saved the best model to path: ./models/task_2/lstm_distilbert-base-multilingual-cased_9.pth


Epoch 11/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 11/20, Loss: 1053.9799, 

accs: 0.5087, 0.4972, 0.4236, 0.5036,  => 0.4833

precs (macro): 0.2771, 0.2920, 0.3165, 0.2203,  => 0.2765
recalls (macro): 0.2782, 0.3235, 0.3234, 0.2107,  => 0.2840
f1s (macro): 0.2517, 0.2638, 0.2908, 0.1773,  => 0.2459

precs (micro): 0.5087, 0.4972, 0.4236, 0.5036,  => 0.4833
recalls (micro): 0.5087, 0.4972, 0.4236, 0.5036,  => 0.4833
f1s (micro): 0.5087, 0.4972, 0.4236, 0.5036,  => 0.4833

precs (weighed): 0.6324, 0.6077, 0.5140, 0.7479,  => 0.6255
recalls (weighed): 0.5087, 0.4972, 0.4236, 0.5036,  => 0.4833
f1s (weighed): 0.5624, 0.5468, 0.4579, 0.6018,  => 0.5422


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 123.6409, 

accs: 0.7436, 0.6556, 0.6033, 0.8548,  => 0.7143

precs (macro): 0.4577, 0.6362, 0.3511, 0.2201,  => 0.4163
recalls (macro): 0.4026, 0.6347, 0.3865, 0.2498,  => 0.4184
f1s (macro): 0.4060, 0.6353, 0.3631, 0.2340,  => 0.4096

precs (micro): 0.7436, 0.6556, 0.6033, 0.8548,  => 0.7143
recalls (micro): 0.7436, 0.6556, 0.6033, 0.8548,  => 0.7143
f1s (micro): 0.7436, 0.6556, 0.6033, 0.8548,  => 0.7143

precs (weighed): 0.7137, 0.6539, 0.5805, 0.7532,  => 0.6753
recalls (weighed): 0.7436, 0.6556, 0.6033, 0.8548,  => 0.7143
f1s (weighed): 0.7050, 0.6547, 0.5873, 0.8008,  => 0.6869



Epoch 12/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 12/20, Loss: 1057.6393, 

accs: 0.5049, 0.4856, 0.4202, 0.5124,  => 0.4808

precs (macro): 0.2734, 0.2824, 0.3158, 0.2207,  => 0.2731
recalls (macro): 0.2754, 0.3172, 0.3178, 0.2758,  => 0.2966
f1s (macro): 0.2499, 0.2561, 0.2895, 0.1794,  => 0.2437

precs (micro): 0.5049, 0.4856, 0.4202, 0.5124,  => 0.4808
recalls (micro): 0.5049, 0.4856, 0.4202, 0.5124,  => 0.4808
f1s (micro): 0.5049, 0.4856, 0.4202, 0.5124,  => 0.4808

precs (weighed): 0.6266, 0.5893, 0.5139, 0.7491,  => 0.6197
recalls (weighed): 0.5049, 0.4856, 0.4202, 0.5124,  => 0.4808
f1s (weighed): 0.5576, 0.5323, 0.4560, 0.6084,  => 0.5386


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 124.8487, 

accs: 0.7402, 0.6822, 0.6332, 0.8548,  => 0.7276

precs (macro): 0.4504, 0.6900, 0.3584, 0.2208,  => 0.4299
recalls (macro): 0.4060, 0.6143, 0.3797, 0.2498,  => 0.4124
f1s (macro): 0.4104, 0.6059, 0.3626, 0.2344,  => 0.4033

precs (micro): 0.7402, 0.6822, 0.6332, 0.8548,  => 0.7276
recalls (micro): 0.7402, 0.6822, 0.6332, 0.8548,  => 0.7276
f1s (micro): 0.7402, 0.6822, 0.6332, 0.8548,  => 0.7276

precs (weighed): 0.7091, 0.6873, 0.5781, 0.7558,  => 0.6826
recalls (weighed): 0.7402, 0.6822, 0.6332, 0.8548,  => 0.7276
f1s (weighed): 0.7069, 0.6449, 0.5962, 0.8022,  => 0.6876



Epoch 13/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 13/20, Loss: 1048.3189, 

accs: 0.5062, 0.5062, 0.4246, 0.5071,  => 0.4860

precs (macro): 0.2708, 0.2983, 0.3180, 0.2207,  => 0.2770
recalls (macro): 0.2635, 0.2458, 0.3264, 0.2742,  => 0.2775
f1s (macro): 0.2463, 0.2694, 0.2926, 0.1783,  => 0.2467

precs (micro): 0.5062, 0.5062, 0.4246, 0.5071,  => 0.4860
recalls (micro): 0.5062, 0.5062, 0.4246, 0.5071,  => 0.4860
f1s (micro): 0.5062, 0.5062, 0.4246, 0.5071,  => 0.4860

precs (weighed): 0.6247, 0.6200, 0.5175, 0.7492,  => 0.6278
recalls (weighed): 0.5062, 0.5062, 0.4246, 0.5071,  => 0.4860
f1s (weighed): 0.5581, 0.5572, 0.4595, 0.6046,  => 0.5449


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 122.3287, 

accs: 0.7510, 0.7120, 0.6423, 0.8515,  => 0.7392

precs (macro): 0.4817, 0.7032, 0.3384, 0.2244,  => 0.4369
recalls (macro): 0.3974, 0.6690, 0.3937, 0.2488,  => 0.4272
f1s (macro): 0.3981, 0.6739, 0.3634, 0.2360,  => 0.4178

precs (micro): 0.7510, 0.7120, 0.6423, 0.8515,  => 0.7392
recalls (micro): 0.7510, 0.7120, 0.6423, 0.8515,  => 0.7392
f1s (micro): 0.7510, 0.7120, 0.6423, 0.8515,  => 0.7392

precs (weighed): 0.7314, 0.7075, 0.5602, 0.7680,  => 0.6918
recalls (weighed): 0.7510, 0.7120, 0.6423, 0.8515,  => 0.7392
f1s (weighed): 0.7018, 0.6990, 0.5976, 0.8076,  => 0.7015



Epoch 14/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 14/20, Loss: 1046.0446, 

accs: 0.5157, 0.4978, 0.4267, 0.5015,  => 0.4854

precs (macro): 0.2765, 0.2928, 0.3246, 0.2212,  => 0.2788
recalls (macro): 0.2881, 0.3236, 0.3348, 0.2726,  => 0.3048
f1s (macro): 0.2545, 0.2642, 0.2968, 0.1773,  => 0.2482

precs (micro): 0.5157, 0.4978, 0.4267, 0.5015,  => 0.4854
recalls (micro): 0.5157, 0.4978, 0.4267, 0.5015,  => 0.4854
f1s (micro): 0.5157, 0.4978, 0.4267, 0.5015,  => 0.4854

precs (weighed): 0.6330, 0.6076, 0.5277, 0.7509,  => 0.6298
recalls (weighed): 0.5157, 0.4978, 0.4267, 0.5015,  => 0.4854
f1s (weighed): 0.5666, 0.5472, 0.4638, 0.6012,  => 0.5447


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 124.5830, 

accs: 0.7444, 0.6498, 0.6315, 0.8515,  => 0.7193

precs (macro): 0.5029, 0.6514, 0.3439, 0.2213,  => 0.4299
recalls (macro): 0.3769, 0.6594, 0.3976, 0.2488,  => 0.4207
f1s (macro): 0.3648, 0.6459, 0.3649, 0.2342,  => 0.4024

precs (micro): 0.7444, 0.6498, 0.6315, 0.8515,  => 0.7193
recalls (micro): 0.7444, 0.6498, 0.6315, 0.8515,  => 0.7193
f1s (micro): 0.7444, 0.6498, 0.6315, 0.8515,  => 0.7193

precs (weighed): 0.7426, 0.6772, 0.5698, 0.7574,  => 0.6867
recalls (weighed): 0.7444, 0.6498, 0.6315, 0.8515,  => 0.7193
f1s (weighed): 0.6743, 0.6543, 0.5930, 0.8017,  => 0.6808



Epoch 15/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 15/20, Loss: 1049.5688, 

accs: 0.5042, 0.4964, 0.4298, 0.5040,  => 0.4836

precs (macro): 0.2716, 0.2931, 0.3250, 0.2225,  => 0.2780
recalls (macro): 0.2555, 0.2400, 0.3370, 0.2733,  => 0.2764
f1s (macro): 0.2463, 0.2638, 0.2984, 0.1782,  => 0.2467

precs (micro): 0.5042, 0.4964, 0.4298, 0.5040,  => 0.4836
recalls (micro): 0.5042, 0.4964, 0.4298, 0.5040,  => 0.4836
f1s (micro): 0.5042, 0.4964, 0.4298, 0.5040,  => 0.4836

precs (weighed): 0.6271, 0.6100, 0.5275, 0.7551,  => 0.6299
recalls (weighed): 0.5042, 0.4964, 0.4298, 0.5040,  => 0.4836
f1s (weighed): 0.5579, 0.5473, 0.4657, 0.6043,  => 0.5438


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 120.9330, 

accs: 0.7510, 0.7087, 0.6481, 0.8506,  => 0.7396

precs (macro): 0.4707, 0.7139, 0.3467, 0.2250,  => 0.4391
recalls (macro): 0.4054, 0.6525, 0.4005, 0.2485,  => 0.4267
f1s (macro): 0.4093, 0.6543, 0.3713, 0.2362,  => 0.4178

precs (micro): 0.7510, 0.7087, 0.6481, 0.8506,  => 0.7396
recalls (micro): 0.7510, 0.7087, 0.6481, 0.8506,  => 0.7396
f1s (micro): 0.7510, 0.7087, 0.6481, 0.8506,  => 0.7396

precs (weighed): 0.7254, 0.7119, 0.5694, 0.7700,  => 0.6942
recalls (weighed): 0.7510, 0.7087, 0.6481, 0.8506,  => 0.7396
f1s (weighed): 0.7104, 0.6852, 0.6058, 0.8083,  => 0.7024

Saved the best model to path: ./models/task_2/lstm_distilbert-base-multilingual-cased_14.pth


Epoch 16/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 16/20, Loss: 1047.5161, 

accs: 0.5103, 0.4864, 0.4267, 0.5020,  => 0.4813

precs (macro): 0.2768, 0.2928, 0.3187, 0.2229,  => 0.2778
recalls (macro): 0.2587, 0.3189, 0.3207, 0.2103,  => 0.2771
f1s (macro): 0.2516, 0.2613, 0.2929, 0.1778,  => 0.2459

precs (micro): 0.5103, 0.4864, 0.4267, 0.5020,  => 0.4813
recalls (micro): 0.5103, 0.4864, 0.4267, 0.5020,  => 0.4813
f1s (micro): 0.5103, 0.4864, 0.4267, 0.5020,  => 0.4813

precs (weighed): 0.6362, 0.6084, 0.5181, 0.7569,  => 0.6299
recalls (weighed): 0.5103, 0.4864, 0.4267, 0.5020,  => 0.4813
f1s (weighed): 0.5651, 0.5404, 0.4619, 0.6035,  => 0.5427


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 122.5805, 

accs: 0.7494, 0.7046, 0.6282, 0.8523,  => 0.7336

precs (macro): 0.4793, 0.7120, 0.3488, 0.2215,  => 0.4404
recalls (macro): 0.3953, 0.6456, 0.3980, 0.2490,  => 0.4220
f1s (macro): 0.3953, 0.6457, 0.3661, 0.2345,  => 0.4104

precs (micro): 0.7494, 0.7046, 0.6282, 0.8523,  => 0.7336
recalls (micro): 0.7494, 0.7046, 0.6282, 0.8523,  => 0.7336
f1s (micro): 0.7494, 0.7046, 0.6282, 0.8523,  => 0.7336

precs (weighed): 0.7288, 0.7092, 0.5804, 0.7582,  => 0.6942
recalls (weighed): 0.7494, 0.7046, 0.6282, 0.8523,  => 0.7336
f1s (weighed): 0.6992, 0.6782, 0.5949, 0.8025,  => 0.6937



Epoch 17/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 17/20, Loss: 1048.0159, 

accs: 0.5052, 0.4937, 0.4269, 0.5085,  => 0.4836

precs (macro): 0.2728, 0.2922, 0.3221, 0.2215,  => 0.2772
recalls (macro): 0.2678, 0.2399, 0.3225, 0.2122,  => 0.2606
f1s (macro): 0.2494, 0.2634, 0.2943, 0.1788,  => 0.2465

precs (micro): 0.5052, 0.4937, 0.4269, 0.5085,  => 0.4836
recalls (micro): 0.5052, 0.4937, 0.4269, 0.5085,  => 0.4836
f1s (micro): 0.5052, 0.4937, 0.4269, 0.5085,  => 0.4836

precs (weighed): 0.6279, 0.6071, 0.5245, 0.7521,  => 0.6279
recalls (weighed): 0.5052, 0.4937, 0.4269, 0.5085,  => 0.4836
f1s (weighed): 0.5582, 0.5443, 0.4643, 0.6067,  => 0.5434


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 121.2528, 

accs: 0.7427, 0.7012, 0.6324, 0.8523,  => 0.7322

precs (macro): 0.4907, 0.6843, 0.3496, 0.2256,  => 0.4376
recalls (macro): 0.3774, 0.6798, 0.3967, 0.2490,  => 0.4257
f1s (macro): 0.3664, 0.6816, 0.3687, 0.2367,  => 0.4134

precs (micro): 0.7427, 0.7012, 0.6324, 0.8523,  => 0.7322
recalls (micro): 0.7427, 0.7012, 0.6324, 0.8523,  => 0.7322
f1s (micro): 0.7427, 0.7012, 0.6324, 0.8523,  => 0.7322

precs (weighed): 0.7327, 0.6983, 0.5799, 0.7721,  => 0.6958
recalls (weighed): 0.7427, 0.7012, 0.6324, 0.8523,  => 0.7322
f1s (weighed): 0.6749, 0.6994, 0.6010, 0.8102,  => 0.6964



Epoch 18/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 18/20, Loss: 1044.1203, 

accs: 0.5091, 0.4984, 0.4316, 0.5030,  => 0.4855

precs (macro): 0.2761, 0.2951, 0.3239, 0.2233,  => 0.2796
recalls (macro): 0.2776, 0.4916, 0.3287, 0.2106,  => 0.3271
f1s (macro): 0.2523, 0.2663, 0.2971, 0.1782,  => 0.2485

precs (micro): 0.5091, 0.4984, 0.4316, 0.5030,  => 0.4855
recalls (micro): 0.5091, 0.4984, 0.4316, 0.5030,  => 0.4855
f1s (micro): 0.5091, 0.4984, 0.4316, 0.5030,  => 0.4855

precs (weighed): 0.6325, 0.6137, 0.5271, 0.7582,  => 0.6329
recalls (weighed): 0.5091, 0.4984, 0.4316, 0.5030,  => 0.4855
f1s (weighed): 0.5624, 0.5496, 0.4678, 0.6047,  => 0.5461


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 121.7694, 

accs: 0.7477, 0.7029, 0.6548, 0.8515,  => 0.7392

precs (macro): 0.4651, 0.7172, 0.3409, 0.2266,  => 0.4374
recalls (macro): 0.4045, 0.6399, 0.3961, 0.2488,  => 0.4223
f1s (macro): 0.4084, 0.6378, 0.3661, 0.2372,  => 0.4124

precs (micro): 0.7477, 0.7029, 0.6548, 0.8515,  => 0.7392
recalls (micro): 0.7477, 0.7029, 0.6548, 0.8515,  => 0.7392
f1s (micro): 0.7477, 0.7029, 0.6548, 0.8515,  => 0.7392

precs (weighed): 0.7203, 0.7120, 0.5572, 0.7755,  => 0.6913
recalls (weighed): 0.7477, 0.7029, 0.6548, 0.8515,  => 0.7392
f1s (weighed): 0.7083, 0.6723, 0.6017, 0.8117,  => 0.6985



Epoch 19/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 19/20, Loss: 1043.6093, 

accs: 0.5011, 0.4938, 0.4191, 0.5180,  => 0.4830

precs (macro): 0.2776, 0.2911, 0.3157, 0.2237,  => 0.2770
recalls (macro): 0.2733, 0.3225, 0.3171, 0.3399,  => 0.3132
f1s (macro): 0.2514, 0.2627, 0.2884, 0.1816,  => 0.2461

precs (micro): 0.5011, 0.4938, 0.4191, 0.5180,  => 0.4830
recalls (micro): 0.5011, 0.4938, 0.4191, 0.5180,  => 0.4830
f1s (micro): 0.5011, 0.4938, 0.4191, 0.5180,  => 0.4830

precs (weighed): 0.6341, 0.6058, 0.5148, 0.7591,  => 0.6285
recalls (weighed): 0.5011, 0.4938, 0.4191, 0.5180,  => 0.4830
f1s (weighed): 0.5580, 0.5438, 0.4557, 0.6155,  => 0.5432


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 118.6155, 

accs: 0.7519, 0.7154, 0.6481, 0.8556,  => 0.7427

precs (macro): 0.4712, 0.6996, 0.3638, 0.2159,  => 0.4376
recalls (macro): 0.4071, 0.6906, 0.4056, 0.2500,  => 0.4383
f1s (macro): 0.4114, 0.6938, 0.3820, 0.2317,  => 0.4297

precs (micro): 0.7519, 0.7154, 0.6481, 0.8556,  => 0.7427
recalls (micro): 0.7519, 0.7154, 0.6481, 0.8556,  => 0.7427
f1s (micro): 0.7519, 0.7154, 0.6481, 0.8556,  => 0.7427

precs (weighed): 0.7264, 0.7112, 0.5969, 0.7388,  => 0.6933
recalls (weighed): 0.7519, 0.7154, 0.6481, 0.8556,  => 0.7427
f1s (weighed): 0.7122, 0.7120, 0.6208, 0.7929,  => 0.7095

Saved the best model to path: ./models/task_2/lstm_distilbert-base-multilingual-cased_18.pth


Epoch 20/20:   0%|          | 0/264 [00:00<?, ?it/s]
Epoch 20/20, Loss: 1046.1254, 

accs: 0.4972, 0.4993, 0.4295, 0.5023,  => 0.4821

precs (macro): 0.2758, 0.2960, 0.3247, 0.2226,  => 0.2798
recalls (macro): 0.2730, 0.2431, 0.3264, 0.1479,  => 0.2476
f1s (macro): 0.2491, 0.2669, 0.2964, 0.1777,  => 0.2475

precs (micro): 0.4972, 0.4993, 0.4295, 0.5023,  => 0.4821
recalls (micro): 0.4972, 0.4993, 0.4295, 0.5023,  => 0.4821
f1s (micro): 0.4972, 0.4993, 0.4295, 0.5023,  => 0.4821

precs (weighed): 0.6302, 0.6145, 0.5279, 0.7562,  => 0.6322
recalls (weighed): 0.4972, 0.4993, 0.4295, 0.5023,  => 0.4821
f1s (weighed): 0.5542, 0.5507, 0.4670, 0.6036,  => 0.5439


Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]
Evaluation, Loss: 121.6368, 

accs: 0.7419, 0.6996, 0.5801, 0.8531,  => 0.7187

precs (macro): 0.4518, 0.7190, 0.3420, 0.2231,  => 0.4340
recalls (macro): 0.4043, 0.6333, 0.3738, 0.2493,  => 0.4151
f1s (macro): 0.4080, 0.6285, 0.3452, 0.2355,  => 0.4043

precs (micro): 0.7419, 0.6996, 0.5801, 0.8531,  => 0.7187
recalls (micro): 0.7419, 0.6996, 0.5801, 0.8531,  => 0.7187
f1s (micro): 0.7419, 0.6996, 0.5801, 0.8531,  => 0.7187

precs (weighed): 0.7102, 0.7123, 0.5747, 0.7635,  => 0.6902
recalls (weighed): 0.7419, 0.6996, 0.5801, 0.8531,  => 0.7187
f1s (weighed): 0.7062, 0.6651, 0.5599, 0.8058,  => 0.6843

Evaluating LSTM
lstm + vinai/phobert-base

[19:44:04] task: task-2                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: vinai/phobert-base                                                              my_import.py:133
           padding_len: 256                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/lstm_phobert-base                                              my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

model_weight_path: ./models/task_2/lstm_phobert-base_12.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|| 38/38 [00:06<00:00,  6.20it/s]
Evaluation, Loss: 121.1282,

accs: 0.7519, 0.6755, 0.7303, 0.8556,  => 0.7533

precs (macro): 0.5330, 0.6570, 0.5844, 0.2141,  => 0.4971
recalls (macro): 0.3822, 0.6305, 0.5348, 0.2500,  => 0.4494
f1s (macro): 0.3721, 0.6326, 0.5468, 0.2306,  => 0.4455

precs (micro): 0.7519, 0.6755, 0.7303, 0.8556,  => 0.7533
recalls (micro): 0.7519, 0.6755, 0.7303, 0.8556,  => 0.7533
f1s (micro): 0.7519, 0.6755, 0.7303, 0.8556,  => 0.7533

precs (weighed): 0.7688, 0.6661, 0.7083, 0.7327,  => 0.7189
recalls (weighed): 0.7519, 0.6755, 0.7303, 0.8556,  => 0.7533
f1s (weighed): 0.6824, 0.6608, 0.7040, 0.7894,  => 0.7091

Confusion Matrix of title aspect
[[  0  11   0]
 [  0 854   9]
 [  0 279  52]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.75      0.99      0.85       863
           2       0.85      0.16      0.27       331

    accuracy                           0.75      1205
   macro avg       0.53      0.38      0.37      1205
weighted avg       0.77      0.75      0.68      1205

Confusion Matrix of desc aspect
[[613 125]
 [266 201]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.70      0.83      0.76       738
           2       0.62      0.43      0.51       467

    accuracy                           0.68      1205
   macro avg       0.66      0.63      0.63      1205
weighted avg       0.67      0.68      0.66      1205

Confusion Matrix of company aspect
[[  0  30  25   1]
 [  0 550  30   5]
 [  0 167 224   6]
 [  0  36  25 106]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.70      0.94      0.80       585
           2       0.74      0.56      0.64       397
           3       0.90      0.63      0.74       167

    accuracy                           0.73      1205
   macro avg       0.58      0.53      0.55      1205
weighted avg       0.71      0.73      0.70      1205

Confusion Matrix of other aspect
[[   0    0    0    0]
 [   0 1031    0    0]
 [   0  137    0    0]
 [   1   36    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.86      1.00      0.92      1031
           2       0.00      0.00      0.00       137
           3       0.00      0.00      0.00        37

    accuracy                           0.86      1205
   macro avg       0.21      0.25      0.23      1205
weighted avg       0.73      0.86      0.79      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|| 76/76 [00:10<00:00,  7.22it/s]
Evaluation, Loss: 239.8919,

accs: 0.7446, 0.7032, 0.7098, 0.8520,  => 0.7524

precs (macro): 0.3869, 0.4558, 0.5563, 0.2136,  => 0.4032
recalls (macro): 0.2846, 0.4363, 0.5114, 0.2500,  => 0.3706
f1s (macro): 0.2754, 0.4396, 0.5226, 0.2304,  => 0.3670

precs (micro): 0.7446, 0.7032, 0.7098, 0.8520,  => 0.7524
recalls (micro): 0.7446, 0.7032, 0.7098, 0.8520,  => 0.7524
f1s (micro): 0.7446, 0.7032, 0.7098, 0.8520,  => 0.7524

precs (weighed): 0.7490, 0.6938, 0.6846, 0.7280,  => 0.7139
recalls (weighed): 0.7446, 0.7032, 0.7098, 0.8520,  => 0.7524
f1s (weighed): 0.6729, 0.6900, 0.6828, 0.7851,  => 0.7077

Confusion Matrix of title aspect
[[   0   28    1    0]
 [   0 1696   22    0]
 [   0  561  100    0]
 [   0    3    1    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.74      0.99      0.85      1718
           2       0.81      0.15      0.25       661
           3       0.00      0.00      0.00         4

    accuracy                           0.74      2412
   macro avg       0.39      0.28      0.28      2412
weighted avg       0.75      0.74      0.67      2412

Confusion Matrix of desc aspect
[[   0    2    0]
 [   0 1282  231]
 [   0  483  414]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.73      0.85      0.78      1513
           2       0.64      0.46      0.54       897

    accuracy                           0.70      2412
   macro avg       0.46      0.44      0.44      2412
weighted avg       0.69      0.70      0.69      2412

Confusion Matrix of company aspect
[[   0   60   34    5]
 [   0 1135   92    8]
 [   0  365  389   24]
 [   0   64   48  188]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.70      0.92      0.79      1235
           2       0.69      0.50      0.58       778
           3       0.84      0.63      0.72       300

    accuracy                           0.71      2412
   macro avg       0.56      0.51      0.52      2412
weighted avg       0.68      0.71      0.68      2412

Confusion Matrix of other aspect
[[   0    0    0    0]
 [   0 2055    0    0]
 [   7  255    0    0]
 [   0   95    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.85      1.00      0.92      2055
           2       0.00      0.00      0.00       262
           3       0.00      0.00      0.00        95

    accuracy                           0.85      2412
   macro avg       0.21      0.25      0.23      2412
weighted avg       0.73      0.85      0.79      2412

lstm + uitnlp/visobert

[19:44:41] task: task-2                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: uitnlp/visobert                                                                 my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/lstm_visobert                                                  my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/visobert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

model_weight_path: ./models/task_2/lstm_visobert_8.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|| 38/38 [00:10<00:00,  3.62it/s]
Evaluation, Loss: 120.8133,

accs: 0.7444, 0.6979, 0.6266, 0.8822,  => 0.7378

precs (macro): 0.4881, 0.6808, 0.3513, 0.5389,  => 0.5148
recalls (macro): 0.3812, 0.6767, 0.4096, 0.4281,  => 0.4739
f1s (macro): 0.3730, 0.6784, 0.3772, 0.4516,  => 0.4701

precs (micro): 0.7444, 0.6979, 0.6266, 0.8822,  => 0.7378
recalls (micro): 0.7444, 0.6979, 0.6266, 0.8822,  => 0.7378
f1s (micro): 0.7444, 0.6979, 0.6266, 0.8822,  => 0.7378

precs (weighed): 0.7316, 0.6952, 0.5468, 0.8438,  => 0.7044
recalls (weighed): 0.7444, 0.6979, 0.6266, 0.8822,  => 0.7378
f1s (weighed): 0.6806, 0.6962, 0.5824, 0.8500,  => 0.7023

Confusion Matrix of title aspect
[[  0  11   0]
 [  0 841  22]
 [  0 275  56]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.75      0.97      0.85       863
           2       0.72      0.17      0.27       331

    accuracy                           0.74      1205
   macro avg       0.49      0.38      0.37      1205
weighted avg       0.73      0.74      0.68      1205

Confusion Matrix of desc aspect
[[569 169]
 [195 272]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.74      0.77      0.76       738
           2       0.62      0.58      0.60       467

    accuracy                           0.70      1205
   macro avg       0.68      0.68      0.68      1205
weighted avg       0.70      0.70      0.70      1205

Confusion Matrix of company aspect
[[  7  17  32   0]
 [  6 458 121   0]
 [ 10  97 290   0]
 [ 40  48  79   0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.11      0.12      0.12        56
           1       0.74      0.78      0.76       585
           2       0.56      0.73      0.63       397
           3       0.00      0.00      0.00       167

    accuracy                           0.63      1205
   macro avg       0.35      0.41      0.38      1205
weighted avg       0.55      0.63      0.58      1205

Confusion Matrix of other aspect
[[1023    8    0]
 [  97   40    0]
 [  30    7    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.89      0.99      0.94      1031
           2       0.73      0.29      0.42       137
           3       0.00      0.00      0.00        37

    accuracy                           0.88      1205
   macro avg       0.54      0.43      0.45      1205
weighted avg       0.84      0.88      0.85      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|| 76/76 [00:20<00:00,  3.75it/s]
Evaluation, Loss: 240.8559,

accs: 0.7409, 0.7077, 0.6335, 0.8702,  => 0.7381

precs (macro): 0.3678, 0.4574, 0.3486, 0.5232,  => 0.4243
recalls (macro): 0.2852, 0.4546, 0.4007, 0.4104,  => 0.3877
f1s (macro): 0.2776, 0.4558, 0.3717, 0.4292,  => 0.3835

precs (micro): 0.7409, 0.7077, 0.6335, 0.8702,  => 0.7381
recalls (micro): 0.7409, 0.7077, 0.6335, 0.8702,  => 0.7381
f1s (micro): 0.7409, 0.7077, 0.6335, 0.8702,  => 0.7381

precs (weighed): 0.7282, 0.7040, 0.5637, 0.8226,  => 0.7046
recalls (weighed): 0.7409, 0.7077, 0.6335, 0.8702,  => 0.7381
f1s (weighed): 0.6738, 0.7055, 0.5948, 0.8316,  => 0.7014

Confusion Matrix of title aspect
[[   0   29    0    0]
 [   0 1679   39    0]
 [   0  553  108    0]
 [   0    3    1    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.74      0.98      0.84      1718
           2       0.73      0.16      0.27       661
           3       0.00      0.00      0.00         4

    accuracy                           0.74      2412
   macro avg       0.37      0.29      0.28      2412
weighted avg       0.73      0.74      0.67      2412

Confusion Matrix of desc aspect
[[   0    1    1]
 [   0 1188  325]
 [   0  378  519]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.76      0.79      0.77      1513
           2       0.61      0.58      0.60       897

    accuracy                           0.71      2412
   macro avg       0.46      0.45      0.46      2412
weighted avg       0.70      0.71      0.71      2412

Confusion Matrix of company aspect
[[ 11  30  58   0]
 [  4 963 268   0]
 [ 27 197 554   0]
 [ 68  94 138   0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.10      0.11      0.11        99
           1       0.75      0.78      0.76      1235
           2       0.54      0.71      0.62       778
           3       0.00      0.00      0.00       300

    accuracy                           0.63      2412
   macro avg       0.35      0.40      0.37      2412
weighted avg       0.56      0.63      0.59      2412

Confusion Matrix of other aspect
[[2036   19    0]
 [ 199   63    0]
 [  86    9    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.88      0.99      0.93      2055
           2       0.69      0.24      0.36       262
           3       0.00      0.00      0.00        95

    accuracy                           0.87      2412
   macro avg       0.52      0.41      0.43      2412
weighted avg       0.82      0.87      0.83      2412

lstm + uitnlp/CafeBERT

[19:45:31] task: task-2                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: uitnlp/CafeBERT                                                                 my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/lstm_CafeBERT                                                  my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133
Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/CafeBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

model_weight_path: ./models/task_2/lstm_CafeBERT_15.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|| 38/38 [00:31<00:00,  1.20it/s]
Evaluation, Loss: 127.0332,

accs: 0.7411, 0.6382, 0.6166, 0.8556,  => 0.7129

precs (macro): 0.5091, 0.6319, 0.3442, 0.2852,  => 0.4426
recalls (macro): 0.3704, 0.5517, 0.4008, 0.3333,  => 0.4140
f1s (macro): 0.3531, 0.5118, 0.3695, 0.3074,  => 0.3855

precs (micro): 0.7411, 0.6382, 0.6166, 0.8556,  => 0.7129
recalls (micro): 0.7411, 0.6382, 0.6166, 0.8556,  => 0.7129
f1s (micro): 0.7411, 0.6382, 0.6166, 0.8556,  => 0.7129

precs (weighed): 0.7458, 0.6337, 0.5404, 0.7321,  => 0.6630
recalls (weighed): 0.7411, 0.6382, 0.6166, 0.8556,  => 0.7129
f1s (weighed): 0.6642, 0.5677, 0.5758, 0.7890,  => 0.6492

Confusion Matrix of title aspect
[[  0  11   0]
 [  0 852  11]
 [  0 290  41]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.74      0.99      0.85       863
           2       0.79      0.12      0.21       331

    accuracy                           0.74      1205
   macro avg       0.51      0.37      0.35      1205
weighted avg       0.75      0.74      0.66      1205

Confusion Matrix of desc aspect
[[691  47]
 [389  78]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.64      0.94      0.76       738
           2       0.62      0.17      0.26       467

    accuracy                           0.64      1205
   macro avg       0.63      0.55      0.51      1205
weighted avg       0.63      0.64      0.57      1205

Confusion Matrix of company aspect
[[  8  17  31   0]
 [ 11 483  91   0]
 [  7 138 252   0]
 [ 64  35  68   0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.09      0.14      0.11        56
           1       0.72      0.83      0.77       585
           2       0.57      0.63      0.60       397
           3       0.00      0.00      0.00       167

    accuracy                           0.62      1205
   macro avg       0.34      0.40      0.37      1205
weighted avg       0.54      0.62      0.58      1205

Confusion Matrix of other aspect
[[1031    0    0]
 [ 137    0    0]
 [  37    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.86      1.00      0.92      1031
           2       0.00      0.00      0.00       137
           3       0.00      0.00      0.00        37

    accuracy                           0.86      1205
   macro avg       0.29      0.33      0.31      1205
weighted avg       0.73      0.86      0.79      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|| 76/76 [01:02<00:00,  1.22it/s]
Evaluation, Loss: 252.2688,

accs: 0.7347, 0.6567, 0.6173, 0.8520,  => 0.7152

precs (macro): 0.3682, 0.4350, 0.3321, 0.2840,  => 0.3548
recalls (macro): 0.2762, 0.3726, 0.3755, 0.3333,  => 0.3394
f1s (macro): 0.2618, 0.3498, 0.3517, 0.3067,  => 0.3175

precs (micro): 0.7347, 0.6567, 0.6173, 0.8520,  => 0.7152
recalls (micro): 0.7347, 0.6567, 0.6173, 0.8520,  => 0.7152
f1s (micro): 0.7347, 0.6567, 0.6173, 0.8520,  => 0.7152

precs (weighed): 0.7255, 0.6533, 0.5559, 0.7259,  => 0.6652
recalls (weighed): 0.7347, 0.6567, 0.6173, 0.8520,  => 0.7152
f1s (weighed): 0.6559, 0.5883, 0.5849, 0.7839,  => 0.6532

Confusion Matrix of title aspect
[[   0   26    3    0]
 [   0 1693   25    0]
 [   0  582   79    0]
 [   0    4    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.73      0.99      0.84      1718
           2       0.74      0.12      0.21       661
           3       0.00      0.00      0.00         4

    accuracy                           0.73      2412
   macro avg       0.37      0.28      0.26      2412
weighted avg       0.73      0.73      0.66      2412

Confusion Matrix of desc aspect
[[   0    2    0]
 [   0 1428   85]
 [   0  741  156]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.66      0.94      0.78      1513
           2       0.65      0.17      0.27       897

    accuracy                           0.66      2412
   macro avg       0.44      0.37      0.35      2412
weighted avg       0.65      0.66      0.59      2412

Confusion Matrix of company aspect
[[  7  38  54   0]
 [ 13 996 226   0]
 [ 28 264 486   0]
 [131  65 104   0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.04      0.07      0.05        99
           1       0.73      0.81      0.77      1235
           2       0.56      0.62      0.59       778
           3       0.00      0.00      0.00       300

    accuracy                           0.62      2412
   macro avg       0.33      0.38      0.35      2412
weighted avg       0.56      0.62      0.58      2412

Confusion Matrix of other aspect
[[2055    0    0]
 [ 262    0    0]
 [  95    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.85      1.00      0.92      2055
           2       0.00      0.00      0.00       262
           3       0.00      0.00      0.00        95

    accuracy                           0.85      2412
   macro avg       0.28      0.33      0.31      2412
weighted avg       0.73      0.85      0.78      2412

lstm + xlm-roberta-base

[19:47:28] task: task-2                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: xlm-roberta-base                                                                my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/lstm_xlm-roberta-base                                          my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

model_weight_path: ./models/task_2/lstm_xlm-roberta-base_15.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|| 38/38 [00:10<00:00,  3.46it/s]
Evaluation, Loss: 141.8422,

accs: 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672

precs (macro): 0.2387, 0.3062, 0.1220, 0.2852,  => 0.2380
recalls (macro): 0.3333, 0.5000, 0.2496, 0.3333,  => 0.3541
f1s (macro): 0.2782, 0.3798, 0.1639, 0.3074,  => 0.2823

precs (micro): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672
recalls (micro): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672
f1s (micro): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672

precs (weighed): 0.5129, 0.3751, 0.2369, 0.7321,  => 0.4642
recalls (weighed): 0.7162, 0.6124, 0.4846, 0.8556,  => 0.6672
f1s (weighed): 0.5977, 0.4652, 0.3182, 0.7890,  => 0.5426

Confusion Matrix of title aspect
[[  0  11   0]
 [  0 863   0]
 [  0 331   0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.72      1.00      0.83       863
           2       0.00      0.00      0.00       331

    accuracy                           0.72      1205
   macro avg       0.24      0.33      0.28      1205
weighted avg       0.51      0.72      0.60      1205

Confusion Matrix of desc aspect
[[738   0]
 [467   0]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.61      1.00      0.76       738
           2       0.00      0.00      0.00       467

    accuracy                           0.61      1205
   macro avg       0.31      0.50      0.38      1205
weighted avg       0.38      0.61      0.47      1205

Confusion Matrix of company aspect
[[  0  56   0   0]
 [  1 584   0   0]
 [  4 393   0   0]
 [  3 164   0   0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.49      1.00      0.66       585
           2       0.00      0.00      0.00       397
           3       0.00      0.00      0.00       167

    accuracy                           0.48      1205
   macro avg       0.12      0.25      0.16      1205
weighted avg       0.24      0.48      0.32      1205

Confusion Matrix of other aspect
[[1031    0    0]
 [ 137    0    0]
 [  37    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.86      1.00      0.92      1031
           2       0.00      0.00      0.00       137
           3       0.00      0.00      0.00        37

    accuracy                           0.86      1205
   macro avg       0.29      0.33      0.31      1205
weighted avg       0.73      0.86      0.79      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|| 76/76 [00:20<00:00,  3.65it/s]
Evaluation, Loss: 282.6397,

accs: 0.7123, 0.6273, 0.5129, 0.8520,  => 0.6761

precs (macro): 0.1781, 0.2091, 0.1604, 0.2840,  => 0.2079
recalls (macro): 0.2500, 0.3333, 0.2574, 0.3333,  => 0.2935
f1s (macro): 0.2080, 0.2570, 0.1825, 0.3067,  => 0.2385

precs (micro): 0.7123, 0.6273, 0.5129, 0.8520,  => 0.6761
recalls (micro): 0.7123, 0.6273, 0.5129, 0.8520,  => 0.6761
f1s (micro): 0.7123, 0.6273, 0.5129, 0.8520,  => 0.6761

precs (weighed): 0.5073, 0.3935, 0.2697, 0.7259,  => 0.4741
recalls (weighed): 0.7123, 0.6273, 0.5129, 0.8520,  => 0.6761
f1s (weighed): 0.5926, 0.4836, 0.3508, 0.7839,  => 0.5527

Confusion Matrix of title aspect
[[   0   29    0    0]
 [   0 1718    0    0]
 [   0  661    0    0]
 [   0    4    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.71      1.00      0.83      1718
           2       0.00      0.00      0.00       661
           3       0.00      0.00      0.00         4

    accuracy                           0.71      2412
   macro avg       0.18      0.25      0.21      2412
weighted avg       0.51      0.71      0.59      2412

Confusion Matrix of desc aspect
[[   0    2    0]
 [   0 1513    0]
 [   0  897    0]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.63      1.00      0.77      1513
           2       0.00      0.00      0.00       897

    accuracy                           0.63      2412
   macro avg       0.21      0.33      0.26      2412
weighted avg       0.39      0.63      0.48      2412

Confusion Matrix of company aspect
[[   3   96    0    0]
 [   1 1234    0    0]
 [   7  771    0    0]
 [  13  287    0    0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.12      0.03      0.05        99
           1       0.52      1.00      0.68      1235
           2       0.00      0.00      0.00       778
           3       0.00      0.00      0.00       300

    accuracy                           0.51      2412
   macro avg       0.16      0.26      0.18      2412
weighted avg       0.27      0.51      0.35      2412

Confusion Matrix of other aspect
[[2055    0    0]
 [ 262    0    0]
 [  95    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.85      1.00      0.92      2055
           2       0.00      0.00      0.00       262
           3       0.00      0.00      0.00        95

    accuracy                           0.85      2412
   macro avg       0.28      0.33      0.31      2412
weighted avg       0.73      0.85      0.78      2412

lstm + bert-base-multilingual-cased

[19:48:17] task: task-2                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: bert-base-multilingual-cased                                                    my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/lstm_bert-base-multilingual-cased                              my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

model_weight_path: ./models/task_2/lstm_bert-base-multilingual-cased_13.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|| 38/38 [00:10<00:00,  3.63it/s]
Evaluation, Loss: 135.2770,

accs: 0.7162, 0.6365, 0.5361, 0.8598,  => 0.6871

precs (macro): 0.2387, 0.6656, 0.2489, 0.4672,  => 0.4051
recalls (macro): 0.3333, 0.5397, 0.2945, 0.3497,  => 0.3793
f1s (macro): 0.2782, 0.4773, 0.2506, 0.3397,  => 0.3365

precs (micro): 0.7162, 0.6365, 0.5361, 0.8598,  => 0.6871
recalls (micro): 0.7162, 0.6365, 0.5361, 0.8598,  => 0.6871
f1s (micro): 0.7162, 0.6365, 0.5361, 0.8598,  => 0.6871

precs (weighed): 0.5129, 0.6581, 0.4148, 0.7998,  => 0.5964
recalls (weighed): 0.7162, 0.6365, 0.5361, 0.8598,  => 0.6871
f1s (weighed): 0.5977, 0.5422, 0.4397, 0.8027,  => 0.5956

Confusion Matrix of title aspect
[[  0  11   0]
 [  0 863   0]
 [  0 331   0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.72      1.00      0.83       863
           2       0.00      0.00      0.00       331

    accuracy                           0.72      1205
   macro avg       0.24      0.33      0.28      1205
weighted avg       0.51      0.72      0.60      1205

Confusion Matrix of desc aspect
[[716  22]
 [416  51]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.63      0.97      0.77       738
           2       0.70      0.11      0.19       467

    accuracy                           0.64      1205
   macro avg       0.67      0.54      0.48      1205
weighted avg       0.66      0.64      0.54      1205

Confusion Matrix of company aspect
[[  0  38  18   0]
 [  0 555  30   0]
 [  0 306  91   0]
 [  0  99  68   0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        56
           1       0.56      0.95      0.70       585
           2       0.44      0.23      0.30       397
           3       0.00      0.00      0.00       167

    accuracy                           0.54      1205
   macro avg       0.25      0.29      0.25      1205
weighted avg       0.41      0.54      0.44      1205

Confusion Matrix of other aspect
[[1029    2    0]
 [ 130    7    0]
 [  33    4    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.86      1.00      0.93      1031
           2       0.54      0.05      0.09       137
           3       0.00      0.00      0.00        37

    accuracy                           0.86      1205
   macro avg       0.47      0.35      0.34      1205
weighted avg       0.80      0.86      0.80      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|| 76/76 [00:19<00:00,  3.84it/s]
Evaluation, Loss: 268.0382,

accs: 0.7123, 0.6488, 0.5564, 0.8611,  => 0.6947

precs (macro): 0.1781, 0.4395, 0.2565, 0.5540,  => 0.3570
recalls (macro): 0.2500, 0.3602, 0.2928, 0.3635,  => 0.3166
f1s (macro): 0.2080, 0.3233, 0.2548, 0.3633,  => 0.2874

precs (micro): 0.7123, 0.6488, 0.5564, 0.8611,  => 0.6947
recalls (micro): 0.7123, 0.6488, 0.5564, 0.8611,  => 0.6947
f1s (micro): 0.7123, 0.6488, 0.5564, 0.8611,  => 0.6947

precs (weighed): 0.5073, 0.6556, 0.4404, 0.8212,  => 0.6061
recalls (weighed): 0.7123, 0.6488, 0.5564, 0.8611,  => 0.6947
f1s (weighed): 0.5926, 0.5588, 0.4645, 0.8063,  => 0.6055

Confusion Matrix of title aspect
[[   0   29    0    0]
 [   0 1718    0    0]
 [   0  661    0    0]
 [   0    4    0    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.71      1.00      0.83      1718
           2       0.00      0.00      0.00       661
           3       0.00      0.00      0.00         4

    accuracy                           0.71      2412
   macro avg       0.18      0.25      0.21      2412
weighted avg       0.51      0.71      0.59      2412

Confusion Matrix of desc aspect
[[   0    2    0]
 [   0 1463   50]
 [   0  795  102]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.65      0.97      0.78      1513
           2       0.67      0.11      0.19       897

    accuracy                           0.65      2412
   macro avg       0.44      0.36      0.32      2412
weighted avg       0.66      0.65      0.56      2412

Confusion Matrix of company aspect
[[   0   72   27    0]
 [   0 1164   71    0]
 [   0  600  178    0]
 [   0  179  121    0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        99
           1       0.58      0.94      0.72      1235
           2       0.45      0.23      0.30       778
           3       0.00      0.00      0.00       300

    accuracy                           0.56      2412
   macro avg       0.26      0.29      0.25      2412
weighted avg       0.44      0.56      0.46      2412

Confusion Matrix of other aspect
[[2053    2    0]
 [ 238   24    0]
 [  91    4    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           1       0.86      1.00      0.93      2055
           2       0.80      0.09      0.16       262
           3       0.00      0.00      0.00        95

    accuracy                           0.86      2412
   macro avg       0.55      0.36      0.36      2412
weighted avg       0.82      0.86      0.81      2412

lstm + distilbert-base-multilingual-cased

[19:49:03] task: task-2                                                                                my_import.py:133
           model_type: lstm                                                                            my_import.py:133
           model_name: distilbert-base-multilingual-cased                                              my_import.py:133
           padding_len: 512                                                                            my_import.py:131
           batch_size: 32                                                                              my_import.py:133
           learning_rate: 0.001                                                                        my_import.py:133
           epochs: 10                                                                                  my_import.py:133
           fine_tune: True                                                                             my_import.py:133
           hidden_size: 128                                                                            my_import.py:133
           num_layers: 1                                                                               my_import.py:133
           device: cuda                                                                                my_import.py:133
           saving_path: ./models/task_2/lstm_distilbert-base-multilingual-cased                        my_import.py:133
           train_shape: (8437, 27)                                                                     my_import.py:133
           dev_shape: (1205, 27)                                                                       my_import.py:133
           test_shape: (2412, 27)                                                                      my_import.py:133

model_weight_path: ./models/task_2/lstm_distilbert-base-multilingual-cased_18.pth
Loading model weight successfully!

Evaluation on dev set
Evaluation, Batch 38/38: 100%|| 38/38 [00:06<00:00,  6.09it/s]
Evaluation, Loss: 118.6155,

accs: 0.7519, 0.7154, 0.6481, 0.8556,  => 0.7427

precs (macro): 0.4712, 0.6996, 0.3638, 0.2159,  => 0.4376
recalls (macro): 0.4071, 0.6906, 0.4056, 0.2500,  => 0.4383
f1s (macro): 0.4114, 0.6938, 0.3820, 0.2317,  => 0.4297

precs (micro): 0.7519, 0.7154, 0.6481, 0.8556,  => 0.7427
recalls (micro): 0.7519, 0.7154, 0.6481, 0.8556,  => 0.7427
f1s (micro): 0.7519, 0.7154, 0.6481, 0.8556,  => 0.7427

precs (weighed): 0.7264, 0.7112, 0.5969, 0.7388,  => 0.6933
recalls (weighed): 0.7519, 0.7154, 0.6481, 0.8556,  => 0.7427
f1s (weighed): 0.7122, 0.7120, 0.6208, 0.7929,  => 0.7095

Confusion Matrix of title aspect
[[  0  10   1]
 [  0 814  49]
 [  0 239  92]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        11
           1       0.77      0.94      0.85       863
           2       0.65      0.28      0.39       331

    accuracy                           0.75      1205
   macro avg       0.47      0.41      0.41      1205
weighted avg       0.73      0.75      0.71      1205

Confusion Matrix of desc aspect
[[591 147]
 [196 271]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           1       0.75      0.80      0.78       738
           2       0.65      0.58      0.61       467

    accuracy                           0.72      1205
   macro avg       0.70      0.69      0.69      1205
weighted avg       0.71      0.72      0.71      1205

Confusion Matrix of company aspect
[[  3  13  40   0]
 [  9 483  93   0]
 [  6  96 295   0]
 [117  17  33   0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.02      0.05      0.03        56
           1       0.79      0.83      0.81       585
           2       0.64      0.74      0.69       397
           3       0.00      0.00      0.00       167

    accuracy                           0.65      1205
   macro avg       0.36      0.41      0.38      1205
weighted avg       0.60      0.65      0.62      1205

Confusion Matrix of other aspect
[[   0    0    0    0]
 [   0 1031    0    0]
 [   9  128    0    0]
 [   2   35    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.86      1.00      0.93      1031
           2       0.00      0.00      0.00       137
           3       0.00      0.00      0.00        37

    accuracy                           0.86      1205
   macro avg       0.22      0.25      0.23      1205
weighted avg       0.74      0.86      0.79      1205

Evaluation on test set
Evaluation, Batch 76/76: 100%|| 76/76 [00:11<00:00,  6.60it/s]
Evaluation, Loss: 236.0640,

accs: 0.7550, 0.7276, 0.6546, 0.8507,  => 0.7470

precs (macro): 0.3606, 0.4721, 0.3636, 0.2157,  => 0.3530
recalls (macro): 0.3090, 0.4694, 0.4021, 0.2496,  => 0.3575
f1s (macro): 0.3124, 0.4706, 0.3796, 0.2314,  => 0.3485

precs (micro): 0.7550, 0.7276, 0.6546, 0.8507,  => 0.7470
recalls (micro): 0.7550, 0.7276, 0.6546, 0.8507,  => 0.7470
f1s (micro): 0.7550, 0.7276, 0.6546, 0.8507,  => 0.7470

precs (weighed): 0.7306, 0.7245, 0.6154, 0.7352,  => 0.7014
recalls (weighed): 0.7550, 0.7276, 0.6546, 0.8507,  => 0.7470
f1s (weighed): 0.7139, 0.7258, 0.6331, 0.7888,  => 0.7154

Confusion Matrix of title aspect
[[   0   27    2    0]
 [   0 1632   86    0]
 [   0  472  189    0]
 [   0    2    2    0]]
Classification Report for title aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        29
           1       0.77      0.95      0.85      1718
           2       0.68      0.29      0.40       661
           3       0.00      0.00      0.00         4

    accuracy                           0.75      2412
   macro avg       0.36      0.31      0.31      2412
weighted avg       0.73      0.75      0.71      2412

Confusion Matrix of desc aspect
[[   0    2    0]
 [   0 1208  305]
 [   0  350  547]]
Classification Report for desc aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.77      0.80      0.79      1513
           2       0.64      0.61      0.63       897

    accuracy                           0.73      2412
   macro avg       0.47      0.47      0.47      2412
weighted avg       0.72      0.73      0.73      2412

Confusion Matrix of company aspect
[[  6  26  67   0]
 [ 10 997 228   0]
 [ 26 176 576   0]
 [211  36  53   0]]
Classification Report for company aspect
              precision    recall  f1-score   support

           0       0.02      0.06      0.03        99
           1       0.81      0.81      0.81      1235
           2       0.62      0.74      0.68       778
           3       0.00      0.00      0.00       300

    accuracy                           0.65      2412
   macro avg       0.36      0.40      0.38      2412
weighted avg       0.62      0.65      0.63      2412

Confusion Matrix of other aspect
[[   0    0    0    0]
 [   3 2052    0    0]
 [  29  233    0    0]
 [   2   93    0    0]]
Classification Report for other aspect
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.86      1.00      0.93      2055
           2       0.00      0.00      0.00       262
           3       0.00      0.00      0.00        95

    accuracy                           0.85      2412
   macro avg       0.22      0.25      0.23      2412
weighted avg       0.74      0.85      0.79      2412